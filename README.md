<div align="center" style="margin-bottom: 1em;">

<img width=50% src="docs/assets/images/logo.png" alt="Fast-LLM Logo"></img>

[![Tests][tests-badge]][tests]
[![Documentation][docs-badge]][docs]
[![License][license-badge]][license]

*Accelerating your LLM training to full speed*

Made with â¤ï¸ by [ServiceNow Research][servicenow-research]

</div>

## Overview

Fast-LLM is a new open-source library for training large language models. It's design focuses on speed, scalability, flexibility, and ease of use. Fast-LLM is built on top of [PyTorch](https://pytorch.org/) and [Triton](https://triton-lang.org) to provide a state-of-the-art training experience.

## Why Fast-LLM?

1. ğŸš€ **Fast-LLM is Blazing Fast**:
    - âš¡ï¸ Optimized kernel efficiency and reduced overheads.
    - ğŸ”‹ Better memory usage.
    - â±â³ Reduced training time and cost.
  
2. ğŸ“ˆ **Fast-LLM is Highly Scalable**:
    - ğŸ“¡ Distributed training across multiple GPUs and nodes using 3D parallelism (DP+TP+PP).
    - ğŸ”„ Supports sequence length parallelism.
    - ğŸ§  ZeRO-1, ZeRO-2, and ZeRO-3 offloading for memory efficiency.
    - ğŸ›ï¸ Support for mixed precision training.
    - ğŸ‹ï¸â€â™‚ï¸ Large batch training and gradient accumulation support.

3. ğŸ¨ **Fast-LLM is Incredibly Flexible**:
    - ğŸ¤– Compatible with all common language model architectures in a unified class.
    - âš¡ Efficient dropless Mixture-of-Experts (MoE) support.
    - ğŸ§© Customizable for language model architectures, data loaders, loss functions, and optimizers.
    - ğŸ¤— Compatible with [Hugging Face Transformers](https://huggingface.co/transformers/).

4. ğŸ¯ **Fast-LLM is Super Easy to Use**:
    - ğŸ“¦ Pre-built Docker images for fast deployment.
    - ğŸ“ Simple YAML configuration for hassle-free setup.
    - ğŸ’» Command-line interface for seamless launches.
    - ğŸ“Š Detailed logging and real-time monitoring features.
    - ğŸ“š Comprehensive documentation and helpful tutorials.

5. ğŸŒ **Fast-LLM is Truly Open Source**:
    - âš–ï¸ Apache 2.0 License.
    - ğŸ’» Fully developed on GitHub with a public roadmap and transparent issue tracking.
    - ğŸ¤ Contributions and collaboration are always welcome!

## Usage

## Next Steps

  **Want to learn more?** Check out our [documentation](https://servicenow.github.io/Fast-LLM) for more information on how to use Fast-LLM.

ğŸ”¨ **We welcome contributions to Fast-LLM!** Have a look at our [contribution guidelines](CONTRIBUTING.md).

ğŸ **Something doesn't work?** Open an [issue](https://github.com/ServiceNow/Fast-LLM/issues)!

## License

Fast-LLM is licensed by ServiceNow, Inc. under the Apache 2.0 License. See [LICENSE](LICENSE) for more information.

## Vulnerability Reporting

If you find a security vulnerability in Fast-LLM, please report it to us at [psirt-oss@servicenow.com](mailto:psirt-oss@servicenow.com) as soon as possible. Please refer to our [security policy](SECURITY.md) for more information.

[tests-badge]: https://github.com/ServiceNow/Fast-LLM/actions/workflows/pythonci.yml/badge.svg
[tests]: https://github.com/ServiceNow/Fast-LLM/actions/workflows/pythonci.yml
[docs-badge]: https://github.com/ServiceNow/Fast-LLM/actions/workflows/docs_cd.yml/badge.svg
[docs]: https://servicenow.github.io/Fast-LLM
[license-badge]: https://img.shields.io/badge/License-Apache%202.0-blue.svg
[license]: LICENSE
[servicenow-research]: https://www.servicenow.com/research/
