# Dataset Discovery Tool

A tool to recursively discover datasets in a directory and generate a concatenated dataset configuration for Fast-LLM.

## Overview

This tool walks through a directory tree, identifies datasets by their `fast_llm_config*.yaml` files, and generates a configuration file that concatenates all discovered datasets.

## Features

- **Recursive Discovery**: Automatically finds all dataset configs in nested directories
- **Flexible Output**: Can use file references or inline full configs
- **Blended Datasets**: Option to create blended datasets with weights proportional to token counts

## Usage

### Command Line

```bash
python tools/discover_datasets.py <directory> -o <output.yaml> [options]
```

**Arguments:**

- `directory`: Directory to search for datasets recursively (required)
- `-o, --output`: Output path for the generated config YAML file (required)
- `--no-file-refs`: Inline configs instead of using file references (optional)
- `--blended`: Create blended datasets with weights proportional to token counts (optional)

**Examples:**

```bash
# Basic usage - discover all datasets (concatenated)
python tools/discover_datasets.py /path/to/datasets -o combined_dataset.yaml

# Create blended datasets with token-proportional weights
python tools/discover_datasets.py /path/to/datasets -o blended_dataset.yaml --blended

# Inline full configs instead of using file references
python tools/discover_datasets.py /path/to/datasets -o combined_dataset.yaml --no-file-refs
```

### Config File

Create a config file:

```yaml
# discover_config.yaml
directory: /path/to/datasets
output: combined_dataset.yaml
use_file_refs: true
use_blended: false  # Set to true for blended datasets with token-proportional weights
```

Run with:

```bash
python tools/discover_datasets.py --config discover_config.yaml
```

## Dataset Identification

The tool identifies datasets by looking for files matching the pattern `fast_llm_config*.yaml`:

- `fast_llm_config.yaml` - Unsplit dataset
- `fast_llm_config_training.yaml` - Training split
- `fast_llm_config_validation.yaml` - Validation split
- Any other `fast_llm_config_*.yaml` files

These files are typically generated by the `fast-llm prepare` command during dataset preparation.

## Output Formats

### Concatenated Datasets (Default)

The tool generates a concatenated dataset config that includes all discovered datasets:

```yaml
type: concatenated
name: my_datasets
datasets:
  - type: file
    path: /path/to/dataset1/fast_llm_config_training.yaml
  - type: file
    path: /path/to/dataset1/fast_llm_config_validation.yaml
  - type: file
    path: /path/to/dataset2/fast_llm_config.yaml
```

With concatenated datasets, all datasets are combined sequentially - you'll see all samples from dataset1 first, then all from dataset2, etc.

### Blended Datasets (with `--blended`)

With the `--blended` flag, the tool creates a blended dataset config with weights proportional to the number of tokens in each dataset:

```yaml
type: blended
name: my_datasets
datasets:
  - type: file
    path: /path/to/dataset1/fast_llm_config_training.yaml
  - type: file
    path: /path/to/dataset1/fast_llm_config_validation.yaml
  - type: file
    path: /path/to/dataset2/fast_llm_config.yaml
weights:
  - 1500000  # Dataset 1 has 1.5M tokens
  - 500000   # Dataset 2 has 500K tokens
  - 2000000  # Dataset 3 has 2M tokens
```

With blended datasets, samples are drawn from each dataset proportionally to their weights during training. This means:

- Larger datasets (more tokens) will be sampled more frequently
- Smaller datasets will be sampled less frequently
- The sampling is interleaved, not sequential
- Each dataset maintains its internal order, but samples from different datasets are mixed

**Hierarchical blending:** When datasets are in nested directories, the tool automatically calculates proper token-proportional weights at all levels. Subdirectories are weighted by their total token count (sum of all datasets within them), ensuring accurate proportional sampling across the entire directory structure.

**When to use blended vs concatenated:**

- **Concatenated**: Use when you want to see all data from each dataset sequentially, or when combining dataset splits (train/val).
- **Blended**: Use when you want proportional sampling from multiple data sources during training (e.g., mixing code, books, and web data proportionally).

### Using in Training Config

Both formats can be used directly in a training config:

```yaml
data:
  datasets:
    training:
      type: file
      path: combined_dataset.yaml
```

## Example Workflow

### 1. Prepare Multiple Datasets

```bash
# Prepare dataset 1
fast-llm prepare --config dataset1_prepare.yaml

# Prepare dataset 2
fast-llm prepare --config dataset2_prepare.yaml

# Prepare dataset 3
fast-llm prepare --config dataset3_prepare.yaml
```

This creates a directory structure like:

```
my_datasets/
├── dataset1/
│   ├── fast_llm_config_training.yaml
│   ├── fast_llm_config_validation.yaml
│   ├── dataset1_training.fast_llm_dataset
│   └── dataset1_validation.fast_llm_dataset
├── dataset2/
│   ├── fast_llm_config_training.yaml
│   ├── fast_llm_config_validation.yaml
│   ├── dataset2_training.fast_llm_dataset
│   └── dataset2_validation.fast_llm_dataset
└── dataset3/
    └── experiments/
        ├── fast_llm_config_training.yaml
        └── dataset3_training.fast_llm_dataset
```

### 2. Discover and Combine Datasets

```bash
python tools/discover_datasets.py my_datasets/ -o combined_datasets.yaml
```

This generates `combined_datasets.yaml`:

```yaml
type: concatenated
name: my_datasets
datasets:
  - type: file
    path: my_datasets/dataset1/fast_llm_config_training.yaml
  - type: file
    path: my_datasets/dataset1/fast_llm_config_validation.yaml
  - type: file
    path: my_datasets/dataset2/fast_llm_config_training.yaml
  - type: file
    path: my_datasets/dataset2/fast_llm_config_validation.yaml
  - type: file
    path: my_datasets/dataset3/experiments/fast_llm_config_training.yaml
```

### 3. Use in Training Config

```yaml
# training_config.yaml
model:
  # ... model config ...

data:
  datasets:
    training:
      type: file
      path: combined_datasets.yaml
  sampling:
    shuffle: skip_first_epoch
    seed: 784569

# ... rest of training config ...
```

### 4. Train

```bash
fast-llm train --config training_config.yaml
```

## Use Cases

### 1. Combining Multiple Data Sources

You have data from different sources (web scrapes, books, code, etc.) prepared separately:

```bash
python tools/discover_datasets.py /data/pretraining -o all_pretraining_data.yaml
```

### 2. Incremental Data Addition

You keep adding new datasets over time and want to automatically include all of them:

```bash
# Just add new prepared datasets to the directory
# Re-run discovery to update the combined config
python tools/discover_datasets.py /data/pretraining -o all_pretraining_data.yaml
```

### 3. Experiment Organization

You have experiments with different preprocessing or filtering:

```
experiments/
├── baseline/
│   ├── fast_llm_config_training.yaml
│   └── fast_llm_config_validation.yaml
├── filtered_v1/
│   ├── fast_llm_config_training.yaml
│   └── fast_llm_config_validation.yaml
└── filtered_v2/
    ├── fast_llm_config_training.yaml
    └── fast_llm_config_validation.yaml
```

```bash
python tools/discover_datasets.py experiments/ -o all_experiments.yaml
```

## Notes

- **File References**: By default, the tool uses `type: file` references which lazily load the actual dataset configs. This keeps the generated config small and readable.

- **Absolute Paths**: The tool uses absolute paths for file references to ensure configs work regardless of where they're used from.

- **Ordering**: Datasets are discovered and ordered alphabetically by path for consistency.

- **Empty Directories**: If no `fast_llm_config*.yaml` files are found, the tool will raise an error.

- **All Files Included**: The tool concatenates ALL discovered config files. This means if you have both training and validation configs in the same directory, they will all be concatenated together. You may want to organize your directory structure accordingly or filter the specific configs you need after generation.

## See Also

- [Fast-LLM Data Configuration Documentation](../docs/recipes/data-configuration.md)
- [Dataset Preparation Guide](../docs/recipes/data-preparation.md)
