--index-url https://download.pytorch.org/whl/nightly/cu128
--extra-index-url https://pypi.org/simple
--extra-index-url https://pypi.fla-org.com/simple

# Core nightly stack
--pre torch
triton-nightly

# KDA deps compiled against the nightly toolchain
flash-linear-attention @ git+https://github.com/fla-org/flash-linear-attention@main
causal-conv1d@git+https://github.com/Dao-AILab/causal-conv1d@2a288a1
mamba_ssm[causal-conv1d]==2.2.4
flash-attn==2.7.3
