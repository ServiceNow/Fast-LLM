{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toolkit/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "fast_llm_path = \"/home/toolkit/dev/Fast-LLM\"\n",
    "\n",
    "# add fast_llm to the python path\n",
    "import sys\n",
    "sys.path.append(fast_llm_path)\n",
    "from fast_llm.models.ssm.external.apriel_hybrid.modeling_ssm_hybrid_apriel import AprielSSMHybridConfig\n",
    "from fast_llm.models.ssm.external.apriel_hybrid.modeling_ssm_hybrid_apriel import AprielSSMHybridModel, AprielSSMDecoderLayer, AprielSSMHybridForCausalLM\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 0.612615\n",
    "layer_scores = {\n",
    "    \"22\": 0.607389,\n",
    "    \"24\": 0.603498,\n",
    "    \"19\": 0.597907,\n",
    "    \"27\": 0.597173,\n",
    "    \"20\": 0.590442,\n",
    "    \"5\": 0.578949,\n",
    "    \"4\": 0.576852,\n",
    "    \"9\": 0.576484,\n",
    "    \"23\": 0.574833,\n",
    "    \"7\": 0.571860,\n",
    "    \"8\": 0.571790,\n",
    "    \"6\": 0.571614,\n",
    "    \"2\": 0.571330,\n",
    "    \"26\": 0.570205,\n",
    "    \"11\": 0.567128,\n",
    "    \"14\": 0.566175,\n",
    "    \"15\": 0.566076,\n",
    "    \"3\": 0.562861,\n",
    "    \"1\": 0.560154,\n",
    "    \"13\": 0.559304,\n",
    "    \"16\": 0.559017,\n",
    "    \"10\": 0.558789,\n",
    "    \"12\": 0.555186,\n",
    "    \"17\": 0.554236,\n",
    "    \"25\": 0.549215,\n",
    "    \"18\": 0.537257,\n",
    "    \"0\": 0.233085,\n",
    "}\n",
    "layer_scores = {k: base - v for k, v in layer_scores.items()}\n",
    "layer_importanfce = sorted(layer_scores.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('22', 0.005226000000000064),\n",
       " ('24', 0.009117000000000042),\n",
       " ('19', 0.014708000000000054),\n",
       " ('27', 0.015442000000000067),\n",
       " ('20', 0.022173),\n",
       " ('5', 0.033665999999999974),\n",
       " ('4', 0.03576299999999999),\n",
       " ('9', 0.036131000000000024),\n",
       " ('23', 0.03778199999999998),\n",
       " ('7', 0.040754999999999986),\n",
       " ('8', 0.040825),\n",
       " ('6', 0.041001000000000065),\n",
       " ('2', 0.041285000000000016),\n",
       " ('26', 0.04241000000000006),\n",
       " ('11', 0.045487000000000055),\n",
       " ('14', 0.04644000000000004),\n",
       " ('15', 0.046539),\n",
       " ('3', 0.049754000000000076),\n",
       " ('1', 0.05246099999999998),\n",
       " ('13', 0.053311),\n",
       " ('16', 0.053598000000000035),\n",
       " ('10', 0.05382600000000004),\n",
       " ('12', 0.05742900000000006),\n",
       " ('17', 0.05837900000000007),\n",
       " ('25', 0.06340000000000001),\n",
       " ('18', 0.07535800000000004),\n",
       " ('0', 0.37953000000000003)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importanfce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hybrid with any number of SSM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"ServiceNow-AI/Apriel-5B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "device = \"cuda\"\n",
    "n_hybrid = 14\n",
    "\n",
    "index_swaped = []\n",
    "hybrid_block_layout = [\"t\"] * config.num_hidden_layers\n",
    "for i in range(n_hybrid):\n",
    "    hybrid_block_layout[int(layer_importanfce[i][0])] = \"m2d\"\n",
    "    index_swaped.append(int(layer_importanfce[i][0]))\n",
    "\n",
    "hybrdif_apriel_config = AprielSSMHybridConfig(**config.to_dict(),\n",
    "                                              hybrid_block_layout=hybrid_block_layout,\n",
    "                                              ssm_cfg={\n",
    "                                                  \"d_state\": 64,\n",
    "                                                  \"n_v_heads\": 24,\n",
    "                                                  \"n_qk_heads\": 24,\n",
    "                                                  \"expand\": 1,\n",
    "                                                  \"chunk_size\": 128,\n",
    "                                                  \"activation\": \"identity\",\n",
    "                                                  \"bias\": False,\n",
    "                                                  \"d_inner\": 24 * 128,  # num_heads * head_dim\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrdif_apriel_config.hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AprielSSMHybridForCausalLM(\n",
       "  (model): AprielSSMHybridModel(\n",
       "    (embed_tokens): Embedding(131072, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2): AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (3): AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (4-9): 6 x AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (10-18): 9 x AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (19-20): 2 x AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (21): AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22-24): 3 x AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (25): AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (26-27): 2 x AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): AprielRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_apriel_model = AprielSSMHybridForCausalLM(hybrdif_apriel_config)\n",
    "hybrid_apriel_model.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "apriel_model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "apriel_state_dict = apriel_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing, unexpected = hybrid_apriel_model.load_state_dict(apriel_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['model.layers.2.mixer.z_bias', 'model.layers.2.mixer.D', 'model.layers.2.mixer.in_proj.weight', 'model.layers.2.mixer.conv1d.weight', 'model.layers.2.mixer.conv1d.bias', 'model.layers.2.mixer.out_proj.weight', 'model.layers.4.mixer.z_bias', 'model.layers.4.mixer.D', 'model.layers.4.mixer.in_proj.weight', 'model.layers.4.mixer.conv1d.weight', 'model.layers.4.mixer.conv1d.bias', 'model.layers.4.mixer.out_proj.weight', 'model.layers.5.mixer.z_bias', 'model.layers.5.mixer.D', 'model.layers.5.mixer.in_proj.weight', 'model.layers.5.mixer.conv1d.weight', 'model.layers.5.mixer.conv1d.bias', 'model.layers.5.mixer.out_proj.weight', 'model.layers.6.mixer.z_bias', 'model.layers.6.mixer.D', 'model.layers.6.mixer.in_proj.weight', 'model.layers.6.mixer.conv1d.weight', 'model.layers.6.mixer.conv1d.bias', 'model.layers.6.mixer.out_proj.weight', 'model.layers.7.mixer.z_bias', 'model.layers.7.mixer.D', 'model.layers.7.mixer.in_proj.weight', 'model.layers.7.mixer.conv1d.weight', 'model.layers.7.mixer.conv1d.bias', 'model.layers.7.mixer.out_proj.weight', 'model.layers.8.mixer.z_bias', 'model.layers.8.mixer.D', 'model.layers.8.mixer.in_proj.weight', 'model.layers.8.mixer.conv1d.weight', 'model.layers.8.mixer.conv1d.bias', 'model.layers.8.mixer.out_proj.weight', 'model.layers.9.mixer.z_bias', 'model.layers.9.mixer.D', 'model.layers.9.mixer.in_proj.weight', 'model.layers.9.mixer.conv1d.weight', 'model.layers.9.mixer.conv1d.bias', 'model.layers.9.mixer.out_proj.weight', 'model.layers.19.mixer.z_bias', 'model.layers.19.mixer.D', 'model.layers.19.mixer.in_proj.weight', 'model.layers.19.mixer.conv1d.weight', 'model.layers.19.mixer.conv1d.bias', 'model.layers.19.mixer.out_proj.weight', 'model.layers.20.mixer.z_bias', 'model.layers.20.mixer.D', 'model.layers.20.mixer.in_proj.weight', 'model.layers.20.mixer.conv1d.weight', 'model.layers.20.mixer.conv1d.bias', 'model.layers.20.mixer.out_proj.weight', 'model.layers.22.mixer.z_bias', 'model.layers.22.mixer.D', 'model.layers.22.mixer.in_proj.weight', 'model.layers.22.mixer.conv1d.weight', 'model.layers.22.mixer.conv1d.bias', 'model.layers.22.mixer.out_proj.weight', 'model.layers.23.mixer.z_bias', 'model.layers.23.mixer.D', 'model.layers.23.mixer.in_proj.weight', 'model.layers.23.mixer.conv1d.weight', 'model.layers.23.mixer.conv1d.bias', 'model.layers.23.mixer.out_proj.weight', 'model.layers.24.mixer.z_bias', 'model.layers.24.mixer.D', 'model.layers.24.mixer.in_proj.weight', 'model.layers.24.mixer.conv1d.weight', 'model.layers.24.mixer.conv1d.bias', 'model.layers.24.mixer.out_proj.weight', 'model.layers.26.mixer.z_bias', 'model.layers.26.mixer.D', 'model.layers.26.mixer.in_proj.weight', 'model.layers.26.mixer.conv1d.weight', 'model.layers.26.mixer.conv1d.bias', 'model.layers.26.mixer.out_proj.weight', 'model.layers.27.mixer.z_bias', 'model.layers.27.mixer.D', 'model.layers.27.mixer.in_proj.weight', 'model.layers.27.mixer.conv1d.weight', 'model.layers.27.mixer.conv1d.bias', 'model.layers.27.mixer.out_proj.weight']\n",
      "Unexpected keys: ['model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "# unexpected will contain keys from the SSM layers we added\n",
    "print(\"Missing keys:\", missing)\n",
    "# unexpected will contain keys from the transformer layers we replaced\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from fast_llm.models.ssm.external.apriel_ssm.modeling_ssm_apriel import AprielSSMModel, AprielSSMForCausalLM\n",
    "\n",
    "mohawk_path = \"/mnt/checkpoints/ssm/mohawk_distributed_stage2_apriel_8GPU_16ksteps_lr0.0_layernorm/final\"\n",
    "# config = AutoConfig.from_pretrained(mohawk_path, trust_remote_code=True)\n",
    "apriel_model = AprielSSMForCausalLM.from_pretrained(mohawk_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "apriel_state_dict = apriel_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing, unexpected = hybrid_apriel_model.load_state_dict(apriel_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight']\n",
      "Unexpected keys: ['model.layers.0.mixer.z_bias', 'model.layers.0.mixer.D', 'model.layers.0.mixer.in_proj.weight', 'model.layers.0.mixer.conv1d.weight', 'model.layers.0.mixer.conv1d.bias', 'model.layers.0.mixer.out_proj.weight', 'model.layers.1.mixer.z_bias', 'model.layers.1.mixer.D', 'model.layers.1.mixer.in_proj.weight', 'model.layers.1.mixer.conv1d.weight', 'model.layers.1.mixer.conv1d.bias', 'model.layers.1.mixer.out_proj.weight', 'model.layers.3.mixer.z_bias', 'model.layers.3.mixer.D', 'model.layers.3.mixer.in_proj.weight', 'model.layers.3.mixer.conv1d.weight', 'model.layers.3.mixer.conv1d.bias', 'model.layers.3.mixer.out_proj.weight', 'model.layers.10.mixer.z_bias', 'model.layers.10.mixer.D', 'model.layers.10.mixer.in_proj.weight', 'model.layers.10.mixer.conv1d.weight', 'model.layers.10.mixer.conv1d.bias', 'model.layers.10.mixer.out_proj.weight', 'model.layers.11.mixer.z_bias', 'model.layers.11.mixer.D', 'model.layers.11.mixer.in_proj.weight', 'model.layers.11.mixer.conv1d.weight', 'model.layers.11.mixer.conv1d.bias', 'model.layers.11.mixer.out_proj.weight', 'model.layers.12.mixer.z_bias', 'model.layers.12.mixer.D', 'model.layers.12.mixer.in_proj.weight', 'model.layers.12.mixer.conv1d.weight', 'model.layers.12.mixer.conv1d.bias', 'model.layers.12.mixer.out_proj.weight', 'model.layers.13.mixer.z_bias', 'model.layers.13.mixer.D', 'model.layers.13.mixer.in_proj.weight', 'model.layers.13.mixer.conv1d.weight', 'model.layers.13.mixer.conv1d.bias', 'model.layers.13.mixer.out_proj.weight', 'model.layers.14.mixer.z_bias', 'model.layers.14.mixer.D', 'model.layers.14.mixer.in_proj.weight', 'model.layers.14.mixer.conv1d.weight', 'model.layers.14.mixer.conv1d.bias', 'model.layers.14.mixer.out_proj.weight', 'model.layers.15.mixer.z_bias', 'model.layers.15.mixer.D', 'model.layers.15.mixer.in_proj.weight', 'model.layers.15.mixer.conv1d.weight', 'model.layers.15.mixer.conv1d.bias', 'model.layers.15.mixer.out_proj.weight', 'model.layers.16.mixer.z_bias', 'model.layers.16.mixer.D', 'model.layers.16.mixer.in_proj.weight', 'model.layers.16.mixer.conv1d.weight', 'model.layers.16.mixer.conv1d.bias', 'model.layers.16.mixer.out_proj.weight', 'model.layers.17.mixer.z_bias', 'model.layers.17.mixer.D', 'model.layers.17.mixer.in_proj.weight', 'model.layers.17.mixer.conv1d.weight', 'model.layers.17.mixer.conv1d.bias', 'model.layers.17.mixer.out_proj.weight', 'model.layers.18.mixer.z_bias', 'model.layers.18.mixer.D', 'model.layers.18.mixer.in_proj.weight', 'model.layers.18.mixer.conv1d.weight', 'model.layers.18.mixer.conv1d.bias', 'model.layers.18.mixer.out_proj.weight', 'model.layers.21.mixer.z_bias', 'model.layers.21.mixer.D', 'model.layers.21.mixer.in_proj.weight', 'model.layers.21.mixer.conv1d.weight', 'model.layers.21.mixer.conv1d.bias', 'model.layers.21.mixer.out_proj.weight', 'model.layers.25.mixer.z_bias', 'model.layers.25.mixer.D', 'model.layers.25.mixer.in_proj.weight', 'model.layers.25.mixer.conv1d.weight', 'model.layers.25.mixer.conv1d.bias', 'model.layers.25.mixer.out_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "# unexpected will contain keys from the SSM layers we added\n",
    "print(\"Missing keys:\", missing)\n",
    "# unexpected will contain keys from the transformer layers we replaced\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_apriel_model.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_instruct_hybrid_14ssm_leastimportant_init_MOHAWK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the hybrid model\n",
    "output_path = \"/mnt/checkpoints/ssm/iterative_hybrids_5b\"\n",
    "assert len(index_swaped) == 1\n",
    "layer_swaped = index_swaped[0]\n",
    "hybrid_apriel_model.save_pretrained(\n",
    "        f\"{output_path}/apriel_ssm_instruct5b_hybrid_{layer_swaped+1}ssm_leastimportant_32h_init_rand\"\n",
    "    )\n",
    "print(f\"Hybrid model saved to {output_path}/apriel_ssm_instruct5b_hybrid_{layer_swaped+1}ssm_leastimportant_32h_init_rand\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
