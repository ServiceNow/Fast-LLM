{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "fast_llm_path = \"/home/toolkit/dev/Fast-LLM\"\n",
    "\n",
    "# add fast_llm to the python path\n",
    "import sys\n",
    "sys.path.append(fast_llm_path)\n",
    "from apriel_hybrid.modeling_ssm_hybrid_apriel import AprielSSMHybridConfig\n",
    "from apriel_hybrid.modeling_ssm_hybrid_apriel import AprielSSMHybridModel, AprielSSMDecoderLayer, AprielSSMHybridForCausalLM\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 0.612615\n",
    "layer_scores = {\n",
    "    \"22\": 0.607389,\n",
    "    \"24\": 0.603498,\n",
    "    \"19\": 0.597907,\n",
    "    \"27\": 0.597173,\n",
    "    \"20\": 0.590442,\n",
    "    \"5\": 0.578949,\n",
    "    \"4\": 0.576852,\n",
    "    \"9\": 0.576484,\n",
    "    \"23\": 0.574833,\n",
    "    \"7\": 0.571860,\n",
    "    \"8\": 0.571790,\n",
    "    \"6\": 0.571614,\n",
    "    \"2\": 0.571330,\n",
    "    \"26\": 0.570205,\n",
    "    \"11\": 0.567128,\n",
    "    \"14\": 0.566175,\n",
    "    \"15\": 0.566076,\n",
    "    \"3\": 0.562861,\n",
    "    \"1\": 0.560154,\n",
    "    \"13\": 0.559304,\n",
    "    \"16\": 0.559017,\n",
    "    \"10\": 0.558789,\n",
    "    \"12\": 0.555186,\n",
    "    \"17\": 0.554236,\n",
    "    \"25\": 0.549215,\n",
    "    \"18\": 0.537257,\n",
    "    \"0\": 0.233085,\n",
    "}\n",
    "layer_scores = {k: base - v for k, v in layer_scores.items()}\n",
    "layer_importanfce = sorted(layer_scores.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('22', 0.005226000000000064),\n",
       " ('24', 0.009117000000000042),\n",
       " ('19', 0.014708000000000054),\n",
       " ('27', 0.015442000000000067),\n",
       " ('20', 0.022173),\n",
       " ('5', 0.033665999999999974),\n",
       " ('4', 0.03576299999999999),\n",
       " ('9', 0.036131000000000024),\n",
       " ('23', 0.03778199999999998),\n",
       " ('7', 0.040754999999999986),\n",
       " ('8', 0.040825),\n",
       " ('6', 0.041001000000000065),\n",
       " ('2', 0.041285000000000016),\n",
       " ('26', 0.04241000000000006),\n",
       " ('11', 0.045487000000000055),\n",
       " ('14', 0.04644000000000004),\n",
       " ('15', 0.046539),\n",
       " ('3', 0.049754000000000076),\n",
       " ('1', 0.05246099999999998),\n",
       " ('13', 0.053311),\n",
       " ('16', 0.053598000000000035),\n",
       " ('10', 0.05382600000000004),\n",
       " ('12', 0.05742900000000006),\n",
       " ('17', 0.05837900000000007),\n",
       " ('25', 0.06340000000000001),\n",
       " ('18', 0.07535800000000004),\n",
       " ('0', 0.37953000000000003)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importanfce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hybrid with any number of SSM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ServiceNow-AI/Apriel-5B-Instruct:\n",
      "- configuration_apriel.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"ServiceNow-AI/Apriel-5B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "device = \"cuda\"\n",
    "n_hybrid = 1\n",
    "\n",
    "index_swaped = []\n",
    "hybrid_block_layout = [\"t\"] * config.num_hidden_layers\n",
    "for i in range(n_hybrid):\n",
    "    hybrid_block_layout[int(layer_importanfce[i][0])] = \"m2d\"\n",
    "    index_swaped.append(int(layer_importanfce[i][0]))\n",
    "\n",
    "hybrdif_apriel_config = AprielSSMHybridConfig(**config.to_dict(),\n",
    "                                              hybrid_block_layout=hybrid_block_layout,\n",
    "                                              ssm_cfg={\n",
    "                                                  \"d_state\": 64,\n",
    "                                                  \"n_v_heads\": 24,\n",
    "                                                  \"n_qk_heads\": 24,\n",
    "                                                  \"expand\": 1,\n",
    "                                                  \"chunk_size\": 128,\n",
    "                                                  \"activation\": \"identity\",\n",
    "                                                  \"bias\": False,\n",
    "                                                  \"d_inner\": 24 * 128,  # num_heads * head_dim\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrdif_apriel_config.hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AprielSSMHybridForCausalLM(\n",
       "  (model): AprielSSMHybridModel(\n",
       "    (embed_tokens): Embedding(131072, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22): AprielSSMDecoderLayer(\n",
       "        (mixer): DiscreteMamba2(\n",
       "          (in_proj): Linear(in_features=4096, out_features=9240, bias=False)\n",
       "          (conv1d): Conv1d(6144, 6144, kernel_size=(4,), stride=(1,), padding=(3,), groups=6144)\n",
       "          (act): Identity()\n",
       "          (out_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (23-27): 5 x AprielDecoderLayer(\n",
       "        (self_attn): AprielAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): AprielMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): AprielRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): AprielRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_apriel_model = AprielSSMHybridForCausalLM(hybrdif_apriel_config)\n",
    "hybrid_apriel_model.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ServiceNow-AI/Apriel-5B-Instruct:\n",
      "- modeling_apriel.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 12.97it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "apriel_model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "apriel_state_dict = apriel_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing, unexpected = hybrid_apriel_model.load_state_dict(apriel_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['model.layers.22.mixer.z_bias', 'model.layers.22.mixer.D', 'model.layers.22.mixer.in_proj.weight', 'model.layers.22.mixer.conv1d.weight', 'model.layers.22.mixer.conv1d.bias', 'model.layers.22.mixer.out_proj.weight']\n",
      "Unexpected keys: ['model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "# unexpected will contain keys from the SSM layers we added\n",
    "print(\"Missing keys:\", missing)\n",
    "# unexpected will contain keys from the transformer layers we replaced\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid model saved to /mnt/checkpoints/ssm/iterative_hybrids_5b/apriel_ssm_instruct5b_hybrid_23ssm_leastimportant_32h_init_rand\n"
     ]
    }
   ],
   "source": [
    "# save the hybrid model\n",
    "output_path = \"/mnt/checkpoints/ssm/iterative_hybrids_5b\"\n",
    "assert len(index_swaped) == 1\n",
    "layer_swaped = index_swaped[0]\n",
    "hybrid_apriel_model.save_pretrained(\n",
    "        f\"{output_path}/apriel_ssm_instruct5b_hybrid_{layer_swaped+1}ssm_leastimportant_32h_init_rand\"\n",
    "    )\n",
    "print(f\"Hybrid model saved to {output_path}/apriel_ssm_instruct5b_hybrid_{layer_swaped+1}ssm_leastimportant_32h_init_rand\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hymba121_py39_cuda124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
