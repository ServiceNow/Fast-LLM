{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toolkit/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "# from transformers import MistralForCausalLM\n",
    "# from fast_llm.models.ssm.external.apriel_15b_hybrid.configuration_ssm_hybrid_apriel15b import AprielSSMHybridConfig\n",
    "# from fast_llm.models.ssm.external.apriel_15b_hybrid.modeling_ssm_hybrid_apriel15b import AprielSSMHybridForCausalLM\n",
    "# autoreload changes to the code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/mnt/checkpoints/fast_llm_exp/slam_ssm_distill/15bch-ifrhyb20l32h-bs768-lr0.0003-lrs0-0-0-0-sl4096_ti1000_lm2/export/apriel_ssm_thinker_hybrid/1000\"\n",
    "# AutoConfig.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/mnt/checkpoints/fast_llm_exp/slam_ssm_distill/15bch-ifrhyb20l32h-bs768-lr0.0003-lrs0-0-0-0-sl4096_ti1000_lm2/export/apriel_ssm_thinker_hybrid/1000\"\n",
    "# m = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path, trust_remote_code=True,\n",
    "#     config=AutoConfig.from_pretrained(model_path, trust_remote_code=True),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slam 15B upcycled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lead the weights of https://huggingface.co/ServiceNow-AI/Slam-15B-Upcycled/ into Thiked modeling, it shoudl work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/toolkit/dev/fml-ops/__oo_playground\")\n",
    "from results_analysis.results_loader import ResultsLoader\n",
    "layer_importance_path = \"/mnt/evaluations/training_evaluation/model_runs/lm_eval_runner/apriel_ssm_importance/\"\n",
    "results_loader = ResultsLoader(layer_importance_path)\n",
    "\n",
    "results_loader.deserialize_results()\n",
    "results_df = results_loader.to_df()\n",
    "results_df[\"layer_index\"] = results_df.apply(lambda row: int(row[\"model_name_sanitized\"].split(\"_\")[-1] if \"layers_\" in row[\"model_name_sanitized\"] else -1), axis=1)\n",
    "results_df = results_df[results_df[\"metric\"] == \"acc_norm\"]\n",
    "columns_to_keep = [\"layer_index\", \"metric_value\"]\n",
    "results_df = results_df[columns_to_keep]\n",
    "layer_importance = results_df.groupby(\"layer_index\").mean()\n",
    "layer_importance = layer_importance.sort_values(by=\"metric_value\", ascending=False).reset_index()\n",
    "layer_importance = layer_importance[layer_importance[\"layer_index\"]!= -1]\n",
    "layer_importance = list(layer_importance[\"layer_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22,\n",
       " 25,\n",
       " 20,\n",
       " 31,\n",
       " 29,\n",
       " 46,\n",
       " 23,\n",
       " 26,\n",
       " 33,\n",
       " 24,\n",
       " 47,\n",
       " 27,\n",
       " 21,\n",
       " 41,\n",
       " 17,\n",
       " 18,\n",
       " 34,\n",
       " 42,\n",
       " 44,\n",
       " 30,\n",
       " 16,\n",
       " 8,\n",
       " 43,\n",
       " 35,\n",
       " 19,\n",
       " 38,\n",
       " 15,\n",
       " 28,\n",
       " 32,\n",
       " 45,\n",
       " 37,\n",
       " 40,\n",
       " 7,\n",
       " 36,\n",
       " 13,\n",
       " 10,\n",
       " 5,\n",
       " 39,\n",
       " 6,\n",
       " 14,\n",
       " 4,\n",
       " 12,\n",
       " 9,\n",
       " 48,\n",
       " 1,\n",
       " 3,\n",
       " 11,\n",
       " 49,\n",
       " 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_thinker = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "n_ssm = 25\n",
    "\n",
    "config_thinker = AutoConfig.from_pretrained(path_thinker)\n",
    "hybrid_block_layout = [\"t\"] * config_thinker.num_hidden_layers\n",
    "\n",
    "for i in range(n_ssm):\n",
    "    hybrid_block_layout[layer_importance[i]] = \"m2d\"\n",
    "\n",
    "config_hybrid = AprielSSMHybridConfig(\n",
    "    **config_thinker.to_dict(),\n",
    "    hybrid_block_layout=hybrid_block_layout,\n",
    "    ssm_cfg = {\n",
    "            \"d_state\": 64,\n",
    "            \"n_v_heads\": 32,\n",
    "            \"n_qk_heads\": 32,\n",
    "            \"expand\": 1,\n",
    "            \"chunk_size\": 128,\n",
    "            \"activation\": \"identity\",\n",
    "            \"bias\": False,\n",
    "            \"d_conv\": 4,\n",
    "            \"d_inner\": 32 * 128\n",
    "        }\n",
    ")\n",
    "model_hybrid = AprielSSMHybridForCausalLM(config_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type mistral. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.52it/s]\n"
     ]
    }
   ],
   "source": [
    "path_base = \"/mnt/checkpoints/upstream/Slam-15B-Upcycled\"\n",
    "# path_base = \"/mnt/checkpoints/upstream/Slam-15B-Upcycled-mistral\"\n",
    "model_base = MistralForCausalLM.from_pretrained(path_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.layers.8.mixer.z_bias', 'model.layers.8.mixer.D', 'model.layers.8.mixer.in_proj.weight', 'model.layers.8.mixer.conv1d.weight', 'model.layers.8.mixer.conv1d.bias', 'model.layers.8.mixer.out_proj.weight', 'model.layers.16.mixer.z_bias', 'model.layers.16.mixer.D', 'model.layers.16.mixer.in_proj.weight', 'model.layers.16.mixer.conv1d.weight', 'model.layers.16.mixer.conv1d.bias', 'model.layers.16.mixer.out_proj.weight', 'model.layers.17.mixer.z_bias', 'model.layers.17.mixer.D', 'model.layers.17.mixer.in_proj.weight', 'model.layers.17.mixer.conv1d.weight', 'model.layers.17.mixer.conv1d.bias', 'model.layers.17.mixer.out_proj.weight', 'model.layers.18.mixer.z_bias', 'model.layers.18.mixer.D', 'model.layers.18.mixer.in_proj.weight', 'model.layers.18.mixer.conv1d.weight', 'model.layers.18.mixer.conv1d.bias', 'model.layers.18.mixer.out_proj.weight', 'model.layers.19.mixer.z_bias', 'model.layers.19.mixer.D', 'model.layers.19.mixer.in_proj.weight', 'model.layers.19.mixer.conv1d.weight', 'model.layers.19.mixer.conv1d.bias', 'model.layers.19.mixer.out_proj.weight', 'model.layers.20.mixer.z_bias', 'model.layers.20.mixer.D', 'model.layers.20.mixer.in_proj.weight', 'model.layers.20.mixer.conv1d.weight', 'model.layers.20.mixer.conv1d.bias', 'model.layers.20.mixer.out_proj.weight', 'model.layers.21.mixer.z_bias', 'model.layers.21.mixer.D', 'model.layers.21.mixer.in_proj.weight', 'model.layers.21.mixer.conv1d.weight', 'model.layers.21.mixer.conv1d.bias', 'model.layers.21.mixer.out_proj.weight', 'model.layers.22.mixer.z_bias', 'model.layers.22.mixer.D', 'model.layers.22.mixer.in_proj.weight', 'model.layers.22.mixer.conv1d.weight', 'model.layers.22.mixer.conv1d.bias', 'model.layers.22.mixer.out_proj.weight', 'model.layers.23.mixer.z_bias', 'model.layers.23.mixer.D', 'model.layers.23.mixer.in_proj.weight', 'model.layers.23.mixer.conv1d.weight', 'model.layers.23.mixer.conv1d.bias', 'model.layers.23.mixer.out_proj.weight', 'model.layers.24.mixer.z_bias', 'model.layers.24.mixer.D', 'model.layers.24.mixer.in_proj.weight', 'model.layers.24.mixer.conv1d.weight', 'model.layers.24.mixer.conv1d.bias', 'model.layers.24.mixer.out_proj.weight', 'model.layers.25.mixer.z_bias', 'model.layers.25.mixer.D', 'model.layers.25.mixer.in_proj.weight', 'model.layers.25.mixer.conv1d.weight', 'model.layers.25.mixer.conv1d.bias', 'model.layers.25.mixer.out_proj.weight', 'model.layers.26.mixer.z_bias', 'model.layers.26.mixer.D', 'model.layers.26.mixer.in_proj.weight', 'model.layers.26.mixer.conv1d.weight', 'model.layers.26.mixer.conv1d.bias', 'model.layers.26.mixer.out_proj.weight', 'model.layers.27.mixer.z_bias', 'model.layers.27.mixer.D', 'model.layers.27.mixer.in_proj.weight', 'model.layers.27.mixer.conv1d.weight', 'model.layers.27.mixer.conv1d.bias', 'model.layers.27.mixer.out_proj.weight', 'model.layers.29.mixer.z_bias', 'model.layers.29.mixer.D', 'model.layers.29.mixer.in_proj.weight', 'model.layers.29.mixer.conv1d.weight', 'model.layers.29.mixer.conv1d.bias', 'model.layers.29.mixer.out_proj.weight', 'model.layers.30.mixer.z_bias', 'model.layers.30.mixer.D', 'model.layers.30.mixer.in_proj.weight', 'model.layers.30.mixer.conv1d.weight', 'model.layers.30.mixer.conv1d.bias', 'model.layers.30.mixer.out_proj.weight', 'model.layers.31.mixer.z_bias', 'model.layers.31.mixer.D', 'model.layers.31.mixer.in_proj.weight', 'model.layers.31.mixer.conv1d.weight', 'model.layers.31.mixer.conv1d.bias', 'model.layers.31.mixer.out_proj.weight', 'model.layers.33.mixer.z_bias', 'model.layers.33.mixer.D', 'model.layers.33.mixer.in_proj.weight', 'model.layers.33.mixer.conv1d.weight', 'model.layers.33.mixer.conv1d.bias', 'model.layers.33.mixer.out_proj.weight', 'model.layers.34.mixer.z_bias', 'model.layers.34.mixer.D', 'model.layers.34.mixer.in_proj.weight', 'model.layers.34.mixer.conv1d.weight', 'model.layers.34.mixer.conv1d.bias', 'model.layers.34.mixer.out_proj.weight', 'model.layers.35.mixer.z_bias', 'model.layers.35.mixer.D', 'model.layers.35.mixer.in_proj.weight', 'model.layers.35.mixer.conv1d.weight', 'model.layers.35.mixer.conv1d.bias', 'model.layers.35.mixer.out_proj.weight', 'model.layers.41.mixer.z_bias', 'model.layers.41.mixer.D', 'model.layers.41.mixer.in_proj.weight', 'model.layers.41.mixer.conv1d.weight', 'model.layers.41.mixer.conv1d.bias', 'model.layers.41.mixer.out_proj.weight', 'model.layers.42.mixer.z_bias', 'model.layers.42.mixer.D', 'model.layers.42.mixer.in_proj.weight', 'model.layers.42.mixer.conv1d.weight', 'model.layers.42.mixer.conv1d.bias', 'model.layers.42.mixer.out_proj.weight', 'model.layers.43.mixer.z_bias', 'model.layers.43.mixer.D', 'model.layers.43.mixer.in_proj.weight', 'model.layers.43.mixer.conv1d.weight', 'model.layers.43.mixer.conv1d.bias', 'model.layers.43.mixer.out_proj.weight', 'model.layers.44.mixer.z_bias', 'model.layers.44.mixer.D', 'model.layers.44.mixer.in_proj.weight', 'model.layers.44.mixer.conv1d.weight', 'model.layers.44.mixer.conv1d.bias', 'model.layers.44.mixer.out_proj.weight', 'model.layers.46.mixer.z_bias', 'model.layers.46.mixer.D', 'model.layers.46.mixer.in_proj.weight', 'model.layers.46.mixer.conv1d.weight', 'model.layers.46.mixer.conv1d.bias', 'model.layers.46.mixer.out_proj.weight', 'model.layers.47.mixer.z_bias', 'model.layers.47.mixer.D', 'model.layers.47.mixer.in_proj.weight', 'model.layers.47.mixer.conv1d.weight', 'model.layers.47.mixer.conv1d.bias', 'model.layers.47.mixer.out_proj.weight'], unexpected_keys=['model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.41.self_attn.q_proj.weight', 'model.layers.41.self_attn.k_proj.weight', 'model.layers.41.self_attn.v_proj.weight', 'model.layers.41.self_attn.o_proj.weight', 'model.layers.42.self_attn.q_proj.weight', 'model.layers.42.self_attn.k_proj.weight', 'model.layers.42.self_attn.v_proj.weight', 'model.layers.42.self_attn.o_proj.weight', 'model.layers.43.self_attn.q_proj.weight', 'model.layers.43.self_attn.k_proj.weight', 'model.layers.43.self_attn.v_proj.weight', 'model.layers.43.self_attn.o_proj.weight', 'model.layers.44.self_attn.q_proj.weight', 'model.layers.44.self_attn.k_proj.weight', 'model.layers.44.self_attn.v_proj.weight', 'model.layers.44.self_attn.o_proj.weight', 'model.layers.46.self_attn.q_proj.weight', 'model.layers.46.self_attn.k_proj.weight', 'model.layers.46.self_attn.v_proj.weight', 'model.layers.46.self_attn.o_proj.weight', 'model.layers.47.self_attn.q_proj.weight', 'model.layers.47.self_attn.k_proj.weight', 'model.layers.47.self_attn.v_proj.weight', 'model.layers.47.self_attn.o_proj.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hybrid.load_state_dict(model_base.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_base15b_hybrid_25ssm_leastimportant_32h_init_rand\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinker 15B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.configuration_ssm_hybrid_apriel15b import AprielSSMHybridConfig\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.modeling_ssm_hybrid_apriel15b import AprielThinkerSSMHybridForCausalLM\n",
    "\n",
    "# enable file reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repalce specific layer from MOHAWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:03<00:00,  3.98it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 16.26it/s]\n"
     ]
    }
   ],
   "source": [
    "l3checkpoint = \"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_rand\" #/mnt/checkpoints/ssm/iterative_hybrids_only_new_layer_train/apriel_ssm_thinker15b_hybrid_3ssm_leastimportant_32h_init_rand\"\n",
    "model_l3 = AprielThinkerSSMHybridForCausalLM.from_pretrained(l3checkpoint, device_map=\"cpu\")\n",
    "mohawk_checkpoint = \"/mnt/checkpoints_fml/ssm/final_stitched_model_L0-49_resaved\"\n",
    "model_mohawk = AprielThinkerSSMHybridForCausalLM.from_pretrained(mohawk_checkpoint, device_map=\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l3.config.hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.0.mixer.z_bias', 'model.layers.0.mixer.D', 'model.layers.0.mixer.in_proj.weight', 'model.layers.0.mixer.conv1d.weight', 'model.layers.0.mixer.conv1d.bias', 'model.layers.0.mixer.out_proj.weight', 'model.layers.1.mixer.z_bias', 'model.layers.1.mixer.D', 'model.layers.1.mixer.in_proj.weight', 'model.layers.1.mixer.conv1d.weight', 'model.layers.1.mixer.conv1d.bias', 'model.layers.1.mixer.out_proj.weight', 'model.layers.2.mixer.z_bias', 'model.layers.2.mixer.D', 'model.layers.2.mixer.in_proj.weight', 'model.layers.2.mixer.conv1d.weight', 'model.layers.2.mixer.conv1d.bias', 'model.layers.2.mixer.out_proj.weight', 'model.layers.3.mixer.z_bias', 'model.layers.3.mixer.D', 'model.layers.3.mixer.in_proj.weight', 'model.layers.3.mixer.conv1d.weight', 'model.layers.3.mixer.conv1d.bias', 'model.layers.3.mixer.out_proj.weight', 'model.layers.4.mixer.z_bias', 'model.layers.4.mixer.D', 'model.layers.4.mixer.in_proj.weight', 'model.layers.4.mixer.conv1d.weight', 'model.layers.4.mixer.conv1d.bias', 'model.layers.4.mixer.out_proj.weight', 'model.layers.5.mixer.z_bias', 'model.layers.5.mixer.D', 'model.layers.5.mixer.in_proj.weight', 'model.layers.5.mixer.conv1d.weight', 'model.layers.5.mixer.conv1d.bias', 'model.layers.5.mixer.out_proj.weight', 'model.layers.6.mixer.z_bias', 'model.layers.6.mixer.D', 'model.layers.6.mixer.in_proj.weight', 'model.layers.6.mixer.conv1d.weight', 'model.layers.6.mixer.conv1d.bias', 'model.layers.6.mixer.out_proj.weight', 'model.layers.7.mixer.z_bias', 'model.layers.7.mixer.D', 'model.layers.7.mixer.in_proj.weight', 'model.layers.7.mixer.conv1d.weight', 'model.layers.7.mixer.conv1d.bias', 'model.layers.7.mixer.out_proj.weight', 'model.layers.8.mixer.z_bias', 'model.layers.8.mixer.D', 'model.layers.8.mixer.in_proj.weight', 'model.layers.8.mixer.conv1d.weight', 'model.layers.8.mixer.conv1d.bias', 'model.layers.8.mixer.out_proj.weight', 'model.layers.9.mixer.z_bias', 'model.layers.9.mixer.D', 'model.layers.9.mixer.in_proj.weight', 'model.layers.9.mixer.conv1d.weight', 'model.layers.9.mixer.conv1d.bias', 'model.layers.9.mixer.out_proj.weight', 'model.layers.10.mixer.z_bias', 'model.layers.10.mixer.D', 'model.layers.10.mixer.in_proj.weight', 'model.layers.10.mixer.conv1d.weight', 'model.layers.10.mixer.conv1d.bias', 'model.layers.10.mixer.out_proj.weight', 'model.layers.11.mixer.z_bias', 'model.layers.11.mixer.D', 'model.layers.11.mixer.in_proj.weight', 'model.layers.11.mixer.conv1d.weight', 'model.layers.11.mixer.conv1d.bias', 'model.layers.11.mixer.out_proj.weight', 'model.layers.12.mixer.z_bias', 'model.layers.12.mixer.D', 'model.layers.12.mixer.in_proj.weight', 'model.layers.12.mixer.conv1d.weight', 'model.layers.12.mixer.conv1d.bias', 'model.layers.12.mixer.out_proj.weight', 'model.layers.13.mixer.z_bias', 'model.layers.13.mixer.D', 'model.layers.13.mixer.in_proj.weight', 'model.layers.13.mixer.conv1d.weight', 'model.layers.13.mixer.conv1d.bias', 'model.layers.13.mixer.out_proj.weight', 'model.layers.14.mixer.z_bias', 'model.layers.14.mixer.D', 'model.layers.14.mixer.in_proj.weight', 'model.layers.14.mixer.conv1d.weight', 'model.layers.14.mixer.conv1d.bias', 'model.layers.14.mixer.out_proj.weight', 'model.layers.15.mixer.z_bias', 'model.layers.15.mixer.D', 'model.layers.15.mixer.in_proj.weight', 'model.layers.15.mixer.conv1d.weight', 'model.layers.15.mixer.conv1d.bias', 'model.layers.15.mixer.out_proj.weight', 'model.layers.16.mixer.z_bias', 'model.layers.16.mixer.D', 'model.layers.16.mixer.in_proj.weight', 'model.layers.16.mixer.conv1d.weight', 'model.layers.16.mixer.conv1d.bias', 'model.layers.16.mixer.out_proj.weight', 'model.layers.17.mixer.z_bias', 'model.layers.17.mixer.D', 'model.layers.17.mixer.in_proj.weight', 'model.layers.17.mixer.conv1d.weight', 'model.layers.17.mixer.conv1d.bias', 'model.layers.17.mixer.out_proj.weight', 'model.layers.18.mixer.z_bias', 'model.layers.18.mixer.D', 'model.layers.18.mixer.in_proj.weight', 'model.layers.18.mixer.conv1d.weight', 'model.layers.18.mixer.conv1d.bias', 'model.layers.18.mixer.out_proj.weight', 'model.layers.19.mixer.z_bias', 'model.layers.19.mixer.D', 'model.layers.19.mixer.in_proj.weight', 'model.layers.19.mixer.conv1d.weight', 'model.layers.19.mixer.conv1d.bias', 'model.layers.19.mixer.out_proj.weight', 'model.layers.20.mixer.z_bias', 'model.layers.20.mixer.D', 'model.layers.20.mixer.in_proj.weight', 'model.layers.20.mixer.conv1d.weight', 'model.layers.20.mixer.conv1d.bias', 'model.layers.20.mixer.out_proj.weight', 'model.layers.21.mixer.z_bias', 'model.layers.21.mixer.D', 'model.layers.21.mixer.in_proj.weight', 'model.layers.21.mixer.conv1d.weight', 'model.layers.21.mixer.conv1d.bias', 'model.layers.21.mixer.out_proj.weight', 'model.layers.22.mixer.z_bias', 'model.layers.22.mixer.D', 'model.layers.22.mixer.in_proj.weight', 'model.layers.22.mixer.conv1d.weight', 'model.layers.22.mixer.conv1d.bias', 'model.layers.22.mixer.out_proj.weight', 'model.layers.23.mixer.z_bias', 'model.layers.23.mixer.D', 'model.layers.23.mixer.in_proj.weight', 'model.layers.23.mixer.conv1d.weight', 'model.layers.23.mixer.conv1d.bias', 'model.layers.23.mixer.out_proj.weight', 'model.layers.24.mixer.z_bias', 'model.layers.24.mixer.D', 'model.layers.24.mixer.in_proj.weight', 'model.layers.24.mixer.conv1d.weight', 'model.layers.24.mixer.conv1d.bias', 'model.layers.24.mixer.out_proj.weight', 'model.layers.25.mixer.z_bias', 'model.layers.25.mixer.D', 'model.layers.25.mixer.in_proj.weight', 'model.layers.25.mixer.conv1d.weight', 'model.layers.25.mixer.conv1d.bias', 'model.layers.25.mixer.out_proj.weight', 'model.layers.26.mixer.z_bias', 'model.layers.26.mixer.D', 'model.layers.26.mixer.in_proj.weight', 'model.layers.26.mixer.conv1d.weight', 'model.layers.26.mixer.conv1d.bias', 'model.layers.26.mixer.out_proj.weight', 'model.layers.27.mixer.z_bias', 'model.layers.27.mixer.D', 'model.layers.27.mixer.in_proj.weight', 'model.layers.27.mixer.conv1d.weight', 'model.layers.27.mixer.conv1d.bias', 'model.layers.27.mixer.out_proj.weight', 'model.layers.28.mixer.z_bias', 'model.layers.28.mixer.D', 'model.layers.28.mixer.in_proj.weight', 'model.layers.28.mixer.conv1d.weight', 'model.layers.28.mixer.conv1d.bias', 'model.layers.28.mixer.out_proj.weight', 'model.layers.29.mixer.z_bias', 'model.layers.29.mixer.D', 'model.layers.29.mixer.in_proj.weight', 'model.layers.29.mixer.conv1d.weight', 'model.layers.29.mixer.conv1d.bias', 'model.layers.29.mixer.out_proj.weight', 'model.layers.30.mixer.z_bias', 'model.layers.30.mixer.D', 'model.layers.30.mixer.in_proj.weight', 'model.layers.30.mixer.conv1d.weight', 'model.layers.30.mixer.conv1d.bias', 'model.layers.30.mixer.out_proj.weight', 'model.layers.31.mixer.z_bias', 'model.layers.31.mixer.D', 'model.layers.31.mixer.in_proj.weight', 'model.layers.31.mixer.conv1d.weight', 'model.layers.31.mixer.conv1d.bias', 'model.layers.31.mixer.out_proj.weight', 'model.layers.32.mixer.z_bias', 'model.layers.32.mixer.D', 'model.layers.32.mixer.in_proj.weight', 'model.layers.32.mixer.conv1d.weight', 'model.layers.32.mixer.conv1d.bias', 'model.layers.32.mixer.out_proj.weight', 'model.layers.33.mixer.z_bias', 'model.layers.33.mixer.D', 'model.layers.33.mixer.in_proj.weight', 'model.layers.33.mixer.conv1d.weight', 'model.layers.33.mixer.conv1d.bias', 'model.layers.33.mixer.out_proj.weight', 'model.layers.34.mixer.z_bias', 'model.layers.34.mixer.D', 'model.layers.34.mixer.in_proj.weight', 'model.layers.34.mixer.conv1d.weight', 'model.layers.34.mixer.conv1d.bias', 'model.layers.34.mixer.out_proj.weight', 'model.layers.35.mixer.z_bias', 'model.layers.35.mixer.D', 'model.layers.35.mixer.in_proj.weight', 'model.layers.35.mixer.conv1d.weight', 'model.layers.35.mixer.conv1d.bias', 'model.layers.35.mixer.out_proj.weight', 'model.layers.36.mixer.z_bias', 'model.layers.36.mixer.D', 'model.layers.36.mixer.in_proj.weight', 'model.layers.36.mixer.conv1d.weight', 'model.layers.36.mixer.conv1d.bias', 'model.layers.36.mixer.out_proj.weight', 'model.layers.37.mixer.z_bias', 'model.layers.37.mixer.D', 'model.layers.37.mixer.in_proj.weight', 'model.layers.37.mixer.conv1d.weight', 'model.layers.37.mixer.conv1d.bias', 'model.layers.37.mixer.out_proj.weight', 'model.layers.38.mixer.z_bias', 'model.layers.38.mixer.D', 'model.layers.38.mixer.in_proj.weight', 'model.layers.38.mixer.conv1d.weight', 'model.layers.38.mixer.conv1d.bias', 'model.layers.38.mixer.out_proj.weight', 'model.layers.39.mixer.z_bias', 'model.layers.39.mixer.D', 'model.layers.39.mixer.in_proj.weight', 'model.layers.39.mixer.conv1d.weight', 'model.layers.39.mixer.conv1d.bias', 'model.layers.39.mixer.out_proj.weight', 'model.layers.40.mixer.z_bias', 'model.layers.40.mixer.D', 'model.layers.40.mixer.in_proj.weight', 'model.layers.40.mixer.conv1d.weight', 'model.layers.40.mixer.conv1d.bias', 'model.layers.40.mixer.out_proj.weight', 'model.layers.41.mixer.z_bias', 'model.layers.41.mixer.D', 'model.layers.41.mixer.in_proj.weight', 'model.layers.41.mixer.conv1d.weight', 'model.layers.41.mixer.conv1d.bias', 'model.layers.41.mixer.out_proj.weight', 'model.layers.42.mixer.z_bias', 'model.layers.42.mixer.D', 'model.layers.42.mixer.in_proj.weight', 'model.layers.42.mixer.conv1d.weight', 'model.layers.42.mixer.conv1d.bias', 'model.layers.42.mixer.out_proj.weight', 'model.layers.43.mixer.z_bias', 'model.layers.43.mixer.D', 'model.layers.43.mixer.in_proj.weight', 'model.layers.43.mixer.conv1d.weight', 'model.layers.43.mixer.conv1d.bias', 'model.layers.43.mixer.out_proj.weight', 'model.layers.44.mixer.z_bias', 'model.layers.44.mixer.D', 'model.layers.44.mixer.in_proj.weight', 'model.layers.44.mixer.conv1d.weight', 'model.layers.44.mixer.conv1d.bias', 'model.layers.44.mixer.out_proj.weight', 'model.layers.45.mixer.z_bias', 'model.layers.45.mixer.D', 'model.layers.45.mixer.in_proj.weight', 'model.layers.45.mixer.conv1d.weight', 'model.layers.45.mixer.conv1d.bias', 'model.layers.45.mixer.out_proj.weight', 'model.layers.46.mixer.z_bias', 'model.layers.46.mixer.D', 'model.layers.46.mixer.in_proj.weight', 'model.layers.46.mixer.conv1d.weight', 'model.layers.46.mixer.conv1d.bias', 'model.layers.46.mixer.out_proj.weight', 'model.layers.48.mixer.z_bias', 'model.layers.48.mixer.D', 'model.layers.48.mixer.in_proj.weight', 'model.layers.48.mixer.conv1d.weight', 'model.layers.48.mixer.conv1d.bias', 'model.layers.48.mixer.out_proj.weight', 'model.layers.49.mixer.z_bias', 'model.layers.49.mixer.D', 'model.layers.49.mixer.in_proj.weight', 'model.layers.49.mixer.conv1d.weight', 'model.layers.49.mixer.conv1d.bias', 'model.layers.49.mixer.out_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# layer_ids = \"47\"\n",
    "sdm = model_mohawk.state_dict()\n",
    "# layer_sd = {k: v for k, v in sdm.items() if f\"layers.{layer_ids}\" in k}.copy()\n",
    "# r = model_l3.load_state_dict(layer_sd, strict=False)\n",
    "r = model_l3.load_state_dict(sdm, strict=False)\n",
    "print(r.unexpected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_l3.save_pretrained(\"/mnt/checkpoints/ssm/iterative_hybrids_only_new_layer_train/apriel_ssm_thinker15b_hybrid_3ssm_leastimportant_32h_init_mohawk_layer24\")\n",
    "model_l3.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_rand_mohawk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/toolkit/dev/fml-ops/__oo_playground\")\n",
    "from results_analysis.results_loader import ResultsLoader\n",
    "layer_importance_path = \"/mnt/evaluations/training_evaluation/model_runs/lm_eval_runner/apriel_ssm_importance/\"\n",
    "results_loader = ResultsLoader(layer_importance_path)\n",
    "\n",
    "results_loader.deserialize_results()\n",
    "results_df = results_loader.to_df()\n",
    "results_df[\"layer_index\"] = results_df.apply(lambda row: int(row[\"model_name_sanitized\"].split(\"_\")[-1] if \"layers_\" in row[\"model_name_sanitized\"] else -1), axis=1)\n",
    "results_df = results_df[results_df[\"metric\"] == \"acc\"]\n",
    "columns_to_keep = [\"layer_index\", \"metric_value\"]\n",
    "results_df = results_df[columns_to_keep]\n",
    "layer_importance = results_df.groupby(\"layer_index\").mean()\n",
    "layer_importance = layer_importance.sort_values(by=\"metric_value\", ascending=False).reset_index()\n",
    "layer_importance = layer_importance[layer_importance[\"layer_index\"]!= -1]\n",
    "layer_importance = list(layer_importance[\"layer_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47,\n",
       " 39,\n",
       " 24,\n",
       " 36,\n",
       " 31,\n",
       " 43,\n",
       " 32,\n",
       " 20,\n",
       " 38,\n",
       " 37,\n",
       " 30,\n",
       " 33,\n",
       " 22,\n",
       " 23,\n",
       " 40,\n",
       " 42,\n",
       " 44,\n",
       " 35,\n",
       " 41,\n",
       " 27,\n",
       " 21,\n",
       " 46,\n",
       " 45,\n",
       " 49,\n",
       " 25,\n",
       " 34,\n",
       " 29,\n",
       " 28,\n",
       " 19,\n",
       " 26,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 6,\n",
       " 11,\n",
       " 5,\n",
       " 48,\n",
       " 7,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_thinker = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "n_ssm = 1\n",
    "\n",
    "config_thinker = AutoConfig.from_pretrained(path_thinker)\n",
    "# config_thinker.num_hidden_layers = 5\n",
    "hybrid_block_layout = [\"t\"] * config_thinker.num_hidden_layers\n",
    "# hybrid_block_layout[0] = \"m2\"\n",
    "# hybrid_block_layout[0] = \"m2d\"\n",
    "\n",
    "for i in range(n_ssm):\n",
    "    hybrid_block_layout[layer_importance[i]] = \"m2d\"\n",
    "    # if i % 2 == 0:\n",
    "        # hybrid_block_layout[layer_importance[i]] = \"m2\"\n",
    "        # hybrid_block_layout[i] = \"m2\"\n",
    "\n",
    "# hybrid_block_layout = [\"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"m2d\", \"t\", \"t\", \"t\", \"m2d\", \"t\", \"t\", \"t\", \"t\", \"t\", \"m2d\", \"m2d\", \"t\", \"t\", \"t\", \"t\", \"m2d\", \"m2d\", \"m2d\", \"m2d\", \"t\", \"t\", \"t\", \"m2d\", \"t\", \"t\", \"t\", \"m2d\", \"t\", \"m2d\"]\n",
    "\n",
    "dstate = 64\n",
    "\n",
    "config_hybrid = AprielSSMHybridConfig(\n",
    "    **config_thinker.to_dict(),\n",
    "    hybrid_block_layout=hybrid_block_layout,\n",
    "    # discrete mamba2\n",
    "    ssm_cfg = {\n",
    "            \"d_state\": dstate,\n",
    "            \"n_v_heads\": 32,\n",
    "            \"n_qk_heads\": 32,\n",
    "            \"expand\": 1,\n",
    "            \"chunk_size\": 128,\n",
    "            \"activation\": \"identity\",\n",
    "            \"bias\": False,\n",
    "            \"d_conv\": 4,\n",
    "            \"d_inner\": 32 * 128,\n",
    "        }\n",
    "    # mamba 2: uses expantion nternally\n",
    "    # https://github.com/jxiw/M1/blob/537a1ca5407a786a99dc6c721873493cf8750d5e/mamba/hybrid_mamba_config.py\n",
    "    # ssm_cfg = {\n",
    "    #         \"d_state\": dstate,\n",
    "    #         \"expand\": 2,\n",
    "    #         \"d_conv\": 4,\n",
    "    #         \"d_inner\": None, # will be same as d_model * expand,\n",
    "    #         \"conv_bias\": True,\n",
    "    #         \"bias\": False,\n",
    "    #     }\n",
    ")\n",
    "model_hybrid = AprielThinkerSSMHybridForCausalLM(config_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.layers.47.mixer.z_bias', 'model.layers.47.mixer.D', 'model.layers.47.mixer.in_proj.weight', 'model.layers.47.mixer.conv1d.weight', 'model.layers.47.mixer.conv1d.bias', 'model.layers.47.mixer.out_proj.weight'], unexpected_keys=['model.layers.47.self_attn.q_proj.weight', 'model.layers.47.self_attn.k_proj.weight', 'model.layers.47.self_attn.v_proj.weight', 'model.layers.47.self_attn.o_proj.weight'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_base = path_thinker\n",
    "model_base = MistralForCausalLM.from_pretrained(path_base)\n",
    "\n",
    "model_hybrid.load_state_dict(model_base.state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_32h_init_rand\") # 1 ssm\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_0th_32h_init_rand\") # 1 ssm\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_interleaved_ssm_starting0th_32h_init_rand\") # 1 ssm\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_32h_init_rand\") # 1 ssm\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker5lb_hybrid_1ssm_m2debug\") # 1 ssm\n",
    "### \n",
    "# mamba2, state 64\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_rand\") # 1 ssm\n",
    "# mamba2, state 16\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_16h_init_rand\") # 1 ssm\n",
    "# # discrete mamba2, state 64\n",
    "model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_rand\") # 1 ssm \n",
    "# # discrete mamba2, state 16\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_16h_init_rand\") # 1 ssm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:40<00:00,  5.77s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "path = \"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_25ssm_leastimportant_32h_init_rand\"\n",
    "model = AprielSSMHybridForCausalLM.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:27,  5.53s/it]"
     ]
    }
   ],
   "source": [
    "path_base = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "model_base = MistralForCausalLM.from_pretrained(path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toolkit/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.configuration_ssm_hybrid_apriel15b import AprielSSMHybridConfig\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.modeling_ssm_hybrid_apriel15b import AprielThinkerSSMHybridForCausalLM, AprielSSMM2DecoderLayer\n",
    "from transformers.models.mistral.modeling_mistral import MistralDecoderLayer\n",
    "\n",
    "# enable file reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/toolkit/dev/fml-ops/__oo_playground\")\n",
    "from results_analysis.results_loader import ResultsLoader\n",
    "layer_importance_path = \"/mnt/evaluations/training_evaluation/model_runs/lm_eval_runner/apriel_ssm_importance/\"\n",
    "results_loader = ResultsLoader(layer_importance_path)\n",
    "\n",
    "results_loader.deserialize_results()\n",
    "results_df = results_loader.to_df()\n",
    "results_df[\"layer_index\"] = results_df.apply(lambda row: int(row[\"model_name_sanitized\"].split(\"_\")[-1] if \"layers_\" in row[\"model_name_sanitized\"] else -1), axis=1)\n",
    "results_df = results_df[results_df[\"metric\"] == \"acc\"]\n",
    "columns_to_keep = [\"layer_index\", \"metric_value\"]\n",
    "results_df = results_df[columns_to_keep]\n",
    "layer_importance = results_df.groupby(\"layer_index\").mean()\n",
    "layer_importance = layer_importance.sort_values(by=\"metric_value\", ascending=False).reset_index()\n",
    "layer_importance = layer_importance[layer_importance[\"layer_index\"]!= -1]\n",
    "layer_importance = list(layer_importance[\"layer_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_thinker = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "n_ssm = 1\n",
    "\n",
    "config_thinker = AutoConfig.from_pretrained(path_thinker)\n",
    "hybrid_block_layout = [\"t\"] * config_thinker.num_hidden_layers\n",
    "\n",
    "for i in range(n_ssm):\n",
    "    hybrid_block_layout[layer_importance[i]] = \"m2\"\n",
    "\n",
    "\n",
    "dstate = 16\n",
    "expand = 1\n",
    "# Calculate derived dimensions for the Mamba1 configuration\n",
    "d_model = config_thinker.hidden_size\n",
    "d_inner = 4096 # hard code to match thinker #expand * d_model\n",
    "d_xb =  1024 # hard code to match thinker #config_thinker.num_key_value_heads * (config_thinker.hidden_size // config_thinker.num_attention_heads)\n",
    "\n",
    "config_hybrid = AprielSSMHybridConfig(\n",
    "    **config_thinker.to_dict(),\n",
    "    hybrid_block_layout=hybrid_block_layout,\n",
    "    # discrete mamba2\n",
    "    # ssm_cfg = {\n",
    "    #         \"d_state\": dstate,\n",
    "    #         \"n_v_heads\": 32,\n",
    "    #         \"n_qk_heads\": 32,\n",
    "    #         \"expand\": 1,\n",
    "    #         \"chunk_size\": 128,\n",
    "    #         \"activation\": \"identity\",\n",
    "    #         \"bias\": False,\n",
    "    #         \"d_conv\": 4,\n",
    "    #         \"d_inner\": 32 * 128,\n",
    "    #     }\n",
    "    # mamba 2: uses expantion nternally\n",
    "    # https://github.com/jxiw/M1/blob/537a1ca5407a786a99dc6c721873493cf8750d5e/mamba/hybrid_mamba_config.py\n",
    "    \n",
    "    ssm_cfg = {\n",
    "            \"d_state\": dstate,\n",
    "            \"d_xb\": d_xb,\n",
    "            # \"d_model\": d_model, # will be set automatically\n",
    "            \"expand\": expand,\n",
    "            \"d_conv\": 4,\n",
    "            \"d_inner\": d_inner, # will be same as d_model * expand,\n",
    "            \"conv_bias\": True,\n",
    "            \"bias\": False,\n",
    "        }\n",
    ")\n",
    "model_hybrid = AprielThinkerSSMHybridForCausalLM(config_hybrid)\n",
    "# transformer = AutoModelForCausalLM.from_pretrained(path_thinker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.layers.47.mixer.A_log', 'model.layers.47.mixer.D', 'model.layers.47.mixer.conv1d.weight', 'model.layers.47.mixer.conv1d.bias', 'model.layers.47.mixer.in_proj.weight', 'model.layers.47.mixer.dt_proj.weight', 'model.layers.47.mixer.dt_proj.bias', 'model.layers.47.mixer.out_proj.weight'], unexpected_keys=['model.layers.47.self_attn.q_proj.weight', 'model.layers.47.self_attn.k_proj.weight', 'model.layers.47.self_attn.v_proj.weight', 'model.layers.47.self_attn.o_proj.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load state dict into hybrid model from Thinker\n",
    "model_base = MistralForCausalLM.from_pretrained(path_thinker)\n",
    "model_hybrid.load_state_dict(model_base.state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### \n",
    "# mamba2, state 64\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_rand\") # 1 ssm\n",
    "# mamba2, state 16\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_16h_init_rand\") # 1 ssm\n",
    "\n",
    "# mamba2, state 16, expand 1, i.e. same as M1, but with discrete mamba2\n",
    "model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_16hexp1_init_rand\") # 1 ssm\n",
    "\n",
    "# # discrete mamba2, state 64\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_rand\") # 1 ssm \n",
    "# # discrete mamba2, state 16\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2d_16h_init_rand\") # 1 ssm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_layers(transformer, mamba_config, hybrid_block_layout, init_with_kqvo, attn_bias, torch_dtype):\n",
    "    config = transformer.config\n",
    "    embed_dim = config.hidden_size\n",
    "    num_heads = config.num_attention_heads\n",
    "    num_heads_kv = config.num_key_value_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "    q_dim = head_dim * num_heads\n",
    "    kv_dim = head_dim * num_heads_kv\n",
    "\n",
    "    for layer_idx, type in enumerate(hybrid_block_layout):\n",
    "        print(\"Converting layer %d...\", layer_idx)\n",
    "        # Fetch the layer module for easier access\n",
    "        layer_module = transformer.model.layers._modules[f\"{layer_idx}\"]\n",
    "        if type == \"t\":\n",
    "            print(\"Skipping transformer layer %d...\" % layer_idx)\n",
    "        elif type == \"m2\":\n",
    "            print(\"Converting layer %d...\" % layer_idx)\n",
    "            # Use MambaDecoderLayer for the remaining layers\n",
    "            mamba_encoder = AprielSSMM2DecoderLayer(\n",
    "                mamba_config,\n",
    "                layer_idx,\n",
    "                device=\"cpu\",\n",
    "                dtype=torch_dtype,\n",
    "            )\n",
    "            \n",
    "            mamba_encoder.mlp.load_state_dict(layer_module.mlp.state_dict())\n",
    "            mamba_encoder.input_layernorm.load_state_dict(layer_module.input_layernorm.state_dict())\n",
    "            mamba_encoder.post_attention_layernorm.load_state_dict(layer_module.post_attention_layernorm.state_dict())\n",
    "            mamba_encoder.mixer.out_proj.load_state_dict(layer_module.self_attn.o_proj.state_dict())\n",
    "\n",
    "            if init_with_kqvo:\n",
    "                # Copy weights: [z, x, B, C, dt], x -> v, B -> k, C -> q\n",
    "                mamba_encoder.mixer.in_proj.weight.data[\n",
    "                    mamba_config.ssm_cfg[\"d_inner\"] : mamba_config.ssm_cfg[\"d_inner\"] + mamba_config.ssm_cfg[\"d_xb\"], :\n",
    "                ].copy_(layer_module.self_attn.v_proj.weight.data)\n",
    "                mamba_encoder.mixer.in_proj.weight.data[\n",
    "                    mamba_config.ssm_cfg[\"d_inner\"] + mamba_config.ssm_cfg[\"d_xb\"] : mamba_config.ssm_cfg[\"d_inner\"] + 2 * mamba_config.ssm_cfg[\"d_xb\"], :\n",
    "                ].copy_(layer_module.self_attn.k_proj.weight.data)\n",
    "                mamba_encoder.mixer.in_proj.weight.data[\n",
    "                    mamba_config.ssm_cfg[\"d_inner\"] + 2 * mamba_config.ssm_cfg[\"d_xb\"] : 2 * mamba_config.ssm_cfg[\"d_inner\"] + 2 * mamba_config.ssm_cfg[\"d_xb\"], :\n",
    "                ].copy_(layer_module.self_attn.q_proj.weight.data)\n",
    "\n",
    "                print(\"Init Mamba using Attention\")\n",
    "\n",
    "            transformer.model.layers[layer_idx] = mamba_encoder\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid layer type: {type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:03<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting layer %d... 0\n",
      "Skipping transformer layer 0...\n",
      "Converting layer %d... 1\n",
      "Skipping transformer layer 1...\n",
      "Converting layer %d... 2\n",
      "Skipping transformer layer 2...\n",
      "Converting layer %d... 3\n",
      "Skipping transformer layer 3...\n",
      "Converting layer %d... 4\n",
      "Skipping transformer layer 4...\n",
      "Converting layer %d... 5\n",
      "Skipping transformer layer 5...\n",
      "Converting layer %d... 6\n",
      "Skipping transformer layer 6...\n",
      "Converting layer %d... 7\n",
      "Skipping transformer layer 7...\n",
      "Converting layer %d... 8\n",
      "Skipping transformer layer 8...\n",
      "Converting layer %d... 9\n",
      "Skipping transformer layer 9...\n",
      "Converting layer %d... 10\n",
      "Skipping transformer layer 10...\n",
      "Converting layer %d... 11\n",
      "Skipping transformer layer 11...\n",
      "Converting layer %d... 12\n",
      "Skipping transformer layer 12...\n",
      "Converting layer %d... 13\n",
      "Skipping transformer layer 13...\n",
      "Converting layer %d... 14\n",
      "Skipping transformer layer 14...\n",
      "Converting layer %d... 15\n",
      "Skipping transformer layer 15...\n",
      "Converting layer %d... 16\n",
      "Skipping transformer layer 16...\n",
      "Converting layer %d... 17\n",
      "Skipping transformer layer 17...\n",
      "Converting layer %d... 18\n",
      "Skipping transformer layer 18...\n",
      "Converting layer %d... 19\n",
      "Skipping transformer layer 19...\n",
      "Converting layer %d... 20\n",
      "Skipping transformer layer 20...\n",
      "Converting layer %d... 21\n",
      "Skipping transformer layer 21...\n",
      "Converting layer %d... 22\n",
      "Skipping transformer layer 22...\n",
      "Converting layer %d... 23\n",
      "Skipping transformer layer 23...\n",
      "Converting layer %d... 24\n",
      "Skipping transformer layer 24...\n",
      "Converting layer %d... 25\n",
      "Skipping transformer layer 25...\n",
      "Converting layer %d... 26\n",
      "Skipping transformer layer 26...\n",
      "Converting layer %d... 27\n",
      "Skipping transformer layer 27...\n",
      "Converting layer %d... 28\n",
      "Skipping transformer layer 28...\n",
      "Converting layer %d... 29\n",
      "Skipping transformer layer 29...\n",
      "Converting layer %d... 30\n",
      "Skipping transformer layer 30...\n",
      "Converting layer %d... 31\n",
      "Skipping transformer layer 31...\n",
      "Converting layer %d... 32\n",
      "Skipping transformer layer 32...\n",
      "Converting layer %d... 33\n",
      "Skipping transformer layer 33...\n",
      "Converting layer %d... 34\n",
      "Skipping transformer layer 34...\n",
      "Converting layer %d... 35\n",
      "Skipping transformer layer 35...\n",
      "Converting layer %d... 36\n",
      "Skipping transformer layer 36...\n",
      "Converting layer %d... 37\n",
      "Skipping transformer layer 37...\n",
      "Converting layer %d... 38\n",
      "Skipping transformer layer 38...\n",
      "Converting layer %d... 39\n",
      "Skipping transformer layer 39...\n",
      "Converting layer %d... 40\n",
      "Skipping transformer layer 40...\n",
      "Converting layer %d... 41\n",
      "Skipping transformer layer 41...\n",
      "Converting layer %d... 42\n",
      "Skipping transformer layer 42...\n",
      "Converting layer %d... 43\n",
      "Skipping transformer layer 43...\n",
      "Converting layer %d... 44\n",
      "Skipping transformer layer 44...\n",
      "Converting layer %d... 45\n",
      "Skipping transformer layer 45...\n",
      "Converting layer %d... 46\n",
      "Skipping transformer layer 46...\n",
      "Converting layer %d... 47\n",
      "Converting layer 47...\n",
      "Init Mamba using Attention\n",
      "Converting layer %d... 48\n",
      "Skipping transformer layer 48...\n",
      "Converting layer %d... 49\n",
      "Skipping transformer layer 49...\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModelForCausalLM.from_pretrained(path_thinker)\n",
    "init_with_kqvo = True\n",
    "torch_dtype = torch.bfloat16\n",
    "attn_bias = True\n",
    "convert_layers(transformer, config_hybrid, hybrid_block_layout, init_with_kqvo, attn_bias, torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mamba2, state 16, expand 1, i.e. same as M1, but with discrete mamba2 and MIL\n",
    "transformer.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_m2_16hexp1_init_mil\") # 1 ssm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
