{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of transformers_modules.1000.configuration_ssm_hybrid_apriel15b failed: Traceback (most recent call last):\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/importlib/__init__.py\", line 121, in reload\n",
      "    raise ImportError(f\"parent {parent_name!r} not in sys.modules\",\n",
      "ImportError: parent 'transformers_modules.1000' not in sys.modules\n",
      "]\n",
      "[autoreload of transformers_modules.1000.configuration_ssm_hybrid_apriel15b failed: Traceback (most recent call last):\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/importlib/__init__.py\", line 121, in reload\n",
      "    raise ImportError(f\"parent {parent_name!r} not in sys.modules\",\n",
      "ImportError: parent 'transformers_modules.1000' not in sys.modules\n",
      "]\n",
      "[autoreload of transformers_modules.1000.configuration_ssm_hybrid_apriel15b failed: Traceback (most recent call last):\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/toolkit/.conda/envs/fast_llm/lib/python3.12/importlib/__init__.py\", line 121, in reload\n",
      "    raise ImportError(f\"parent {parent_name!r} not in sys.modules\",\n",
      "ImportError: parent 'transformers_modules.1000' not in sys.modules\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "# from transformers import MistralForCausalLM\n",
    "# from fast_llm.models.ssm.external.apriel_15b_hybrid.configuration_ssm_hybrid_apriel15b import AprielSSMHybridConfig\n",
    "# from fast_llm.models.ssm.external.apriel_15b_hybrid.modeling_ssm_hybrid_apriel15b import AprielSSMHybridForCausalLM\n",
    "# autoreload changes to the code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AprielSSMHybridConfig {\n",
       "  \"architectures\": [\n",
       "    \"AprielSSMHybridForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"configuration_ssm_hybrid_apriel15b.AprielSSMHybridConfig\",\n",
       "    \"AutoModel\": \"modeling_ssm_hybrid_apriel15b.AprielSSMHybridModelForCausalLM\",\n",
       "    \"AutoModelForCausalLM\": \"modeling_ssm_hybrid_apriel15b.AprielSSMHybridForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 5120,\n",
       "  \"hybrid_block_layout\": [\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"m2d\",\n",
       "    \"t\",\n",
       "    \"m2d\"\n",
       "  ],\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"model_type\": \"apriel_ssm_thinker_hybrid\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 50,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"ssm_cfg\": {\n",
       "    \"activation\": \"identity\",\n",
       "    \"bias\": false,\n",
       "    \"chunk_size\": 128,\n",
       "    \"d_conv\": 4,\n",
       "    \"d_inner\": 4096,\n",
       "    \"d_state\": 64,\n",
       "    \"expand\": 1,\n",
       "    \"n_qk_heads\": 32,\n",
       "    \"n_v_heads\": 32\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 131072\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/mnt/checkpoints/fast_llm_exp/slam_ssm_distill/15bch-ifrhyb20l32h-bs768-lr0.0003-lrs0-0-0-0-sl4096_ti1000_lm2/export/apriel_ssm_thinker_hybrid/1000\"\n",
    "AutoConfig.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/checkpoints/fast_llm_exp/slam_ssm_distill/15bch-ifrhyb20l32h-bs768-lr0.0003-lrs0-0-0-0-sl4096_ti1000_lm2/export/apriel_ssm_thinker_hybrid/1000\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m m \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m     model_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m----> 4\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1167\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[1;32m   1166\u001b[0m         config_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    568\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    569\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/configuration_utils.py:770\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m to_remove:\n\u001b[1;32m    768\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 770\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel config \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_unused_kwargs:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config, kwargs\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/configuration_utils.py:802\u001b[0m, in \u001b[0;36mPretrainedConfig.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/configuration_utils.py:914\u001b[0m, in \u001b[0;36mPretrainedConfig.to_json_string\u001b[0;34m(self, use_diff)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03mSerializes this instance to a JSON string.\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    `str`: String containing all the attributes that make up this configuration instance in JSON format.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 914\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_diff_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    916\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/configuration_utils.py:822\u001b[0m, in \u001b[0;36mPretrainedConfig.to_diff_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    819\u001b[0m default_config_dict \u001b[38;5;241m=\u001b[39m PretrainedConfig()\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# get class specific config dict\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m class_config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_no_defaults_at_init \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    824\u001b[0m serializable_config_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# Only serialize values that differ from the default config,\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# except always keep the 'config' attribute.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/1000/configuration_ssm_hybrid_apriel15b.py:22\u001b[0m, in \u001b[0;36mAprielSSMHybridConfig.__init__\u001b[0;34m(self, hybrid_block_layout, ssm_cfg, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhybrid_block_layout \u001b[38;5;241m=\u001b[39m hybrid_block_layout\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_cfg \u001b[38;5;241m=\u001b[39m ssm_cfg \u001b[38;5;129;01mor\u001b[39;00m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_v_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_qk_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_conv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241;43m24\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m,  \u001b[38;5;66;03m# num_heads * head_dim\u001b[39;00m\n\u001b[1;32m     23\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "model_path = \"/mnt/checkpoints/fast_llm_exp/slam_ssm_distill/15bch-ifrhyb20l32h-bs768-lr0.0003-lrs0-0-0-0-sl4096_ti1000_lm2/export/apriel_ssm_thinker_hybrid/1000\"\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    "    config=AutoConfig.from_pretrained(model_path, trust_remote_code=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slam 15B upcycled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lead the weights of https://huggingface.co/ServiceNow-AI/Slam-15B-Upcycled/ into Thiked modeling, it shoudl work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/toolkit/dev/fml-ops/__oo_playground\")\n",
    "from results_analysis.results_loader import ResultsLoader\n",
    "layer_importance_path = \"/mnt/evaluations/training_evaluation/model_runs/lm_eval_runner/apriel_ssm_importance/\"\n",
    "results_loader = ResultsLoader(layer_importance_path)\n",
    "\n",
    "results_loader.deserialize_results()\n",
    "results_df = results_loader.to_df()\n",
    "results_df[\"layer_index\"] = results_df.apply(lambda row: int(row[\"model_name_sanitized\"].split(\"_\")[-1] if \"layers_\" in row[\"model_name_sanitized\"] else -1), axis=1)\n",
    "results_df = results_df[results_df[\"metric\"] == \"acc_norm\"]\n",
    "columns_to_keep = [\"layer_index\", \"metric_value\"]\n",
    "results_df = results_df[columns_to_keep]\n",
    "layer_importance = results_df.groupby(\"layer_index\").mean()\n",
    "layer_importance = layer_importance.sort_values(by=\"metric_value\", ascending=False).reset_index()\n",
    "layer_importance = layer_importance[layer_importance[\"layer_index\"]!= -1]\n",
    "layer_importance = list(layer_importance[\"layer_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22,\n",
       " 25,\n",
       " 20,\n",
       " 31,\n",
       " 29,\n",
       " 46,\n",
       " 23,\n",
       " 26,\n",
       " 33,\n",
       " 24,\n",
       " 47,\n",
       " 27,\n",
       " 21,\n",
       " 41,\n",
       " 17,\n",
       " 18,\n",
       " 34,\n",
       " 42,\n",
       " 44,\n",
       " 30,\n",
       " 16,\n",
       " 8,\n",
       " 43,\n",
       " 35,\n",
       " 19,\n",
       " 38,\n",
       " 15,\n",
       " 28,\n",
       " 32,\n",
       " 45,\n",
       " 37,\n",
       " 40,\n",
       " 7,\n",
       " 36,\n",
       " 13,\n",
       " 10,\n",
       " 5,\n",
       " 39,\n",
       " 6,\n",
       " 14,\n",
       " 4,\n",
       " 12,\n",
       " 9,\n",
       " 48,\n",
       " 1,\n",
       " 3,\n",
       " 11,\n",
       " 49,\n",
       " 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_thinker = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "n_ssm = 25\n",
    "\n",
    "config_thinker = AutoConfig.from_pretrained(path_thinker)\n",
    "hybrid_block_layout = [\"t\"] * config_thinker.num_hidden_layers\n",
    "\n",
    "for i in range(n_ssm):\n",
    "    hybrid_block_layout[layer_importance[i]] = \"m2d\"\n",
    "\n",
    "config_hybrid = AprielSSMHybridConfig(\n",
    "    **config_thinker.to_dict(),\n",
    "    hybrid_block_layout=hybrid_block_layout,\n",
    "    ssm_cfg = {\n",
    "            \"d_state\": 64,\n",
    "            \"n_v_heads\": 32,\n",
    "            \"n_qk_heads\": 32,\n",
    "            \"expand\": 1,\n",
    "            \"chunk_size\": 128,\n",
    "            \"activation\": \"identity\",\n",
    "            \"bias\": False,\n",
    "            \"d_conv\": 4,\n",
    "            \"d_inner\": 32 * 128\n",
    "        }\n",
    ")\n",
    "model_hybrid = AprielSSMHybridForCausalLM(config_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type mistral. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.52it/s]\n"
     ]
    }
   ],
   "source": [
    "path_base = \"/mnt/checkpoints/upstream/Slam-15B-Upcycled\"\n",
    "# path_base = \"/mnt/checkpoints/upstream/Slam-15B-Upcycled-mistral\"\n",
    "model_base = MistralForCausalLM.from_pretrained(path_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.layers.8.mixer.z_bias', 'model.layers.8.mixer.D', 'model.layers.8.mixer.in_proj.weight', 'model.layers.8.mixer.conv1d.weight', 'model.layers.8.mixer.conv1d.bias', 'model.layers.8.mixer.out_proj.weight', 'model.layers.16.mixer.z_bias', 'model.layers.16.mixer.D', 'model.layers.16.mixer.in_proj.weight', 'model.layers.16.mixer.conv1d.weight', 'model.layers.16.mixer.conv1d.bias', 'model.layers.16.mixer.out_proj.weight', 'model.layers.17.mixer.z_bias', 'model.layers.17.mixer.D', 'model.layers.17.mixer.in_proj.weight', 'model.layers.17.mixer.conv1d.weight', 'model.layers.17.mixer.conv1d.bias', 'model.layers.17.mixer.out_proj.weight', 'model.layers.18.mixer.z_bias', 'model.layers.18.mixer.D', 'model.layers.18.mixer.in_proj.weight', 'model.layers.18.mixer.conv1d.weight', 'model.layers.18.mixer.conv1d.bias', 'model.layers.18.mixer.out_proj.weight', 'model.layers.19.mixer.z_bias', 'model.layers.19.mixer.D', 'model.layers.19.mixer.in_proj.weight', 'model.layers.19.mixer.conv1d.weight', 'model.layers.19.mixer.conv1d.bias', 'model.layers.19.mixer.out_proj.weight', 'model.layers.20.mixer.z_bias', 'model.layers.20.mixer.D', 'model.layers.20.mixer.in_proj.weight', 'model.layers.20.mixer.conv1d.weight', 'model.layers.20.mixer.conv1d.bias', 'model.layers.20.mixer.out_proj.weight', 'model.layers.21.mixer.z_bias', 'model.layers.21.mixer.D', 'model.layers.21.mixer.in_proj.weight', 'model.layers.21.mixer.conv1d.weight', 'model.layers.21.mixer.conv1d.bias', 'model.layers.21.mixer.out_proj.weight', 'model.layers.22.mixer.z_bias', 'model.layers.22.mixer.D', 'model.layers.22.mixer.in_proj.weight', 'model.layers.22.mixer.conv1d.weight', 'model.layers.22.mixer.conv1d.bias', 'model.layers.22.mixer.out_proj.weight', 'model.layers.23.mixer.z_bias', 'model.layers.23.mixer.D', 'model.layers.23.mixer.in_proj.weight', 'model.layers.23.mixer.conv1d.weight', 'model.layers.23.mixer.conv1d.bias', 'model.layers.23.mixer.out_proj.weight', 'model.layers.24.mixer.z_bias', 'model.layers.24.mixer.D', 'model.layers.24.mixer.in_proj.weight', 'model.layers.24.mixer.conv1d.weight', 'model.layers.24.mixer.conv1d.bias', 'model.layers.24.mixer.out_proj.weight', 'model.layers.25.mixer.z_bias', 'model.layers.25.mixer.D', 'model.layers.25.mixer.in_proj.weight', 'model.layers.25.mixer.conv1d.weight', 'model.layers.25.mixer.conv1d.bias', 'model.layers.25.mixer.out_proj.weight', 'model.layers.26.mixer.z_bias', 'model.layers.26.mixer.D', 'model.layers.26.mixer.in_proj.weight', 'model.layers.26.mixer.conv1d.weight', 'model.layers.26.mixer.conv1d.bias', 'model.layers.26.mixer.out_proj.weight', 'model.layers.27.mixer.z_bias', 'model.layers.27.mixer.D', 'model.layers.27.mixer.in_proj.weight', 'model.layers.27.mixer.conv1d.weight', 'model.layers.27.mixer.conv1d.bias', 'model.layers.27.mixer.out_proj.weight', 'model.layers.29.mixer.z_bias', 'model.layers.29.mixer.D', 'model.layers.29.mixer.in_proj.weight', 'model.layers.29.mixer.conv1d.weight', 'model.layers.29.mixer.conv1d.bias', 'model.layers.29.mixer.out_proj.weight', 'model.layers.30.mixer.z_bias', 'model.layers.30.mixer.D', 'model.layers.30.mixer.in_proj.weight', 'model.layers.30.mixer.conv1d.weight', 'model.layers.30.mixer.conv1d.bias', 'model.layers.30.mixer.out_proj.weight', 'model.layers.31.mixer.z_bias', 'model.layers.31.mixer.D', 'model.layers.31.mixer.in_proj.weight', 'model.layers.31.mixer.conv1d.weight', 'model.layers.31.mixer.conv1d.bias', 'model.layers.31.mixer.out_proj.weight', 'model.layers.33.mixer.z_bias', 'model.layers.33.mixer.D', 'model.layers.33.mixer.in_proj.weight', 'model.layers.33.mixer.conv1d.weight', 'model.layers.33.mixer.conv1d.bias', 'model.layers.33.mixer.out_proj.weight', 'model.layers.34.mixer.z_bias', 'model.layers.34.mixer.D', 'model.layers.34.mixer.in_proj.weight', 'model.layers.34.mixer.conv1d.weight', 'model.layers.34.mixer.conv1d.bias', 'model.layers.34.mixer.out_proj.weight', 'model.layers.35.mixer.z_bias', 'model.layers.35.mixer.D', 'model.layers.35.mixer.in_proj.weight', 'model.layers.35.mixer.conv1d.weight', 'model.layers.35.mixer.conv1d.bias', 'model.layers.35.mixer.out_proj.weight', 'model.layers.41.mixer.z_bias', 'model.layers.41.mixer.D', 'model.layers.41.mixer.in_proj.weight', 'model.layers.41.mixer.conv1d.weight', 'model.layers.41.mixer.conv1d.bias', 'model.layers.41.mixer.out_proj.weight', 'model.layers.42.mixer.z_bias', 'model.layers.42.mixer.D', 'model.layers.42.mixer.in_proj.weight', 'model.layers.42.mixer.conv1d.weight', 'model.layers.42.mixer.conv1d.bias', 'model.layers.42.mixer.out_proj.weight', 'model.layers.43.mixer.z_bias', 'model.layers.43.mixer.D', 'model.layers.43.mixer.in_proj.weight', 'model.layers.43.mixer.conv1d.weight', 'model.layers.43.mixer.conv1d.bias', 'model.layers.43.mixer.out_proj.weight', 'model.layers.44.mixer.z_bias', 'model.layers.44.mixer.D', 'model.layers.44.mixer.in_proj.weight', 'model.layers.44.mixer.conv1d.weight', 'model.layers.44.mixer.conv1d.bias', 'model.layers.44.mixer.out_proj.weight', 'model.layers.46.mixer.z_bias', 'model.layers.46.mixer.D', 'model.layers.46.mixer.in_proj.weight', 'model.layers.46.mixer.conv1d.weight', 'model.layers.46.mixer.conv1d.bias', 'model.layers.46.mixer.out_proj.weight', 'model.layers.47.mixer.z_bias', 'model.layers.47.mixer.D', 'model.layers.47.mixer.in_proj.weight', 'model.layers.47.mixer.conv1d.weight', 'model.layers.47.mixer.conv1d.bias', 'model.layers.47.mixer.out_proj.weight'], unexpected_keys=['model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.41.self_attn.q_proj.weight', 'model.layers.41.self_attn.k_proj.weight', 'model.layers.41.self_attn.v_proj.weight', 'model.layers.41.self_attn.o_proj.weight', 'model.layers.42.self_attn.q_proj.weight', 'model.layers.42.self_attn.k_proj.weight', 'model.layers.42.self_attn.v_proj.weight', 'model.layers.42.self_attn.o_proj.weight', 'model.layers.43.self_attn.q_proj.weight', 'model.layers.43.self_attn.k_proj.weight', 'model.layers.43.self_attn.v_proj.weight', 'model.layers.43.self_attn.o_proj.weight', 'model.layers.44.self_attn.q_proj.weight', 'model.layers.44.self_attn.k_proj.weight', 'model.layers.44.self_attn.v_proj.weight', 'model.layers.44.self_attn.o_proj.weight', 'model.layers.46.self_attn.q_proj.weight', 'model.layers.46.self_attn.k_proj.weight', 'model.layers.46.self_attn.v_proj.weight', 'model.layers.46.self_attn.o_proj.weight', 'model.layers.47.self_attn.q_proj.weight', 'model.layers.47.self_attn.k_proj.weight', 'model.layers.47.self_attn.v_proj.weight', 'model.layers.47.self_attn.o_proj.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hybrid.load_state_dict(model_base.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_base15b_hybrid_25ssm_leastimportant_32h_init_rand\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinker 15B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.configuration_ssm_hybrid_apriel15b import AprielSSMHybridConfig\n",
    "from fast_llm.models.ssm.external.apriel_15b_hybrid.modeling_ssm_hybrid_apriel15b import AprielSSMHybridForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repalce specific layer from MOHAWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:28<00:00,  4.02s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 232.07it/s]\n"
     ]
    }
   ],
   "source": [
    "l3checkpoint = \"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_25ssm_leastimportant_32h_init_rand\" #/mnt/checkpoints/ssm/iterative_hybrids_only_new_layer_train/apriel_ssm_thinker15b_hybrid_3ssm_leastimportant_32h_init_rand\"\n",
    "model_l3 = AprielSSMHybridForCausalLM.from_pretrained(l3checkpoint, device_map=\"cpu\")\n",
    "mohawk_checkpoint = \"/mnt/checkpoints_fml/ssm/final_stitched_model_L0-49_resaved\"\n",
    "model_mohawk = AprielSSMHybridForCausalLM.from_pretrained(mohawk_checkpoint, device_map=\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 'm2d',\n",
       " 't',\n",
       " 't']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l3.config.hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.0.mixer.z_bias', 'model.layers.0.mixer.D', 'model.layers.0.mixer.in_proj.weight', 'model.layers.0.mixer.conv1d.weight', 'model.layers.0.mixer.conv1d.bias', 'model.layers.0.mixer.out_proj.weight', 'model.layers.1.mixer.z_bias', 'model.layers.1.mixer.D', 'model.layers.1.mixer.in_proj.weight', 'model.layers.1.mixer.conv1d.weight', 'model.layers.1.mixer.conv1d.bias', 'model.layers.1.mixer.out_proj.weight', 'model.layers.2.mixer.z_bias', 'model.layers.2.mixer.D', 'model.layers.2.mixer.in_proj.weight', 'model.layers.2.mixer.conv1d.weight', 'model.layers.2.mixer.conv1d.bias', 'model.layers.2.mixer.out_proj.weight', 'model.layers.3.mixer.z_bias', 'model.layers.3.mixer.D', 'model.layers.3.mixer.in_proj.weight', 'model.layers.3.mixer.conv1d.weight', 'model.layers.3.mixer.conv1d.bias', 'model.layers.3.mixer.out_proj.weight', 'model.layers.4.mixer.z_bias', 'model.layers.4.mixer.D', 'model.layers.4.mixer.in_proj.weight', 'model.layers.4.mixer.conv1d.weight', 'model.layers.4.mixer.conv1d.bias', 'model.layers.4.mixer.out_proj.weight', 'model.layers.5.mixer.z_bias', 'model.layers.5.mixer.D', 'model.layers.5.mixer.in_proj.weight', 'model.layers.5.mixer.conv1d.weight', 'model.layers.5.mixer.conv1d.bias', 'model.layers.5.mixer.out_proj.weight', 'model.layers.6.mixer.z_bias', 'model.layers.6.mixer.D', 'model.layers.6.mixer.in_proj.weight', 'model.layers.6.mixer.conv1d.weight', 'model.layers.6.mixer.conv1d.bias', 'model.layers.6.mixer.out_proj.weight', 'model.layers.7.mixer.z_bias', 'model.layers.7.mixer.D', 'model.layers.7.mixer.in_proj.weight', 'model.layers.7.mixer.conv1d.weight', 'model.layers.7.mixer.conv1d.bias', 'model.layers.7.mixer.out_proj.weight', 'model.layers.9.mixer.z_bias', 'model.layers.9.mixer.D', 'model.layers.9.mixer.in_proj.weight', 'model.layers.9.mixer.conv1d.weight', 'model.layers.9.mixer.conv1d.bias', 'model.layers.9.mixer.out_proj.weight', 'model.layers.10.mixer.z_bias', 'model.layers.10.mixer.D', 'model.layers.10.mixer.in_proj.weight', 'model.layers.10.mixer.conv1d.weight', 'model.layers.10.mixer.conv1d.bias', 'model.layers.10.mixer.out_proj.weight', 'model.layers.11.mixer.z_bias', 'model.layers.11.mixer.D', 'model.layers.11.mixer.in_proj.weight', 'model.layers.11.mixer.conv1d.weight', 'model.layers.11.mixer.conv1d.bias', 'model.layers.11.mixer.out_proj.weight', 'model.layers.12.mixer.z_bias', 'model.layers.12.mixer.D', 'model.layers.12.mixer.in_proj.weight', 'model.layers.12.mixer.conv1d.weight', 'model.layers.12.mixer.conv1d.bias', 'model.layers.12.mixer.out_proj.weight', 'model.layers.13.mixer.z_bias', 'model.layers.13.mixer.D', 'model.layers.13.mixer.in_proj.weight', 'model.layers.13.mixer.conv1d.weight', 'model.layers.13.mixer.conv1d.bias', 'model.layers.13.mixer.out_proj.weight', 'model.layers.14.mixer.z_bias', 'model.layers.14.mixer.D', 'model.layers.14.mixer.in_proj.weight', 'model.layers.14.mixer.conv1d.weight', 'model.layers.14.mixer.conv1d.bias', 'model.layers.14.mixer.out_proj.weight', 'model.layers.15.mixer.z_bias', 'model.layers.15.mixer.D', 'model.layers.15.mixer.in_proj.weight', 'model.layers.15.mixer.conv1d.weight', 'model.layers.15.mixer.conv1d.bias', 'model.layers.15.mixer.out_proj.weight', 'model.layers.28.mixer.z_bias', 'model.layers.28.mixer.D', 'model.layers.28.mixer.in_proj.weight', 'model.layers.28.mixer.conv1d.weight', 'model.layers.28.mixer.conv1d.bias', 'model.layers.28.mixer.out_proj.weight', 'model.layers.32.mixer.z_bias', 'model.layers.32.mixer.D', 'model.layers.32.mixer.in_proj.weight', 'model.layers.32.mixer.conv1d.weight', 'model.layers.32.mixer.conv1d.bias', 'model.layers.32.mixer.out_proj.weight', 'model.layers.36.mixer.z_bias', 'model.layers.36.mixer.D', 'model.layers.36.mixer.in_proj.weight', 'model.layers.36.mixer.conv1d.weight', 'model.layers.36.mixer.conv1d.bias', 'model.layers.36.mixer.out_proj.weight', 'model.layers.37.mixer.z_bias', 'model.layers.37.mixer.D', 'model.layers.37.mixer.in_proj.weight', 'model.layers.37.mixer.conv1d.weight', 'model.layers.37.mixer.conv1d.bias', 'model.layers.37.mixer.out_proj.weight', 'model.layers.38.mixer.z_bias', 'model.layers.38.mixer.D', 'model.layers.38.mixer.in_proj.weight', 'model.layers.38.mixer.conv1d.weight', 'model.layers.38.mixer.conv1d.bias', 'model.layers.38.mixer.out_proj.weight', 'model.layers.39.mixer.z_bias', 'model.layers.39.mixer.D', 'model.layers.39.mixer.in_proj.weight', 'model.layers.39.mixer.conv1d.weight', 'model.layers.39.mixer.conv1d.bias', 'model.layers.39.mixer.out_proj.weight', 'model.layers.40.mixer.z_bias', 'model.layers.40.mixer.D', 'model.layers.40.mixer.in_proj.weight', 'model.layers.40.mixer.conv1d.weight', 'model.layers.40.mixer.conv1d.bias', 'model.layers.40.mixer.out_proj.weight', 'model.layers.45.mixer.z_bias', 'model.layers.45.mixer.D', 'model.layers.45.mixer.in_proj.weight', 'model.layers.45.mixer.conv1d.weight', 'model.layers.45.mixer.conv1d.bias', 'model.layers.45.mixer.out_proj.weight', 'model.layers.48.mixer.z_bias', 'model.layers.48.mixer.D', 'model.layers.48.mixer.in_proj.weight', 'model.layers.48.mixer.conv1d.weight', 'model.layers.48.mixer.conv1d.bias', 'model.layers.48.mixer.out_proj.weight', 'model.layers.49.mixer.z_bias', 'model.layers.49.mixer.D', 'model.layers.49.mixer.in_proj.weight', 'model.layers.49.mixer.conv1d.weight', 'model.layers.49.mixer.conv1d.bias', 'model.layers.49.mixer.out_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# layer_ids = \"47\"\n",
    "sdm = model_mohawk.state_dict()\n",
    "# layer_sd = {k: v for k, v in sdm.items() if f\"layers.{layer_ids}\" in k}.copy()\n",
    "# r = model_l3.load_state_dict(layer_sd, strict=False)\n",
    "r = model_l3.load_state_dict(sdm, strict=False)\n",
    "print(r.unexpected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_l3.save_pretrained(\"/mnt/checkpoints/ssm/iterative_hybrids_only_new_layer_train/apriel_ssm_thinker15b_hybrid_3ssm_leastimportant_32h_init_mohawk_layer24\")\n",
    "model_l3.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_25ssm_leastimportant_32h_init_mohawk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/toolkit/dev/fml-ops/__oo_playground\")\n",
    "from results_analysis.results_loader import ResultsLoader\n",
    "layer_importance_path = \"/mnt/evaluations/training_evaluation/model_runs/lm_eval_runner/apriel_ssm_importance/\"\n",
    "results_loader = ResultsLoader(layer_importance_path)\n",
    "\n",
    "results_loader.deserialize_results()\n",
    "results_df = results_loader.to_df()\n",
    "results_df[\"layer_index\"] = results_df.apply(lambda row: int(row[\"model_name_sanitized\"].split(\"_\")[-1] if \"layers_\" in row[\"model_name_sanitized\"] else -1), axis=1)\n",
    "results_df = results_df[results_df[\"metric\"] == \"acc\"]\n",
    "columns_to_keep = [\"layer_index\", \"metric_value\"]\n",
    "results_df = results_df[columns_to_keep]\n",
    "layer_importance = results_df.groupby(\"layer_index\").mean()\n",
    "layer_importance = layer_importance.sort_values(by=\"metric_value\", ascending=False).reset_index()\n",
    "layer_importance = layer_importance[layer_importance[\"layer_index\"]!= -1]\n",
    "layer_importance = list(layer_importance[\"layer_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_thinker = \"/mnt/checkpoints/upstream/Apriel-Nemotron-15b-Thinker\"\n",
    "n_ssm = 50\n",
    "\n",
    "config_thinker = AutoConfig.from_pretrained(path_thinker)\n",
    "hybrid_block_layout = [\"t\"] * config_thinker.num_hidden_layers\n",
    "# hybrid_block_layout[0] = \"m2d\"\n",
    "\n",
    "for i in range(n_ssm):\n",
    "    # hybrid_block_layout[layer_importance[i]] = \"m2d\"\n",
    "    if i % 2 == 0:\n",
    "        # hybrid_block_layout[layer_importance[i]] = \"m2d\"\n",
    "        hybrid_block_layout[i] = \"m2d\"\n",
    "\n",
    "config_hybrid = AprielSSMHybridConfig(\n",
    "    **config_thinker.to_dict(),\n",
    "    hybrid_block_layout=hybrid_block_layout,\n",
    "    ssm_cfg = {\n",
    "            \"d_state\": 64,\n",
    "            \"n_v_heads\": 32,\n",
    "            \"n_qk_heads\": 32,\n",
    "            \"expand\": 1,\n",
    "            \"chunk_size\": 128,\n",
    "            \"activation\": \"identity\",\n",
    "            \"bias\": False,\n",
    "            \"d_conv\": 4,\n",
    "            \"d_inner\": 32 * 128\n",
    "        }\n",
    ")\n",
    "model_hybrid = AprielSSMHybridForCausalLM(config_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't',\n",
       " 'm2d',\n",
       " 't']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_block_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2684354560 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path_base \u001b[38;5;241m=\u001b[39m path_thinker\n\u001b[0;32m----> 2\u001b[0m model_base \u001b[38;5;241m=\u001b[39m \u001b[43mMistralForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model_hybrid\u001b[38;5;241m.\u001b[39mload_state_dict(model_base\u001b[38;5;241m.\u001b[39mstate_dict(), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/modeling_utils.py:4342\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4336\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4337\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4338\u001b[0m     )\n\u001b[1;32m   4340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4341\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4342\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4344\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4345\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:729\u001b[0m, in \u001b[0;36mMistralForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mMistralModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:440\u001b[0m, in \u001b[0;36mMistralModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    442\u001b[0m     [MistralDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    443\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m MistralRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/.conda/envs/fast_llm/lib/python3.12/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    145\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2684354560 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "path_base = path_thinker\n",
    "model_base = MistralForCausalLM.from_pretrained(path_base)\n",
    "\n",
    "model_hybrid.load_state_dict(model_base.state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_leastimportant_32h_init_rand\") # 1 ssm\n",
    "# model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_1ssm_0th_32h_init_rand\") # 1 ssm\n",
    "model_hybrid.save_pretrained(\"/mnt/checkpoints/ssm/apriel_ssm_thinker15b_hybrid_interleaved_ssm_starting0th_32h_init_rand\") # 1 ssm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
