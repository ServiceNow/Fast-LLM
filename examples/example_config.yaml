run:
  experiment_dir: null
  wandb_post_alerts: false
model:
  base_model:
    transformer:
      num_layers: 12
      hidden_size: 1024
      num_attention_heads: 8
      head_groups: 1
      add_linear_biases: true
      ffn_hidden_size: 4096
      kv_channels: 128
      activation_type: gelu
      init_method_std: 0.03125
      init_method_std_qkv: 0.03125
      init_method_std_attn_proj: 0.0063788795384978605
      init_method_std_mlp_1: 0.03125
      init_method_std_mlp_2: 0.0063788795384978605
      mlp_lr_scale:
      - null
    vocab_size: 49152
    use_position_embeddings: true
    tie_word_embeddings: true
    init_method_std_embed: 0.03125
  distributed:
    distributed_timeout: 60.0
    training_dtype: float32
pretrained:
  pretrained_checkpoint_path: null
  pretrained_checkpoint_type: distributed
batch:
  micro_batch_size: 1
  depth_first_micro_batches: 1
  breadth_first_micro_batches: 1
  sequential_micro_batches: 1
  batch_size: 1
  sequence_length: 2048
  micro_sequence_length: 2048
data:
  split:
  - 969.0
  - 30.0
  - 1.0
  dataset_source: list
  data_path:
  - fkgtiu
  data_sample_warn_time_ms: 1000.0
profiling:
  profile_cuda: false
  profile_ranks: []
optimizer:
  weight_decay: 0.01
  initial_loss_scale: 65536.0
