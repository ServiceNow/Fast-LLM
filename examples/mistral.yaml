training:
  train_iters: 100
  num_workers: 8
  logs:
    interval: 10
  evaluators:
    validation:
      evaluator:
        type: loss
        iterations: null
  test_iters: 0
batch:
  sequence_length: 4096
  micro_batch_size: 2
  batch_size: 64
data:
  datasets:
    training:
      type: random
optimizer:
  learning_rate:
    base: 1.0e-05
    decay_style: constant
    warmup_iterations: 0
  weight_decay: 0.1
  beta_1: 0.9
  beta_2: 0.95
model:
  base_model:
    transformer:
      mixer:
        type: attention
        rotary:
          type: default
          theta: 10000
        heads: 32
        head_groups: 8
        head_size: 128
        add_linear_biases: false
        window_size: 4096
        dropout: 0.0
      mlp:
        intermediate_size: 14336
        add_linear_biases: false
        gated: true
        activation: silu
      normalization:
        type: rms_norm
        epsilon: 1.0e-05
      num_layers: 32
      hidden_size: 4096
      dropout: 0.0
    embeddings_layer:
      vocab_size: 32000
      dropout: 0.0
    output_layer:
      tied_weight: false
      normalization:
        type: rms_norm
        epsilon: 1.0e-05
  multi_stage:
    zero_stage: 2
  distributed:
    training_dtype: bf16
    seed: 984059
run:
  experiment_dir: mistral_example
