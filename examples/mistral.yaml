training:
  train_iters: 100
  num_workers: 8
  logs:
    interval: 10
  evaluators:
    validation:
      evaluator:
        type: loss
        iterations: null
  test_iters: 0
batch:
  sequence_length: 4096
  micro_batch_size: 2
  batch_size: 64
data:
  datasets:
    training:
      type: random
optimizer:
  learning_rate:
    base: 1.0e-05
    decay_style: constant
    warmup_iterations: 0
  weight_decay: 0.1
  beta_1: 0.9
  beta_2: 0.95
model:
  base_model:
    transformer:
      mixer:
        type: attention
        rotary:
          type: default
          theta: 10000
        num_attention_heads: 32
        head_groups: 8
        kv_channels: 128
        window_size: 4096
        attention_dropout: 0.0
      mlp:
        ffn_hidden_size: 14336
        gated: true
        activation_type: silu
      normalization:
        type: rms_norm
        epsilon: 1.0e-05
      num_layers: 32
      hidden_size: 4096
      add_linear_biases: false
      init_method_std: 0.009021
      hidden_dropout: 0.0
    vocab_size: 32000
    tie_word_embeddings: false
  multi_stage:
    zero_stage: 2
  distributed:
    training_dtype: bf16
    seed: 984059
run:
  experiment_dir: mistral_example
