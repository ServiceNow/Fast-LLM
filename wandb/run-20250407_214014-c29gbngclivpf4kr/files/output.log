2025-04-07 21:40:15,400 >>> Allocating 18 weight buffers (2,680.13 MiB)
2025-04-07 21:40:15,990 >>> Allocating 18 grad buffers (5,360.26 MiB)
2025-04-07 21:40:15,994 >>> Allocating 4 shards (21,441.04 MiB)
2025-04-07 21:40:16,003 Total allocated: 29,481.43 MiB
2025-04-07 21:40:16,104 Preparing datasets...
2025-04-07 21:40:16,104 Preparing dataset. This may take several minutes.
/home/toolkit/dev/Fast-LLM/fast_llm/data/dataset/gpt/sampled.py:127: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  document_sizes = torch.from_numpy(self._indexed_dataset.get_document_sizes()).to(self._device)
2025-04-07 21:40:16,169 Using existing sampling for dataset __home__toolkit__dev__fast-llm-tutorial__dataset__shard_0_0
2025-04-07 21:40:16,174 Using existing sampling for dataset __home__toolkit__dev__fast-llm-tutorial__dataset__shard_0_0
2025-04-07 21:40:16,175 Initializing training state from pretrained checkpoint at /mnt/checkpoints_fml/pretrained_models/Llamba-1B (resetting optimizer state)...
2025-04-07 21:40:16,177 No block pattern provided, using default_block [m2]
2025-04-07 21:40:16,592 Loading from /mnt/checkpoints_fml/pretrained_models/Llamba-1B/model.safetensors
2025-04-07 21:40:17,630 1,405,159,936 state entries loaded successfully
2025-04-07 21:40:17,718 done with setup ...
2025-04-07 21:40:17,720 After initial setup:  allocated 29,482.39 MiB | max allocated 30,484.39 MiB | reserved 30,490.00 MiB | max reserved 30,490.00 MiB | global max reserved 30,490.00 MiB
2025-04-07 21:40:17,720 Initializing training dataset iterator from sample 0...
2025-04-07 21:40:17,801 Training ...
