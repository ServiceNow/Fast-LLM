# syntax=docker/dockerfile:1.7-labs
# FROM nvcr.io/nvidia/pytorch:25.05-py3
FROM nvidia/cuda:12.8.0-cudnn-devel-ubuntu24.04
ENV TORCH_CUDA_ARCH_LIST="9.0"
# ARG TORCH_CUDA_ARCH_LIST="8.0;8.6;9.0"
ENV FLASH_ATTENTION_CUDA_ARCHS="90"
ENV PIP_BREAK_SYSTEM_PACKAGES=1

# Basic system deps
RUN apt-get update && apt-get install --no-install-recommends -y \
    python3 python3-pip python3-dev git-lfs build-essential acl \
    libjpeg-dev zlib1g-dev libpng-dev libtiff5-dev \
    && rm -rf /var/lib/apt/lists/* \
    && git lfs install

RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1

# Set the working directory.
WORKDIR /app
# Set the permission to 777 for all files and directories in `/app`, `/home` and python install directories:
#   1. Create directories explicitly because docker use the wrong permission for explicit creation.
#   2. For the rest, set the default ACL to 777 for all users.
RUN mkdir -m 777 /app/Megatron-LM /app/examples /app/fast_llm /app/tests /app/tools \
    && setfacl -m d:u::rwx,d:g::rwx,d:o::rwx,u::rwx,g::rwx,o::rwx \
    /app \
    /home \
    /usr \
    /usr/local \
    /usr/local/bin \
    /usr/local/lib 

# The base image enforces versions for things like pytest for no good reason.
ENV PIP_CONSTRAINT=""

RUN python -m pip install --no-cache-dir --pre \
    torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/nightly/cu128

RUN python -c "import torch, sys; print('PYTHON:', sys.executable); print('TORCH VERSION:', torch.__version__); print('TORCH FILE:', torch.__file__); print('CUDA:', torch.version.cuda)"

RUN python -m pip install --no-cache-dir packaging \
    && python -m pip install --no-build-isolation --no-cache-dir git+https://github.com/NVIDIA/apex.git

# Install flash-linear-attention prerequisites and build from source.
RUN python -m pip install --no-cache-dir einops ninja datasets transformers numpy \
    && (python -m pip uninstall -y flash-linear-attention || true) \
    && python -m pip install -U --no-use-pep517 --no-deps --no-cache-dir \
    git+https://github.com/fla-org/flash-linear-attention

# Optional toolchain pieces for flash-attention.
RUN python -m pip install packaging psutil ninja
RUN python -m pip install --no-cache-dir flit-core packaging

RUN python -m pip uninstall -y causal-conv1d || true
RUN MAX_JOBS=2 python -m pip install --no-build-isolation --no-cache-dir "causal-conv1d@git+https://github.com/Dao-AILab/causal-conv1d@2a288a1"
RUN MAX_JOBS=2 python -m pip install --no-build-isolation --no-cache-dir --no-deps \
    "mamba_ssm@git+https://github.com/state-spaces/mamba@4a8a2a2"

# Optional KDA nightly requirements file for reproducibility.
COPY --chmod=777 requirements-kda-nightly.txt ./
# Copy dependency files with universal write permissions for all users.
COPY --chmod=777 setup.py setup.cfg pyproject.toml ./
COPY --chmod=777 ./fast_llm_external_models/__init__.py fast_llm_external_models/
COPY --chmod=777 ./fast_llm/__init__.py fast_llm/
COPY --chmod=777 ./fast_llm/csrc/ fast_llm/csrc/

RUN python -m pip install --no-cache-dir pybind11
# Install dependencies within the virtual environment.
RUN python -m pip install --no-cache-dir --no-build-isolation -e ".[CORE,OPTIONAL,HUGGINGFACE,SSM,GENERATION,DEV]" 

RUN python -m pip install --no-cache-dir --no-build-isolation \
      -e ".[VISION]"


# We only care about H100 (sm_90)
# RUN MAX_JOBS=1 python -m pip install --no-deps --no-cache-dir --no-build-isolation flash-attn \
#  && python -m pip install pytest
RUN MAX_JOBS=1 python -m pip install pytest

RUN git clone https://github.com/NVIDIA/apex /tmp/apex \
    && cd /tmp/apex \
    && MAX_JOBS=1 APEX_CPP_EXT=1 APEX_CUDA_EXT=1 pip install -v --no-build-isolation . \
    && rm -rf /tmp/apex

# Copy the remaining source code with universal write permissions.
COPY --chmod=777 ./Megatron-LM Megatron-LM
COPY --chmod=777 ./examples examples
COPY --chmod=777 ./tests tests
COPY --chmod=777 ./tools tools
COPY --chmod=777 ./fast_llm_external_models fast_llm_external_models
COPY --chmod=777 --exclude=./fast_llm/csrc/ ./fast_llm/ fast_llm/

# Set a dummy default user so we don't run in root by default.
# The image is still compatible with any user id.
RUN useradd user
USER user
