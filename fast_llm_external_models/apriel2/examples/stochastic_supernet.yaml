# Example: Stochastic supernet with attention + sliding window + gated delta net
#
# Converts a homogeneous attention model to a stochastic supernet
# where each layer can sample from multiple mixer types during training.
#
# Includes:
# - Full attention (direct weight transfer)
# - Sliding window attention (transfer with window size override)
# - Gated delta net (DIL initialization from attention weights)
#
# Usage:
#   python convert.py ServiceNow-AI/Apriel-1.5-15b-Thinker output/ \
#       --surgery examples/stochastic_supernet.yaml

decoder:
  type: fixed
  block:
    mixer:
      type: stochastic
      main_mixer_name: attention
      sampling_strategy: uniform
      mixers:
        # Main attention mixer - inherits config and weights from source
        attention:
          type: attention
          init: transfer

        # Sliding window - same architecture with window size override
        sliding_window:
          type: attention
          init: transfer
          window_size: 4096

        # Gated delta net - DIL initialization maps Q/K/V/O -> GDN projections
        # GDN dimensions are derived from source attention:
        #   value_heads <- heads (40 for Apriel 1.5)
        #   key_heads <- head_groups (8 for Apriel 1.5)
        #   key_head_dim <- head_size (128 for Apriel 1.5)
        #   value_head_dim <- head_size (128 for Apriel 1.5)
        gdn:
          type: gdn
          init: transfer
          convolution_layer:
            kernel_size: 4

    # MLP and normalization transfer from source
    mlp:
      init: transfer

    normalization:
      init: transfer
