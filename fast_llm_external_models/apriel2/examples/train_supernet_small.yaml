# Training config for small Apriel2 stochastic supernet (single GPU)
#
# This config loads a converted Apriel2 model and trains it on multimodal data.
# The stochastic supernet includes attention, sliding window, gated delta net, and KDA mixers.
# Training uses activation-level distillation from a teacher model (attention-only) to guide
# the alternative mixers (GDN, KDA) to produce similar activations.
#
# Prerequisites:
#
# 1. Convert the student model (stochastic supernet) with reduced layers:
#    (Note: multiple --surgery flags are composed left-to-right)
#
#    python fast_llm_external_models/apriel2/convert.py \
#        ServiceNow-AI/Apriel-1.5-15b-Thinker \
#        /tmp/apriel2-supernet-small \
#        --surgery fast_llm_external_models/apriel2/examples/stochastic_supernet.yaml \
#        --surgery fast_llm_external_models/apriel2/examples/small.yaml
#
# 2. Convert the teacher model (attention-only, same layer reduction):
#
#    python fast_llm_external_models/apriel2/convert.py \
#        ServiceNow-AI/Apriel-1.5-15b-Thinker \
#        /tmp/apriel2-teacher-small \
#        --surgery fast_llm_external_models/apriel2/examples/small.yaml
#
# 3. Create a multimodal dataset with matching patch size (16x16):
#
#    python -c "
#    from tests.utils.dataset import _get_test_dataset, DATASET_CACHE
#    from fast_llm.data.preprocessing.image_patch import ImagePatchConfig
#    _get_test_dataset(
#        DATASET_CACHE / 'apriel2_multimodal_dataset',
#        seed=1234,
#        vocab_size=131072,
#        max_images=2,
#        image_patch_config=ImagePatchConfig(
#            height=16, width=16,
#            max_image_height=64, max_image_width=64,
#        ),
#        splits={'training': 100},
#    )
#    "
#
# 4. Run training:
#
#    fast-llm train train_multimodal \
#        -c fast_llm_external_models/apriel2/examples/train_supernet_small.yaml
#
# The trained model will be exported to:
#    /tmp/apriel2-supernet-small-trained/export/apriel2/{iteration}/
#
# 5. Load and test the trained model, then switch mixers at runtime:
#
#    python -c "
#    import torch
#    from transformers import AutoProcessor, AutoModelForImageTextToText
#
#    # Load the trained Apriel2 VLM (includes stochastic supernet with KDA)
#    model = AutoModelForImageTextToText.from_pretrained(
#        '/tmp/apriel2-supernet-small-trained/export/apriel2/10',
#        torch_dtype=torch.bfloat16,
#        device_map='auto',
#        trust_remote_code=True,
#    )
#    processor = AutoProcessor.from_pretrained('ServiceNow-AI/Apriel-1.5-15b-Thinker')
#
#    # Show available mixers in the stochastic supernet
#    block = model.model.decoder.blocks[0]
#    print(f'Available mixers: {list(block.mixer.mixers.keys())}')
#    print(f'Current main mixer: {block.mixer.main_mixer_name}')
#
#    # Switch all blocks to use KDA as the main mixer (used during inference)
#    for block in model.model.decoder.blocks:
#        block.mixer.main_mixer_name = 'kda'
#    print(f'Switched to: {model.model.decoder.blocks[0].mixer.main_mixer_name}')
#
#    # Generate with KDA
#    chat = [{'role': 'user', 'content': [{'type': 'text', 'text': 'Hello'}]}]
#    inputs = processor.apply_chat_template(
#        chat, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt'
#    )
#    inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
#    inputs.pop('token_type_ids', None)
#
#    with torch.no_grad():
#        output_ids = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7)
#    print(processor.decode(output_ids[0], skip_special_tokens=True))
#    "

# Load pretrained model
pretrained:
  path: /tmp/apriel2-supernet-small
  format: apriel2
  model_weights: true
  load_config: model

# Model config (mostly loaded from pretrained, but we need to specify some fast-llm specific settings)
model:
  base_model:
    # Freeze all components except the mixer by setting lr_scale: 0
    # The mixer will train with the default learning rate (lr_scale: 1.0 implicitly)
    decoder:
      block:
        mlp:
          lr_scale: 0.0  # Freeze MLP
        normalization:
          lr_scale: 0.0  # Freeze layer norms (norm_1 and norm_2 in each block)
        # Activation-level distillation: teach mixers to mimic teacher's attention outputs
        distillation_model: teacher
        activation_distillation_factor: 0.1
    embeddings:
      lr_scale: 0.0  # Freeze word embeddings
    head:
      lr_scale: 0.0  # Freeze output head (includes final norm)
      cross_entropy_implementation: torch
    vision_encoder:
      lr_scale: 0.0  # Freeze vision encoder
  multi_stage:
    zero_stage: 2  # ZeRO stage 2 for memory efficiency
  distributed:
    compute_dtype: bf16
    seed: 42

# Teacher model for activation-level distillation
# Uses the same architecture but with standard attention (no stochastic mixer)
reference_models:
  teacher:
    model:
      type: multimodal
    pretrained:
      path: /tmp/apriel2-teacher-small
      format: apriel2
      model_weights: true
      load_config: model

# Batch configuration (small for single GPU)
batch:
  sequence_length: 512  # Short sequences for testing
  micro_batch_size: 1   # Small batch for single GPU
  batch_size: 4         # Accumulate gradients

# Data configuration (multimodal test dataset)
data:
  datasets:
    training:
      type: file
      path: /tmp/fast_llm_tests/common/dataset/apriel2_multimodal_dataset/fast_llm_config_training.yaml

# Optimizer configuration
optimizer:
  learning_rate:
    base: 1.0e-05
    decay_style: constant
    warmup_iterations: 0
  weight_decay: 0.1
  beta_1: 0.9
  beta_2: 0.95

# Training configuration
training:
  train_iters: 100  # Extended training run
  num_workers: 2
  logs:
    interval: 1
  checkpoint:
    interval: null  # Disable checkpointing for quick test
  export:
    interval: 100   # Export at the end
    format: apriel2 # Export back to Apriel2 HF format
  test_iters: 0
  evaluators: {}

# Experiment directory and logging
run:
  experiment_dir: /tmp/apriel2-supernet-small-trained
  # Enable model debug logging to see stochastic mixer selection per iteration
  model_debug_level: 1
