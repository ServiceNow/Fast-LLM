{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fast-LLM: Train Large Language Models Faster Than Ever Before","text":"<p>Introducing Fast-LLM, the cutting-edge open-source library built for training large language models (LLMs) with unmatched speed, scalability, and cost-efficiency. Developed by ServiceNow Research's Foundation Models Lab, Fast-LLM is engineered to meet the rigorous demands of professional AI researchers, AI/ML engineers, academic and industrial research institutions, and enterprise product development teams pushing the limits of generative AI. Achieve groundbreaking research and high-stakes production goals faster with Fast-LLM.</p> <p>Start your journey with Fast-LLM and explore the future of LLM training. Dive into real-world use cases to see how Fast-LLM can elevate your training workflows.</p>"},{"location":"#why-fast-llm","title":"Why Fast-LLM?","text":"<p>Fast-LLM is designed for professionals who demand exceptional performance for efficient, large-scale language model training on GPUs, where maximizing FLOPS is key. Fast-LLM integrates effortlessly into existing ML pipelines and goes beyond off-the-shelf commercial frameworks to deliver a robust, flexible, and high-performance open-source alternative. Whether you're optimizing for speed, cost, or scalability, Fast-LLM helps you get the most out of your training infrastructure.</p>"},{"location":"#the-fast-llm-advantage","title":"The Fast-LLM Advantage","text":"<p>Fast-LLM isn't just another library, it's a platform for powering the next generation of AI breakthroughs. Here's what sets it apart:</p> <ul> <li> <p>\ud83d\ude80 Purpose-Built for Small- and Large-Scale AI: Optimized specifically for training language models of all sizes, Fast-LLM excels from small models around 1B parameters to massive clusters running 70B+ parameter models, with kernels that are fine-tuned for maximum throughput across this entire range. At 10B-parameter scale, Fast-LLM avoids costly 3D-parallelism through memory optimization techniques such as ZeRO and activation recomputation, whereas at 100B-parameter scale, Fast-LLM optimally supports 3D-parallelism; making Fast-LLM the go-to choice for diverse training needs.</p> </li> <li> <p>\ud83e\udde0 Unified Support for GPT-Like Architectures: Fast-LLM streamlines the implementation of GPT-like models into a single, unified module, significantly reducing redundancy and simplifying adaptation to custom architectures. This approach ensures consistency and flexibility while minimizing development overhead.</p> </li> <li> <p>\ud83d\udcb0 Cost Efficiency That Sets Fast-LLM Apart:</p> <ul> <li> <p>Lower Training Costs: With higher throughput per GPU, Fast-LLM reduces the training time required. Training models can be cheaper compared to other frameworks due to faster processing and better memory efficiency.</p> </li> <li> <p>More Tokens for Your Budget: Train on more tokens for the same budget, leading to better-trained models without breaking your financial constraints.</p> </li> </ul> </li> <li> <p>\ud83d\udd13 Openness Without Compromise: Fast-LLM's open-source approach ensures that you can fully customize and extend the library to fit your exact needs, without the restrictions of proprietary software. Developed transparently by a community of experts on GitHub, every change is publicly discussed and vetted, fostering trust and collaboration so you can innovate with confidence, knowing the entire development process and decision making is out in the open.</p> </li> <li> <p>\ud83c\udf0d Community-Driven Development: Built by professionals for professionals, Fast-LLM's development is transparent, with an open invitation to the community to contribute. Join the Fast-LLM community to help shape the future of large-scale AI training.</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>Fast-LLM offers all the capabilities you need to accelerate your LLM training and push the boundaries of what's possible:</p> <ul> <li> <p>\ud83d\ude80 Speed Like No Other: Achieve record-breaking training throughput with Fast-LLM. For instance, train Mistral-7B at 10,350 tokens/s/GPU on a 4-node cluster with 32 H100 GPUs (batch size 64, sequence length 8k). Our optimized kernels, advanced parallelism, and memory-efficient techniques drastically reduce training time and cost.</p> </li> <li> <p>\ud83d\udce1 Unmatched Scalability: Seamlessly scale from a single GPU to large compute clusters. Fast-LLM supports 3D parallelism (data, tensor, and pipeline), sequence length parallelism, and ZeRO-1,2,3 techniques for maximum memory efficiency. Scale to the size you need without sacrificing performance.</p> </li> <li> <p>\ud83c\udf9b\ufe0f Total Flexibility: Compatible with all major language model architectures, including but not limited to Llama, Mistral, StarCoder, and Mixtral. Fast-LLM's modular design gives you full control over your training workflows.</p> </li> <li> <p>\ud83d\udce6 Seamless Integration: Integrate smoothly with popular libraries such as HuggingFace Transformers. Benefit from Fast-LLM's optimizations without disrupting your existing pipelines.</p> </li> <li> <p>\ud83d\udee0\ufe0f Professional-Grade Tools: Enjoy mixed precision training, large batch training, and gradient accumulation. Fast-LLM ensures reproducibility through deterministic behavior and provides pre-built Docker images, YAML configurations, and a simple, intuitive command-line interface.</p> </li> </ul> <p>Get Fast-LLM and start training your large language models in record time. Join the Fast-LLM community and collaborate with like-minded professionals to advance the state-of-the-art in AI research and development.</p>"},{"location":"#use-cases-and-success-stories","title":"Use Cases and Success Stories","text":"<p>Fast-LLM powers the world's most advanced AI projects:</p> <ul> <li>NLP Research and Development: Train state-of-the-art language models for natural language understanding, summarization, and conversational AI.</li> <li>Enterprise AI Solutions: Accelerate time-to-market for AI products by reducing training costs and enabling faster iteration.</li> <li>Academic Collaborations: Drive AI innovation with high-performance training capabilities that support cutting-edge research in machine learning.</li> </ul> <p>See how Fast-LLM has helped early adopters achieve faster results. Explore use cases and success stories.</p>"},{"location":"#project-scope-and-objectives","title":"Project Scope and Objectives","text":"<p>Fast-LLM is designed to be the go-to solution for those training the most sophisticated language models. Our objectives include:</p> <ul> <li>Accelerating Training Workflows: Deliver the fastest LLM training experience with optimized kernel efficiency, parallelism, and memory management.</li> <li>Supporting a Broad Range of Architectures: Offer built-in support for all major language model architectures, with an architecture-agnostic approach that allows users to easily adapt the framework to emerging models.</li> <li>Enabling Seamless Integration and Deployment: Integrate effortlessly into existing ML pipelines, including HuggingFace Transformers and Kubernetes-based clusters.</li> <li>Advancing LLM Research and Production-Readiness: Be suitable for both cutting-edge research and mission-critical production workloads.</li> </ul>"},{"location":"#collaboration-and-contribution","title":"Collaboration and Contribution","text":"<p>As Fast-LLM evolves, we invite the community to contribute and help shape its future. We welcome:</p> <ul> <li>Testing and Bug Fixes: Help us identify issues and improve stability.</li> <li>Feature Development: Contribute new models, new training features, and new optimizations.</li> <li>Documentation and Tutorials: Make Fast-LLM more accessible by improving our documentation and writing practical guides.</li> </ul> <p>Fast-LLM is more than just software, it's a community. Get involved by exploring our contribution guidelines and engaging with us on GitHub Discussions.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out our quick-start guide for an overview of how to set up and run Fast-LLM on different platforms, including Slurm and Kubernetes. Explore the examples for pre-configured setups to help you get started quickly with your own training experiments.</p> <p>For any questions or issues, open an issue or join the community discussion.</p>"},{"location":"about-us/","title":"About Us","text":"<p>Welcome to Fast-LLM! We are a global team of engineers, researchers, and AI professionals led by the Foundation Models Lab at ServiceNow Research, dedicated to advancing large language models (LLMs) and providing the highest-performance tools for serious users. Designed with professionals, research institutions, and enterprises in mind, Fast-LLM offers the speed, scalability, and flexibility needed to train the biggest and most complex models. Our commitment to open-source ensures that you have full control over your workflows, without the limitations or compromises of commercial frameworks.</p>"},{"location":"about-us/#our-mission","title":"\ud83d\ude80 Our Mission","text":"<p>Our mission is to deliver a best-in-class library for training large-scale language models, combining cutting-edge performance with robust, customizable features. Fast-LLM is built to meet the needs of researchers and organizations who push the boundaries of generative AI, enabling them to train state-of-the-art models more efficiently. By optimizing training workflows and scaling to massive compute clusters, we help professionals unlock the full potential of LLMs, reducing costs and time-to-deployment for ambitious AI projects.</p>"},{"location":"about-us/#our-vision","title":"\ud83c\udf0d Our Vision","text":"<p>We envision Fast-LLM as the go-to solution for serious AI practitioners who require more than what typical frameworks can offer. Our goal is to empower research institutions, corporate AI teams, and universities to train sophisticated models that exceed the capabilities of standard tools. By creating a highly performant and customizable library, we aim to be the backbone of cutting-edge AI research and development, equipping experts with the tools they need to tackle the toughest training challenges.</p>"},{"location":"about-us/#our-values","title":"\ud83c\udfaf Our Values","text":"<p>At Fast-LLM, we adhere to a set of guiding principles that define our approach:</p> <ul> <li>Performance-Driven: We are relentless in our pursuit of speed and efficiency. Fast-LLM is built to reduce training time and scale to the largest clusters, enabling our users to achieve breakthrough results faster.</li> <li>Professional-Grade Customization: We understand that serious AI work demands flexibility. Fast-LLM is designed for extensive customization, allowing users to tailor every aspect of the training process to their unique needs.</li> <li>Open Innovation: While we cater to advanced users, our commitment to open-source ensures that innovation remains accessible. We believe in building a community where professionals can collaborate and contribute to shaping the future of AI.</li> <li>Reliability at Scale: Fast-LLM is built with rigorous standards to support production-level workloads. We prioritize stability, reproducibility, and robustness, ensuring that your models can scale from research to real-world applications seamlessly.</li> </ul>"},{"location":"about-us/#meet-the-team","title":"\ud83d\udc65 Meet the Team","text":"<p>Fast-LLM is led by the Foundation Models Lab at ServiceNow Research, with development driven by a dedicated group of professionals who bring extensive expertise in AI, machine learning, and distributed systems. While the project direction is guided by the Foundation Models Lab, contributions come from a growing network of researchers, developers, and industry experts worldwide. Here are some of the key members leading the project:</p> <ul> <li>Joel Lamy Poirier](https://www.servicenow.com/research/author/joel-lamy-poirier.html) - Lead Developer and maintainer, ServiceNow Research: Joel spearheads the core development, ensuring that Fast-LLM delivers on its promise of speed and scalability.</li> <li>Sean Hughes - Ecosystem Director, ServiceNow Research: Sean focuses on building partnerships and open scientific collaborations to advance Fast-LLM's capabilities and reach.</li> <li>Torsten Scholak - Research Lead, ServiceNow Research: Torsten leads our research efforts, driving the scientific innovations that keep Fast-LLM at the forefront of AI training.</li> </ul> <p>Our core team includes members affiliated with ServiceNow Research, as well as other contributors who bring unique perspectives and skills to the project. We welcome new participants from the broader AI community who share our vision of creating the best tools for training large-scale language models.</p>"},{"location":"help/","title":"Help","text":"<p>Welcome to the Fast-LLM Help Center! Here, you'll find fixes for common hiccups, links to dig deeper, tutorials, and pointers for when you need some extra support. Remember, everyone hits a snag now and then. Let's sort them out together and get you back to training.</p>"},{"location":"help/#common-issues-gotchas","title":"Common Issues &amp; Gotchas \ud83d\udea7","text":"<p>Let's stay one step ahead of those pesky gotchas. Here's a list of common issues and quick fixes:</p> <ul> <li> <p>CUDA Out of Memory: When the GPU throws a fit, a few tweaks can help. First, try lowering <code>micro_batch_size</code> or <code>sequence_length</code> in the configuration to fit within the available memory. Still stuck? Try setting the <code>mlp_recompute_level</code> option to <code>activation</code> or <code>full</code> to save memory in the backward pass, or experiment with higher ZeRO stages for reduced memory usage. And if that's not enough, tensor or model parallelism may be your friend.</p> </li> <li> <p>Python Hash Seed Sync Error: Encountering an error like</p> <pre><code>RuntimeError: Desync detected for barrier train begin (66830148464 != 133042721120)\n</code></pre> <p>points to a hashing inconsistency. To fix it, set <code>PYTHONHASHSEED=0</code> in your environment variables. This ensures that Python's hash seed is consistent across all processes. If these processes have different hash seeds, they'll generate different hash values, leading to desynchronization, as seen in the error message.</p> </li> <li> <p><code>torchrun</code> Timeout Errors: If you see timeout errors related to <code>torchrun</code> during rendezvous, it could be DNS resolution or a networking issue. Check that all worker nodes are communicating properly with the master node.</p> </li> <li> <p>NCCL Errors with Timeout Messages: Oh, the joys of NCCL errors! If you see something like</p> <pre><code>Watchdog caught collective operation timeout: WorkNCCL(SeqNum=408951, OpType=_ALLGATHER_BASE, \u2026 , Timeout(ms)=600000) ran for 600351 milliseconds before timing out\n</code></pre> <p>appearing across all GPU workers, it usually means one or more hosts failed to complete a NCCL operation, causing others to block. NCCL errors can be frustrating to diagnose since they rarely specify which node or GPU caused the issue. It is difficult to surface which messages and operations are in progress during these crashes. If the issue happens at a specific moment of training like dataset preparation or model export, the issue might be that this specific procedure took too long and timed out other processes (e.g. when preparing large datasets for long training runs, or saving large models on slow storage). In this case, it can help to increase the timeout <code>distributed_timeout: 3600</code>. In some other cases, the best we can do is to restart the training job and hope it doesn't happen again. If the issue persists, it might be because of network congestion or a problematic GPU. If the worker that crashed is consistent across multiple runs, it's likely a hardware issue. If you can't resolve it, open an issue on GitHub, and we'll help you troubleshoot.</p> </li> </ul> <p>For more detailed solutions, check out our GitHub Issues page. Odds are someone's already tackled a similar problem, and you might find the exact fix you need.</p>"},{"location":"help/#reference","title":"Reference \ud83d\udcda","text":"<p>If you're the type who loves configurations and tweaking every detail, the Configuration Reference is for you. It covers every config option you could imagine. From optimizer settings to batch sizes to distributed training parameters. It's all in there.</p>"},{"location":"help/#tutorials","title":"Tutorials \ud83d\udc68\u200d\ud83c\udfeb","text":"<p>We've got some excellent tutorials to help you get the most out of Fast-LLM:</p> <ul> <li> <p>Quick-Start Guide: Perfect for launching Fast-LLM on a single GPU machine. We walk you through running your first training job (either locally or on a cluster), and handling common issues.</p> </li> <li> <p>Cookbook: Ready to go big? These recipes cover real-world scenarios like training big models from scratch, continuing training from a checkpoint, and more. This is where Fast-LLM really shows its power.</p> </li> </ul>"},{"location":"help/#still-stuck-where-to-find-help","title":"Still Stuck? Where to Find Help \ud83d\ude4b","text":"<p>If Fast-LLM still isn't cooperating, here's where to look next:</p> <ol> <li> <p>GitHub Issues &amp; Discussions: This is your best resource. Use the search function to see if anyone has run into the same issue. The community and our team are pretty active, so you'll likely find a solution or get help quickly.</p> </li> <li> <p>Email (last resort): As a final option, you can email us at fast-llm-team@servicenow.com. This is only for rare cases, though. GitHub is our go-to for answering questions, as it lets others benefit from the conversation too.</p> </li> </ol> <p>Fast-LLM is a growing community, and your questions and contributions help make it better for everyone. Who knows, you might just solve the next person's roadblock!</p> <p>That's it! We're excited to see what you build with Fast-LLM. Happy training!</p>"},{"location":"join-us/","title":"Join Us","text":"<p>Fast-LLM is an open-source project driven by a community of passionate contributors. Whether you're a researcher, developer, or AI enthusiast, there's a place for you to make a real impact on the future of large-scale AI training. Join us, dive in, and help shape the tools that push the boundaries of language model training. Here's how you can get involved:</p>"},{"location":"join-us/#stay-in-the-loop","title":"\ud83d\udcec Stay in the Loop","text":"<p>Want to keep up with the latest Fast-LLM updates and new opportunities to get involved? Star the Fast-LLM repository on GitHub and watch the project for notifications on new releases, discussions, and updates. This way, you'll always know what's happening, from new features to community initiatives.</p> <p>Star and Watch the Fast-LLM repo on GitHub to stay updated on new releases, discussions, and upcoming features.</p>"},{"location":"join-us/#code-contributions","title":"\ud83d\udee0 Code Contributions","text":"<p>Fast-LLM thrives on collaboration, and we're excited to welcome new contributors! From fixing bugs to adding new features, every code contribution makes a difference. If you're just getting started, our Good First Issues on GitHub are labeled to help newcomers find approachable tasks. To set up your development environment and get oriented with Fast-LLM, check out our Developer's Corner for everything you need:</p> <ul> <li>Contributing \u2013 for setup instructions and contributing guidelines</li> <li>Best Practices \u2013 for tips on writing clean, maintainable code</li> </ul> <p>Here's a quick overview of the process:</p> <ol> <li>Fork &amp; Clone: Start by forking the repo and cloning it to your machine.</li> <li>Set Up Your Dev Environment: The Developer's Corner guides you through configuring your environment for maximum productivity.</li> <li>Write Awesome Code: Make your changes, document them, and follow our best practices.</li> <li>Open a Pull Request: Submit a PR to showcase your work and get feedback from our team and the community.</li> </ol> <p>Explore our Developer's Corner for everything you need to get started!</p>"},{"location":"join-us/#feature-requests-ideas","title":"\ud83d\udca1 Feature Requests &amp; Ideas","text":"<p>Got a great idea? We want to hear it! Whether it's a new feature, an enhancement, or even a moonshot idea, head over to GitHub Discussions to share your thoughts. Community feedback drives Fast-LLM's evolution, and your ideas can help shape the future of the project.</p> <p>Share your thoughts on GitHub Discussions.</p>"},{"location":"join-us/#testing-feedback","title":"\ud83d\udd0d Testing &amp; Feedback","text":"<p>Your experience with Fast-LLM is invaluable, whether you're running it in production or experimenting at home. We rely on user feedback to find bugs, optimize performance, and improve documentation. Please share any bugs, performance quirks, or gaps you spot with us on GitHub Issues. This kind of feedback strengthens the entire project.</p> <p>Report issues and share feedback on GitHub Issues.</p>"},{"location":"join-us/#help-support","title":"\ud83e\udd1d Help &amp; Support","text":"<p>Love helping others? Join our GitHub Discussions to answer questions, help troubleshoot, or share tips. Fast-LLM is a community, and the more we support each other, the stronger we become. Helping out is a great way to get involved and learn from others too.</p>"},{"location":"join-us/#spread-the-word","title":"\ud83d\udce3 Spread the Word","text":"<p>If you're excited about Fast-LLM, let the world know! Share on social media, write a blog post, or give a talk at your next tech meetup. Spreading the word helps grow our community and brings new talent into the project.</p>"},{"location":"join-us/#join-our-team","title":"\ud83c\udf1f Join Our Team","text":"<p>Excited about contributing on a deeper level? The Foundation Models Lab at ServiceNow is at the forefront of large-scale AI training. We're looking for passionate individuals to push the boundaries of AI development with us. From research developers focusing on GPU optimization to visiting researchers refining our training frameworks, there's a role for everyone. Explore current opportunities and become a key player in shaping the future of AI at ServiceNow.</p> <p>Check out our Careers page for more information.</p> <p>Let's push the boundaries of large-scale AI training together. We're thrilled to have you here. Welcome to the Fast-LLM community!</p>"},{"location":"license/","title":"License","text":"<p>Fast-LLM is licenced under the Apache 2.0 license:</p> <pre><code>Copyright 2024-2025 ServiceNow, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre>"},{"location":"quick-start/","title":"Quick Start","text":"<p>This guide will get you up and running with Fast-LLM. Let's train a model and see some results!</p>"},{"location":"quick-start/#prerequisites","title":"Prerequisites","text":"<p>To follow this guide, you'll need:</p> <ul> <li>Hardware: At least one NVIDIA GPU, preferably with Ampere architecture or newer. Note that this tutorial is designed for 80 GB A100s or H100 GPUs, and some adjustments are needed to run it with less memory or an earlier architecture.</li> <li>Software: Depending on your setup, you'll need one of the following:<ul> <li>Docker: If you're using the prebuilt Docker image on your local machine.</li> <li>Python 3.10: If you're setting up a custom environment (virtual environment, bare-metal, etc.) on your local machine.</li> <li>Cluster Setup: Access to a Docker-enabled Slurm cluster or to a Kubernetes cluster with Kubeflow if you're using those environments.</li> </ul> </li> </ul>"},{"location":"quick-start/#step-1-initial-setup","title":"\ud83c\udfd7 Step 1: Initial Setup","text":"<p>First, create a working directory for this tutorial:</p> <pre><code>mkdir ./fast-llm-tutorial\n</code></pre> <p>We'll use this directory to store all the files and data needed for training.</p> <p>Now, select the compute environment that matches your setup or preferred workflow. Once you select an environment, all sections of this guide will adapt to provide instructions specific to your choice:</p> Prebuilt DockerCustom InstallationSlurmKubeflow <p>Use a preconfigured Docker container with the Fast-LLM image, which includes all the required software and dependencies. Run the following command to pull the image and start a container:</p> <pre><code>docker run --gpus all -it --rm \\\n    -v $(pwd)/fast-llm-tutorial:/app/fast-llm-tutorial \\\n    ghcr.io/servicenow/fast-llm:latest \\\n    bash\n</code></pre> <p>Replace <code>--gpus all</code> with <code>--gpus '\"device=0,1,2,3,4,5,6,7\"'</code> etc. if you want to use specific GPUs.</p> <p>Once inside the container, all commands from this guide can be executed as-is. The <code>fast-llm-tutorial</code> directory is mounted inside the container at <code>/app/fast-llm-tutorial</code>, so any files saved there will persist and be accessible on your host machine as well.</p> <p>If you prefer not to use the prebuilt Docker image or already have an environment you'd like to use (e.g., a custom Docker image, virtual environment, or bare-metal setup), follow these steps to install the necessary software and dependencies:</p> <ol> <li> <p>Ensure Python 3.12:     Install Python 3.12 (or later) if it's not already available on your system. For a Python virtual environment, run:</p> <pre><code>python3.12 -m venv ./fast-llm-tutorial/venv\nsource ./fast-llm-tutorial/venv/bin/activate\npip install --upgrade pip\n</code></pre> <p>You can deactivate the virtual environment later with <code>deactivate</code>.</p> </li> <li> <p>Verify CUDA Installation:     Make sure CUDA 12.1 or later is installed in your environment. Verify with:</p> <pre><code>nvcc --version\n</code></pre> <p>If CUDA is not installed or the version is incorrect, follow the CUDA installation guide to set it up.</p> </li> <li> <p>Pre-install PyTorch and pybind11:     Install PyTorch and pybind11 to meet Fast-LLM's requirements:</p> <pre><code>pip install pybind11 \"torch&gt;=2.2.2\"\n</code></pre> </li> <li> <p>Install NVIDIA APEX:     Fast-LLM uses certain kernels from APEX. Follow the installation instructions on their GitHub page, ensuring you use the <code>--cuda_ext</code> and <code>--fast_layer_norm</code> options to install all kernels supported by Fast-LLM:</p> <pre><code>git clone https://github.com/NVIDIA/apex ./fast-llm-tutorial/apex\npushd ./fast-llm-tutorial/apex\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" --config-settings \"--build-option=--fast_layer_norm\" ./\npopd\n</code></pre> </li> <li> <p>Install Fast-LLM and Dependencies:     Finally, install Fast-LLM along with its remaining dependencies, including FlashAttention-2:</p> <pre><code>pip install --no-build-isolation \"git+https://github.com/ServiceNow/Fast-LLM.git#egg=fast_llm[CORE,OPTIONAL,DEV]\"\n</code></pre> </li> <li> <p>Verify the Installation:     Confirm the setup with the following commands:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\npython -c \"from amp_C import *\"\npython -c \"import flash_attn; print(flash_attn.__version__)\"\npython -c \"import fast_llm; print(fast_llm.__version__)\"\n</code></pre> </li> </ol> <p>If you made it this far without any errors, your local environment is ready to run Fast-LLM.</p> <p>Use Docker-enabled Slurm for this tutorial. The <code>ghcr.io/servicenow/fast-llm:latest</code> Docker image will be pulled and run on the compute nodes. Ensure the <code>fast-llm-tutorial</code> directory is accessible across all nodes (e.g., via a shared filesystem like NFS).</p> <p>Use Kubernetes with Kubeflow and a <code>PyTorchJob</code> resource to train our model using the <code>ghcr.io/servicenow/fast-llm:latest</code> Docker image. We'll copy the configuration files and dataset to shared persistent volume claims (PVCs) to ensure all nodes have access to the same data. Follow these steps:</p> <ol> <li> <p>Create a Persistent Volume Claim (PVC)</p> <p>Create a PVC named <code>pvc-fast-llm-tutorial</code> to store input data and output results:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: \"v1\"\nkind: \"PersistentVolumeClaim\"\nmetadata:\n  name: \"pvc-fast-llm-tutorial\"\nspec:\n  storageClassName: local-path  # (1)!\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi  # (2)!\nEOF\n</code></pre> <ol> <li>Replace with your cluster's StorageClassName.</li> <li>Adjust the storage size as needed.</li> </ol> <p>StorageClassName</p> <p>Replace <code>local-path</code> with the appropriate <code>StorageClassName</code> for your Kubernetes cluster. Consult your cluster admin or documentation if unsure.</p> </li> <li> <p>Set Up a Temporary Pod for Data Management</p> <p>Create a temporary pod to manage input data and results:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-fast-llm-tutorial\nspec:\n  containers:\n    - name: fast-llm-tutorial-container\n      image: ghcr.io/servicenow/fast-llm:latest\n      command: [\"sleep\", \"infinity\"]\n      volumeMounts:\n        - mountPath: /app/fast-llm-tutorial\n          name: fast-llm-tutorial\n  volumes:\n    - name: fast-llm-tutorial\n      persistentVolumeClaim:\n        claimName: pvc-fast-llm-tutorial\nEOF\n</code></pre> <p>Purpose of the Temporary Pod</p> <p>This pod ensures you have an interactive container for managing input data and retrieving results. Use <code>kubectl exec</code> to interact with it:</p> <pre><code>kubectl exec -it pod-fast-llm-tutorial -- bash\n</code></pre> <p>Use <code>kubectl cp</code> to copy files between the pod and your local machine:</p> <pre><code>kubectl cp ./fast-llm-tutorial pod-fast-llm-tutorial:/app\n</code></pre> </li> </ol>"},{"location":"quick-start/#step-2-choose-your-training-configuration","title":"\ud83e\udd16 Step 2: Choose Your Training Configuration","text":"<p>This guide offers two training configurations:</p> SmallBig <p>For a quick, single-node setup and immediate results to test Fast-LLM with a smaller model. Ideal for getting started and understanding the basics. It's the \"hello world\" of Fast-LLM.</p> <p>For a more advanced setup with more data and larger models to explore Fast-LLM's full capabilities. This configuration requires more resources and time to complete, but it prepares you for production-like workloads.</p> <p>Choose based on your goals for this tutorial.</p>"},{"location":"quick-start/#step-3-download-the-pretrained-model","title":"\ud83d\udce5 Step 3: Download the Pretrained Model","text":"SmallBig <p>For the small configuration, we'll use a SmolLM2 model configuration with 135M parameters, which is fast to train. Run the following commands to download the model configuration and tokenizer:</p> <pre><code>git lfs install\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/HuggingFaceTB/SmolLM2-135M ./fast-llm-tutorial/pretrained-model\n</code></pre> <p>For the big configuration, we'll use a Llama model with 8B parameters. We'll grab the model from the HuggingFace Hub and save it to our inputs folder.</p> <p>Access Required</p> <p>Meta gates access to their Llama models. You need to request access to the model from Meta before you can download it at https://huggingface.co/meta-llama/Llama-3.1-8B. You'll need to authenticate with your HuggingFace account to download the model:</p> <pre><code>pip install huggingface_hub\nhuggingface-cli login\n</code></pre> <p>When asked for whether to use this as git credentials, answer in the affirmative.</p> <pre><code>git lfs install\ngit clone https://huggingface.co/meta-llama/Llama-3.1-8B ./fast-llm-tutorial/pretrained-model\n</code></pre>"},{"location":"quick-start/#step-3-prepare-the-training-data","title":"\ud83d\udcda Step 3: Prepare the Training Data","text":"<p>For this tutorial, we'll use text from the OpenWebText dataset. This dataset is a free approximation of the WebText data OpenAI used for GPT-2, and it's perfect for our test run!</p> <p>Create a configuration file for the dataset preparation. Save the following as `./fast-llm-tutorial/prepare-config.yaml``:</p> SmallBig <pre><code>output_path: fast-llm-tutorial/dataset\n\nloading_workers: 16  # (1)!\ntokenize_workers: 16\nsaving_workers: 16\n\ndataset:\n  path: stas/openwebtext-10k  # (2)!\n  split: \"train\"\n  trust_remote_code: true\n\ntokenizer:\n  path: fast-llm-tutorial/pretrained-model\n\nsplits:  # (3)!\n  training: 0.9\n  validation: 0.1\n</code></pre> <ol> <li>Processing speed scales linearly with the number of CPUs.</li> <li>This small dataset restricts to the first 10K records of the OpenWebText dataset to speed up the process. If you want to use the full dataset, replace with <code>openwebtext</code>.</li> <li>90% train, 10% validation. These settings need to be adjusted based on the size of your dataset.</li> </ol> <pre><code>output_path: fast-llm-tutorial/dataset\n\nloading_workers: 128  # (1)!\ntokenize_workers: 128\nsaving_workers: 128\n\ndataset:\n  path: openwebtext\n  split: train\n  trust_remote_code: true\n\ntokenizer:\n  path: fast-llm-tutorial/pretrained-model\n\nsplits:  # (2)!\n  training: 0.99\n  validation: 0.01\n</code></pre> <ol> <li>Processing speed scales linearly with the number of CPUs.</li> <li>99% train, 1% validation. These settings need to be adjusted based on the size of your dataset.</li> </ol> <p>Fast-LLM ships with a <code>prepare</code> command that will download and preprocess the dataset for you.</p> Prebuilt DockerCustom InstallationSlurmKubeflow <p>Run data preparation with the following command:</p> <pre><code>fast-llm prepare gpt_memmap --config fast-llm-tutorial/prepare-config.yaml\n</code></pre> <p>Run data preparation with the following command:</p> <pre><code>fast-llm prepare gpt_memmap --config fast-llm-tutorial/prepare-config.yaml\n</code></pre> <p>Run data preparation with the following command:</p> <pre><code>sbatch &lt;&lt;EOF\n#!/bin/bash\n# SBATCH --job-name=fast-llm-prepare\n# SBATCH --nodes=4\n# SBATCH --ntasks-per-node=1\n# SBATCH --exclusive\n# SBATCH --output=/app/fast-llm-tutorial/prepare-output.log\n# SBATCH --error=/app/fast-llm-tutorial/prepare-error.log\n\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=8001\n\nexport PYTHONHASHSEED=0\n\nsrun \\\n    --container-image=\"ghcr.io/servicenow/fast-llm:latest\" \\\n    --container-mounts=\"$(pwd)/fast-llm-tutorial:/app/fast-llm-tutorial\" \\\n    --container-env=\"PYTHONHASHSEED\" \\\n    --ntasks-per-node=$SLURM_NTASKS_PER_NODE \\\n    bash -c \"\n        torchrun --rdzv_backend=static \\\n                 --rdzv_id=0 \\\n                 --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                 --node_rank=\\\\$SLURM_NODEID \\\n                 --nproc_per_node=\\\\$SLURM_NTASKS_PER_NODE \\\n                 --nnodes=\\\\$SLURM_NNODES:\\\\$SLURM_NNODES \\\n                 --max_restarts=0 \\\n                 --rdzv_conf=timeout=3600 \\\n                 --no_python \\\n                 fast-llm prepare gpt_memmap \\\n                 --config fast-llm-tutorial/prepare-config.yaml\"\nEOF\n</code></pre> <p>You can follow the job's progress by running <code>squeue -u $USER</code> and checking the logs in <code>fast-llm-tutorial/prepare-output.log</code> and <code>fast-llm-tutorial/prepare-error.log</code>, respectively.</p> <p>Copy the files to the shared PVC if they're not already there:</p> <pre><code>kubectl cp ./fast-llm-tutorial pod-fast-llm-tutorial:/app\n</code></pre> <p>Then, run data preparation with the following command:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: \"kubeflow.org/v1\"\nkind: \"PyTorchJob\"\nmetadata:\n  name: \"fast-llm-prepare\"\nspec:\n  nprocPerNode: \"1\"\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_id=0 \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES}:\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm prepare gpt_memmap \\\n                           --config fast-llm-tutorial/prepare-config.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/fast-llm-tutorial\n                  name: fast-llm-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: fast-llm-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-fast-llm-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\n    Worker:\n      replicas: 3\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_id=0 \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES}:\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm prepare gpt_memmap \\\n                           --config fast-llm-tutorial/prepare-config.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/fast-llm-tutorial\n                  name: fast-llm-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: fast-llm-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-fast-llm-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\nEOF\n</code></pre> <p>You can follow the job's progress by running <code>kubectl get pods</code> and checking the logs with <code>kubectl logs fast-llm-prepare-master-0</code>.</p>"},{"location":"quick-start/#step-4-configure-fast-llm","title":"\u2699\ufe0f Step 4: Configure Fast-LLM","text":"<p>Next, we'll create a configuration file for Fast-LLM.</p> <p>FlashAttention</p> <p>Fast-LLM uses FlashAttention by default. If you're using Volta GPUs, you must disable FlashAttention by setting <code>use_flash_attention: no</code> in the configuration file, as shown below.</p> <p>Micro-Batch Size</p> <p>The <code>micro_batch_size</code> in the configuration below is optimized for 80GB GPUs. If you're using GPUs with less memory, you will need to lower this value. Alternatively, you can decrease the <code>sequence_length</code> to reduce the memory footprint.</p> <p>Save the following as <code>fast-llm-tutorial/train-config.yaml</code>:</p> SmallBig <pre><code>training:\n  train_iters: 100  # (1)!\n  logs:\n    interval: 10\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  export:  # (2)!\n    format: llama\n    interval: 100\n  wandb:  # (3)!\n    project_name: fast-llm-tutorial\n    group_name: Small\n    entity_name: null\nbatch:\n  micro_batch_size: 60  # (4)!\n  sequence_length: 1024\n  batch_size: 480  # (5)!\ndata:\n  datasets:\n    training:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_training.yaml  # (6)!\n    validation:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_validation.yaml  # (6)!\noptimizer:\n  learning_rate:\n    base: 6.0e-04\npretrained:\n  format: llama  # (7)!\n  path: fast-llm-tutorial/pretrained-model\n  model_weights: no  # (8)!\nmodel:\n  base_model:\n    transformer:\n      use_flash_attention: yes  # (9)!\n  distributed:\n    training_dtype: bf16  # (10)!\nrun:\n  experiment_dir: fast-llm-tutorial/experiment\n</code></pre> <ol> <li>For the small run, we'll stop after 100 iterations.</li> <li>The trained model will be saved in <code>Transformers</code> Llama format to <code>fast-llm-tutorial/experiment/export/llama/100</code> at the end of the small run. You can also save as  a <code>Fast-LLM</code> checkpoint by setting the <code>format</code> to <code>fast_llm</code>.</li> <li>Entirely optional, but it's a good idea to track your training progress with Weights &amp; Biases. Replace <code>null</code> with your own W&amp;B entity name. If you don't want to use W&amp;B, just ignore this section.</li> <li>Adjust the number of sequences per GPU based on GPU memory. For SmolLM2-135M at 1024 sequenced length and a 80GB GPU, a <code>micro_batch_size</code> of 60 should work well.</li> <li>Must be divisible by the number of GPUs and the <code>micro_batch_size</code>. At 1024 tokens per sequence, 480 corresponds to about 500,000 tokens per batch.</li> <li>Location of the dataset metadata files generated in Step 4.</li> <li>Format of the pretrained model. Since SmolLM is a Llama model, we set this to <code>llama</code>.</li> <li>We'll train SmolLM2-135M from scratch. You can set to <code>yes</code> to continue training from a checkpoint (if you put one in the model directory).</li> <li>By default, Fast-LLM uses FlashAttention for faster training. If you're using Volta GPUs, set this to <code>no</code>.</li> <li><code>bf16</code> (bfloat16, or Brain Floating Point 16) is supported on Ampere GPUs and higher. On Volta GPUs, use <code>fp16</code> (half-precision floating point) for training instead of <code>bf16</code>.</li> </ol> <pre><code>training:\n  train_iters: 100_000  # (1)!\n  logs:\n    interval: 10\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:  # (2)!\n    format: llama\n    interval: 20_000\n  wandb:  # (3)!\n    project_name: fast-llm-tutorial\n    group_name: Big\n    entity_name: null\nbatch:\n  micro_batch_size: 2  # (4)!\n  sequence_length: 4096\n  batch_size: 512  # (5)!\ndata:\n  datasets:\n    training:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_training.yaml  # (6)!\n    validation:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_validation.yaml  # (6)!\noptimizer:  # (7)!\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:  # (8)!\n    base: 6.0e-04\n    minimum: 6.0e-05\n    decay_style: cosine\n    decay_iterations: 100_000\n    warmup_iterations: 2000\npretrained:\n  format: llama  # (9)!\n  path: fast-llm-tutorial/pretrained-model\n  model_weights: yes  # (10)!\nmodel:\n  base_model:\n    transformer:\n      use_flash_attention: yes  # (11)!\n    cross_entropy_impl: fused  # (12)!\n  multi_stage:\n    zero_stage: 2  # (13)!\n  distributed:\n    training_dtype: bf16  # (14)!\nrun:\n  experiment_dir: fast-llm-tutorial/experiment\n</code></pre> <ol> <li>Total number of training tokens will be approximately 210B: 100,000 iterations * 512 * 4096 tokens per batch.</li> <li>A permanent model checkpoint in <code>Transformers</code> Llama format will be saved to <code>fast-llm-tutorial/experiment/export/llama/[iteration]/</code> every 20,000 iterations. You can also save as a <code>Fast-LLM</code> checkpoint by setting the <code>format</code> to <code>fast_llm</code>.</li> <li>Entirely optional, but it's a good idea to track your training progress with Weights &amp; Biases. Replace <code>null</code> with your own W&amp;B entity name. If you don't want to use W&amp;B, just ignore this section.</li> <li>Adjust the number of sequences per GPU based on GPU memory. Considering a 4k token sequence length and 80GB GPUs, a <code>micro_batch_size</code> of 1 should work well.</li> <li>Must be divisible by the number of GPUs and the <code>micro_batch_size</code>. At 4k tokens per sequence, 512 corresponds to about 2.1 million tokens per batch.</li> <li>Location of the dataset metadata file generated in Step 4.</li> <li>These are good default optimizer settings for training models.</li> <li>We are using a cosine decay schedule with linear warmup. After reaching the peak learning rate <code>base</code> at <code>warmup_iterations</code>, the learning rate will decay to <code>minimum</code> at <code>decay_iterations</code>, following a cosine curve. The minimum learning rate should be 1/10<sup>th</sup> of the base learning rate per Chinchilla.</li> <li>Format of the pretrained model. Since it's a Llama model, we set this to <code>llama</code>.</li> <li>We want to continue training Llama-3.1-8B from a checkpoint. If you're training from scratch, set this to <code>no</code>.</li> <li>By default, Fast-LLM uses FlashAttention for faster training. If you're using Volta GPUs, set this to <code>no</code>.</li> <li>Configure Fast-LLM to use the fused cross-entropy loss implementation rather than the default Triton implementation for models with a large vocabulary size such as Llama-3.1-8B. This avoids issues with block size limitations in our current Triton code.</li> <li>We are using ZeRO stage 2 for this tutorial. You can set this to <code>1</code>, <code>2</code>, or <code>3</code> for ZeRO-1, ZeRO-2, or ZeRO-3, respectively.</li> <li><code>bf16</code> (bfloat16, or Brain Floating Point 16) is supported on Ampere GPUs and higher. On Volta GPUs, use <code>fp16</code> (half-precision floating point) for training instead of <code>bf16</code>.</li> </ol>"},{"location":"quick-start/#optional-step-6-add-your-weights-biases-api-key","title":"\ud83d\udd11 (Optional) Step 6: Add Your Weights &amp; Biases API Key","text":"<p>If you included the W&amp;B section in your configuration, you'll need to add your API key. Save it to <code>./fast-llm-tutorial/.wandb_api_key</code> and use the <code>WANDB_API_KEY_PATH</code> environment variable as shown in the training command.</p>"},{"location":"quick-start/#step-7-launch-training","title":"\ud83d\ude80 Step 7: Launch Training","text":"<p>Alright, the big moment! Let's launch the training run.</p> <p>Python Hash Seed</p> <p>The Python hash seed must be set to 0 to ensure consistent, reproducible ordering in hash-dependent operations across processes. Training will fail if this isn't set.</p> Prebuilt DockerCustom InstallationSlurmKubeflow <p>If you have 8 GPUs available, run the following to start training:</p> <pre><code>export PYTHONHASHSEED=0\n# export WANDB_API_KEY_PATH=/app/fast-llm-tutorial/.wandb_api_key\ntorchrun --standalone --nnodes 1 --nproc_per_node=8 --no_python \\\n    fast-llm train gpt --config fast-llm-tutorial/train-config.yaml\n</code></pre> <p>If you have 8 GPUs available, run the following to start training:</p> <pre><code>export PYTHONHASHSEED=0\n# export WANDB_API_KEY_PATH=/app/fast-llm-tutorial/.wandb_api_key\ntorchrun --standalone --nnodes 1 --nproc_per_node=8 --no_python \\\n    fast-llm train gpt --config fast-llm-tutorial/train-config.yaml\n</code></pre> <p>If you have 4 nodes with 8 GPUs each, run the following to start training:</p> <pre><code>sbatch &lt;&lt;EOF\n#!/bin/bash\n# SBATCH --job-name=fast-llm-train\n# SBATCH --nodes=4\n# SBATCH --gpus-per-node=8\n# SBATCH --ntasks-per-node=1\n# SBATCH --exclusive\n# SBATCH --output=/app/fast-llm-tutorial/train-output.log\n# SBATCH --error=/app/fast-llm-tutorial/train-error.log\n\nexport PYTHONHASHSEED=0\nexport WANDB_API_KEY_PATH=/app/fast-llm-tutorial/.wandb_api_key\nexport TORCH_NCCL_ASYNC_ERROR_HANDLING=1\nexport NCCL_DEBUG=INFO\n\nsrun \\\n    --container-image=\"ghcr.io/servicenow/fast-llm:latest\" \\\n    --container-mounts=\"$(pwd)/fast-llm-tutorial:/app/fast-llm-tutorial\" \\\n    --container-env=\"PYTHONHASHSEED,WANDB_API_KEY_PATH,TORCH_NCCL_ASYNC_ERROR_HANDLING,NCCL_DEBUG\" \\\n    --gpus-per-node=\\$SLURM_GPUS_PER_NODE \\\n    --ntasks-per-node=\\$SLURM_NTASKS_PER_NODE \\\n    bash -c \"\n        torchrun --rdzv_backend=static \\\n                 --rdzv_id=0 \\\n                 --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                 --node_rank=\\\\$SLURM_NODEID \\\n                 --nproc_per_node=\\\\$SLURM_GPUS_PER_NODE \\\n                 --nnodes=\\\\$SLURM_NNODES \\\n                 --max_restarts=0 \\\n                 --rdzv_conf=timeout=3600 \\\n                 --no_python \\\n                 fast-llm train gpt \\\n                 --config fast-llm-tutorial/train-config.yaml\"\nEOF\n</code></pre> <p>Copy the configuration file to the shared PVC:</p> <pre><code>kubectl cp ./fast-llm-tutorial/train-config.yaml pod-fast-llm-tutorial:/app/fast-llm-tutorial\n</code></pre> <p>If you have 4 nodes with 8 GPUs each, run the following to start training:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: \"kubeflow.org/v1\"\nkind: \"PyTorchJob\"\nmetadata:\n  name: \"fast-llm-train\"\nspec:\n  nprocPerNode: \"8\"\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  nvidia.com/gpu: 8\n                  rdma/rdma_shared_device_a: 1\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  nvidia.com/gpu: 8\n                  rdma/rdma_shared_device_a: 1\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm train gpt \\\n                           --config fast-llm-tutorial/train-config.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n                - name: WANDB_API_KEY_PATH\n                  value: \"/app/fast-llm-tutorial/.wandb_api_key\"\n                - name: TORCH_NCCL_ASYNC_ERROR_HANDLING\n                  value: \"1\"\n                - name: NCCL_DEBUG\n                  value: \"INFO\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/fast-llm-tutorial\n                  name: fast-llm-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: fast-llm-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-fast-llm-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\n    Worker:\n      replicas: 3\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  nvidia.com/gpu: 8\n                  rdma/rdma_shared_device_a: 1\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  nvidia.com/gpu: 8\n                  rdma/rdma_shared_device_a: 1\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm train gpt \\\n                           --config fast-llm-tutorial/train-config.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n                - name: WANDB_API_KEY_PATH\n                  value: \"/app/fast-llm-tutorial/.wandb_api_key\"\n                - name: TORCH_NCCL_ASYNC_ERROR_HANDLING\n                  value: \"1\"\n                - name: NCCL_DEBUG\n                  value: \"INFO\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/fast-llm-tutorial\n                  name: fast-llm-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: fast-llm-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-fast-llm-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\nEOF\n</code></pre>"},{"location":"quick-start/#step-8-track-training-progress","title":"\ud83d\udcca Step 8. Track Training Progress","text":"Prebuilt DockerCustom InstallationSlurmKubeflow <p>Fast-LLM will log training progress to the console every 10 iterations.</p> <p>You can cancel training at any time by pressing <code>Ctrl+C</code> in the terminal.</p> <p>Fast-LLM will log training progress to the console every 10 iterations.</p> <p>You can cancel training at any time by pressing <code>Ctrl+C</code> in the terminal.</p> <p>Use <code>squeue -u $USER</code> to see the job status. Follow <code>train-output.log</code> and <code>train-error.log</code> in your working directory for logs. Fast-LLM will log training progress to those files every 10 iterations.</p> <p>You can cancel training by running <code>scancel &lt;job_id&gt;</code>.</p> <p>Use <code>kubectl get pods</code> to see the job status. Use <code>kubectl logs fast-llm-train-master-0</code> to check the logs. Fast-LLM will log training progress to the console every 10 iterations.</p> <p>You can cancel training by deleting the PyTorchJob:</p> <pre><code>kubectl delete pytorchjob fast-llm-train\n</code></pre> <p>Cleaning Up Resources</p> <p>Delete the data management pod and PVC if you're finished with the tutorial:</p> <pre><code>kubectl delete pod pod-fast-llm-tutorial\nkubectl delete pvc pvc-fast-llm-tutorial\n</code></pre> <p>This will shut down the temporary pod and remove the PVC with all its contents.</p> <p>You can expect to see the following performance metrics in Fast-LLM's output:</p> SmallBig Performance Metric 8x V100-SXM2-32GB<sup>1</sup> 8x A100-SXM4-80GB<sup>2</sup> 8x H100-SXM5-80GB<sup>3</sup> tokens/s/GPU 16,700 149,000 294,000 tflop/s (model) 15.3 137 268 peak tflop/s (theoretical)<sup>4</sup> 125 312 990 utilization 12.2% 44% 27% total training time 68 minutes 3.9 minutes Performance Metric 32x V100-SXM2-32GB<sup>5</sup> 32x A100-SXM4-80GB<sup>6</sup> 32x H100-SXM5-80GB<sup>7</sup> tokens/s/GPU 10,100 tflop/s (model) 487 peak tflop/s (theoretical)<sup>4</sup> 125 312 990 utilization 49.2% total training time 180 hours <p>If you included the W&amp;B section in your configuration, you can also track your training progress on the Weights &amp; Biases dashboard as well. Follow the link in the console output to view your training run.</p>"},{"location":"quick-start/#final-thoughts","title":"\ud83c\udf89 Final Thoughts","text":"<p>And that's it! You've set up, prepped data, chosen a model, configured training, and launched a full training run with Fast-LLM. You can try out the saved model directly with Transformers.</p> SmallBig <pre><code>import transformers\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"/app/fast-llm-tutorial/experiment/export/llama/100\").cuda()\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"fast-llm-tutorial/pretrained-model/\")\n\ninputs = {k:v.cuda() for k,v in tokenizer(\"This is what the small model can do after fine-tuning for 100 steps:\", return_tensors=\"pt\").items()}\noutputs=model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0]))\n</code></pre> <pre><code>import transformers\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"/app/fast-llm-tutorial/experiment/export/llama/100000\").cuda()\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"fast-llm-tutorial/pretrained-model/\")\n\ninputs = {k:v.cuda() for k,v in tokenizer(\"This is what the big model can do after fine-tuning for 100K steps:\", return_tensors=\"pt\").items()}\noutputs=model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0]))\n</code></pre> <p>From here, feel free to tweak the model, try out larger datasets, or scale things up to larger clusters. The sky's the limit!</p> <p>Happy training!</p> <ol> <li> <p>Precision was set to <code>fp16</code>, since <code>bf16</code> is not supported on V100 GPUs. FlashAttention was disabled, as it is not supported on V100 GPUs. Micro-batch size was set to 12.\u00a0\u21a9</p> </li> <li> <p>Precision was set to <code>bf16</code>. FlashAttention was enabled. Micro-batch size was set to 60.\u00a0\u21a9</p> </li> <li> <p>Precision was set to <code>bf16</code>. FlashAttention was enabled. Micro-batch size was set to 60.\u00a0\u21a9</p> </li> <li> <p>Theoretical peak performance of the GPU for dense tensors in <code>fp16</code> or <code>bf16</code> precision, depending on the GPU architecture. Source: Wikipedia.\u00a0\u21a9\u21a9</p> </li> <li> <p>Precision was set to <code>fp16</code>, since <code>bf16</code> is not supported on V100 GPUs. FlashAttention was disabled, as it is not supported on V100 GPUs. Micro-batch size was set to 4.\u00a0\u21a9</p> </li> <li> <p>Precision was set to <code>bf16</code>. FlashAttention was enabled. Micro-batch size was set to 1. ZeRO stage 2 was used.\u00a0\u21a9</p> </li> <li> <p>Precision was set to <code>bf16</code>. FlashAttention was enabled. Micro-batch size was set to 2. ZeRO stage 2 was used.\u00a0\u21a9</p> </li> </ol>"},{"location":"contributing/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Fast-LLM! We're thrilled to have you here, and your support is invaluable in helping us accelerate LLM training to full speed. This guide will walk you through the steps to contribute, from reporting issues to submitting changes and setting up your development environment.</p> <p>If you have questions or want to start a discussion, feel free to open a discussion on our GitHub page.</p>"},{"location":"contributing/contributing/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started with contributing to Fast-LLM, follow these steps to set up your environment:</p> <ol> <li>Learn Our Development Practices: Get familiar with our development best practices, which cover development setup, testing, and benchmarking.</li> <li>Read the Style Guide: Follow our style guide to maintain consistency in code style, documentation, and commit messages.</li> </ol>"},{"location":"contributing/contributing/#how-to-report-a-bug","title":"\ud83d\udc1e How to Report a Bug","text":"<p>Found a bug? Let's squash it together! Open an issue and select \"Bug report.\" Please include as much information as possible:</p> <ul> <li>Steps to reproduce the issue.</li> <li>What you expected to happen versus what actually happened.</li> <li>Logs, Fast-LLM configuration, and error messages.</li> <li>Details about your environment setup (e.g., CUDA hardware, PyTorch version, CUDA version).</li> </ul> <p>If you're familiar with the codebase, consider adding a failing unit test to demonstrate the problem (optional, but helpful!).</p>"},{"location":"contributing/contributing/#proposing-changes","title":"\ud83d\udee0\ufe0f Proposing Changes","text":"<p>Before diving into code, open an issue to discuss your proposal. This is especially important if you're planning significant changes or adding new dependencies. Once your idea is approved, follow these steps:</p> <ol> <li>Fork the Repository: Fork Fast-LLM to your own GitHub account.</li> <li>Clone Your Fork Locally: Use <code>git clone</code> to bring the code to your local machine.</li> <li>Create a New Branch: Name your branch descriptively, such as <code>fix/training-memory-leak</code> or <code>feature/rope-scaling</code>.</li> <li>Make Your Changes: Work your magic! Don't forget to add or update tests, benchmarks, or configurations as needed.</li> <li>Push to Your Fork: Push the branch to your GitHub fork.</li> <li>Open a Pull Request: Submit a pull request to the <code>main</code> branch. Reference the original issue number and provide a brief summary of your changes.</li> </ol>"},{"location":"contributing/contributing/#guidelines-for-a-successful-pull-request","title":"\ud83c\udfc6 Guidelines for a Successful Pull Request","text":"<p>Here are some tips to ensure your pull request gets reviewed and merged promptly:</p> <ul> <li>Follow our coding standards: Stick to our style guide and conventions to keep the code clean and consistent.</li> <li>Write tests: Verify your changes with unit tests for new features or bug fixes. See our testing guide for tips and recommendations on testing.</li> <li>Test on GPUs and real-world workloads: Since Fast-LLM is all about training large language models, make sure your changes work smoothly in GPU environments and on typical training setups.</li> <li>Run benchmarks and performance tests: Make sure your changes don't slow things down. If there's any impact on performance, provide benchmark results to back it up.</li> <li>Avoid introducing new issues: Check that there are no new runtime warnings, type checker errors, linting problems, or unhandled edge cases.</li> <li>Comment non-trivial code: Make your code easy to understand for others.</li> <li>Keep sensitive data out: Make sure your code or commit messages don't expose private or proprietary information.</li> <li>Use a clear and descriptive title: The PR title should summarize the key change or feature introduced. Avoid vague titles like \"Fix bug\" or \"Update code.\" Start with a keyword like <code>[feat]</code>, <code>[fix]</code>, <code>[docs]</code>, etc. to categorize the change. Reference the issue number if applicable (e.g., <code>[fix] resolve #123 memory leak in training loop</code>). This title will become the commit message for the squashed merge.</li> <li>Use the PR template: Complete the checklist to make sure everything is in order before hitting submit.</li> <li>Make sure all tests pass before merging: Run the tests with <code>pytest tests/ -v -ra -n 10</code>, and fix any failure before merging. If possible, please run the test in an environment with at least 4 GPUs. See our testing guide for more details on testing and debugging.</li> </ul>"},{"location":"contributing/contributing/#seeking-help-or-clarification","title":"\ud83c\udd98 Seeking Help or Clarification","text":"<p>If you're unsure about something or need help, you've got options:</p> <ul> <li>GitHub Discussions: Start a discussion if you need advice or just want to chat.</li> <li>Project Maintainers: Mention a maintainer in an issue or pull request if you need a review or guidance.</li> </ul>"},{"location":"contributing/contributing/#contributors","title":"\ud83c\udf1f Contributors","text":"<p>We're grateful for all the awesome contributors who help make Fast-LLM better. Join our contributors' list and make your first contribution!</p> <p>To learn more about the team and maintainers, visit our About page.</p>"},{"location":"contributing/dev-practices/","title":"Development Practices","text":"<p>Warning</p> <p>Work in progress! Check back soon for the updated content.</p>"},{"location":"contributing/dev-practices/#recommended-development-setup","title":"Recommended Development Setup","text":"<p>Stay tuned...</p>"},{"location":"contributing/dev-practices/#testing-and-benchmarking","title":"Testing and Benchmarking","text":"<p>Stay tuned...</p>"},{"location":"contributing/how-to-release/","title":"How to Release Fast-LLM: A Step-by-Step Guide","text":"<p>This document walks you through the process of creating a new release of Fast-LLM. We follow these steps to keep releasing smooth, consistent, and hassle-free.</p>"},{"location":"contributing/how-to-release/#release-policy","title":"Release Policy","text":"<ol> <li> <p>Who's in Charge? Only the maintainer is authorized to create and publish releases. This ensures consistency, quality, and accountability.</p> </li> <li> <p>Teamwork Makes the Dream Work: Contributors with write access can propose changes and prep the repository for a release (steps 1 and 2 below in the \"Release Process\" section). But tagging and publishing the release? That's the maintainer's job.</p> </li> <li> <p>Versioning Made Simple: Fast-LLM sticks to Semantic Versioning (aka semver). Here's the gist:</p> <ul> <li>MAJOR versions (like <code>1.0.0</code>) are for big, stable, feature-complete milestones. Since we're still in pre-1.0 territory, we don't have these yet.</li> <li>MINOR versions (e.g., <code>0.2.0</code>) introduce new features and may include breaking changes, as we are in the pre-1.0 phase of development. While we strive for backward compatibility where feasible, breaking changes are acceptable until we reach 1.0.0. MINOR releases are the main focus of our current development efforts. They're tied to milestones and are released on a regular schedule.</li> <li>PATCH versions (e.g., <code>0.2.1</code>) squash bugs and include small, critical fixes without introducing new functionality. These releases are based on stable <code>main</code> commits and are the recommended choice for production-like use cases and important experiments. While we encourage internal and adventurous users to test <code>main</code>, PATCH releases ensure stability for users who need reliability.</li> </ul> </li> <li> <p>Milestones are for MINOR Releases: Each milestone corresponds to a MINOR version (<code>0.2.0</code>, <code>0.3.0</code>, etc.) and includes all issues and pull requests targeted for that release. Milestones have due dates and are used to track progress toward the next MINOR release. PATCH releases? Handled as individual issues or small groups of issues.</p> </li> <li> <p>All Roads Lead to <code>main</code>: Active development happens on the <code>main</code> branch, which may include breaking changes. For production experiments or stability-critical use cases, use the latest PATCH or MINOR release. Internally, we encourage testing <code>main</code> to identify issues early, but important experiments should always use tagged releases to ensure reproducibility and compatibility.</p> </li> </ol>"},{"location":"contributing/how-to-release/#release-process","title":"Release Process","text":""},{"location":"contributing/how-to-release/#1-get-ready-to-release","title":"1. Get Ready to Release","text":"<p>Before tagging anything, make sure the repository is in tip-top shape:</p> <ol> <li>Close or defer all issues in the current milestone (where applicable).</li> <li>Verify that all targeted pull requests are merged.</li> <li> <p>Double-check the repo:</p> <ul> <li>All tests should pass.</li> <li>The documentation should be up to date.</li> <li>Pull requests should have appropriate labels for release notes.</li> </ul> </li> <li> <p>Decide if unresolved bugs need fixing before the release.</p> </li> </ol>"},{"location":"contributing/how-to-release/#2-update-the-version","title":"2. Update the Version","text":"<ol> <li> <p>Update the version in <code>__init__.py</code> and <code>setup.cfg</code>:</p> <pre><code># __init__.py\n__version__ = \"0.2.0\"  # Update this to the new version.\n</code></pre> <pre><code># setup.cfg\nversion = \"0.2.0\"  # Update this to the new version.\n</code></pre> </li> <li> <p>Commit the version bump:</p> <pre><code>git add __init__.py setup.cfg\ngit commit -m \"Bump version to 0.2.0\"\n</code></pre> </li> </ol>"},{"location":"contributing/how-to-release/#3-tag-it","title":"3. Tag It","text":"<ol> <li> <p>Create a new Git tag:</p> <pre><code>git tag -a v0.2.0 -m \"Release version 0.2.0\"\n</code></pre> </li> <li> <p>Push the tag to GitHub:</p> <pre><code>git push origin v0.2.0\n</code></pre> </li> </ol>"},{"location":"contributing/how-to-release/#4-draft-a-release-on-github","title":"4. Draft a Release on GitHub","text":"<ol> <li>Head to the Releases section in the Fast-LLM GitHub repository.</li> <li>Click Create a new release.</li> <li>Under Choose a tag, select the tag you just pushed (e.g., <code>v0.2.0</code>).</li> <li>Use GitHub's automatic release note generation feature by clicking Generate release notes to create release notes based on merged pull requests and commits since the last release.</li> <li>Customize the release notes as needed by highlighting key changes and features.</li> <li>Activate the Create a discussion for this release option to allow users to ask questions and provide feedback.</li> <li>Click Publish release to make the release public.</li> </ol>"},{"location":"contributing/how-to-release/#5-check-the-cicd-pipeline","title":"5. Check the CI/CD Pipeline","text":"<ol> <li>Confirm all CI workflows for the tagged version are green (including tests, docker builds, documentation).</li> <li>Verify that the release artifacts (e.g., Docker images) are available.</li> <li>Ensure updated documentation is live.</li> </ol>"},{"location":"contributing/how-to-release/#6-post-release-checklist","title":"6. Post-Release Checklist","text":"<ol> <li>Spread the Word: Announce the release across Fast-LLM's communication channels, which includes the GitHub Discussions forum and the discussion thread for the release created in step 4.</li> <li> <p>After the release is before the release: Prep for the next version:</p> <ul> <li> <p>Update the <code>__version__</code> string and <code>setup.cfg</code> to reflect the next development version (e.g., <code>0.2.1-dev</code>):</p> <pre><code># __init__.py\n__version__ = \"0.2.1-dev\"\n</code></pre> <pre><code># setup.cfg\nversion = \"0.2.1-dev\"\n</code></pre> </li> <li> <p>Commit and push the changes:</p> <pre><code>git add __init__.py setup.cfg\ngit commit -m \"Start development on version 0.2.1\"\ngit push origin main\n</code></pre> </li> </ul> </li> <li> <p>Update milestones:</p> <ul> <li>Close the milestone for the release (e.g., <code>0.2.0</code>).</li> <li>Create a new milestone for the next MINOR release (e.g., <code>0.3.0</code>).</li> </ul> </li> </ol>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>This section collects general coding style guidelines used in Fast-LLM. Following these will ensure a swift reviewing process and will help maintain consistency and readability. Note that while we try to enforce these principles, exceptions may be allowed on a case-by-case basis, for example if they noticeably improve readability.</p> <p>As a general principle, Fast-LLM prioritizes code readability and maintainability over conciseness, coding speed or individual programmer's preferences. Most of the style choices below are based on this principle.</p>"},{"location":"contributing/style-guide/#basic-style","title":"\ud83c\udfaf Basic Style","text":"<p>Unless otherwise specified, Fast-LLM follows the PEP 8 coding style. This style (and many other conventions) is enforced with automatic formatting through a pre-commit git hook.</p> <p>Please make sure these git hooks are installed by running</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>More on automated formatting</p> <p>Fast-LLM's automated formatting includes Black, isort, autoflake, and a few other packages. See Fast-LLM's pre-commit configuration for more details.</p>"},{"location":"contributing/style-guide/#naming-conventions","title":"\ud83d\udcda Naming Conventions","text":"<p>In addition to PEP 8, we use the following naming conventions for python identifiers (classes, variables, methods, modules, etc.), file names and configuration parameters. For example:</p> <ul> <li>Use meaningful, self-descriptive identifier names (ex. <code>x -&gt; loss</code>). Abstract variable names such as <code>x</code> are however OK for generic methods where more descriptive names aren't appropriate (ex. <code>add(x, y)</code>).</li> <li>Please avoid abbreviations, especially domain-specific ones. This gives everyone a chance to understand the code, regardless of their prior knowledge. Ex. <code>bs -&gt; batch_size</code>.</li> <li>Try to keep names concise, for example by eliminating redundancies and avoiding data type qualifiers such as <code>num</code> (covered by the type hint). This is especially important for configuration parameters as the fully qualified names can get very long. For example, <code>transformer.num_transformers_heads</code> can be simplified to <code>transformer.heads</code> without sacrificing clarity.</li> </ul> <p>Note that these conventions are especially important on user-facing names which are more difficult to change, for example configuration parameters and the public interface of core classes and modules.</p> <p>Why this matters</p> <p>Using explicit, self-explanatory names gives other users a better chance to understand the code, regardless of their prior knowledge, which facilitates collaboration and maintenance. Our conventions follow this principle, while attempting to avoid excessively long names.</p>"},{"location":"contributing/style-guide/#imports","title":"\ud83d\udeec Imports","text":"<p>We use the following conventions for imports (other than those enforced by isort):</p> <ul> <li>Import standard library and third party modules by module (ex. <code>import package.module</code>, not <code>from package.module import method</code>). In addition to keeping the code consistent, this keeps identifier's origin explicit so anyone can tell where it came from with just a quick glance at the code.</li> <li>Avoid renaming with <code>as</code>, except for some (arbitrarily chosen) common ones: <code>numpy as np</code>, <code>triton.language as tl</code>.</li> <li>Import first-party modules through specific identifiers (ex. <code>from fast_llm.module import method</code>, not <code>import fast_llm.module</code>). This keeps Fast-LLM identifiers to a manageable length and makes it easier to track what is used in a given file.</li> <li>Always use absolute imports (ex. no <code>from .module import method</code>)</li> <li>Include all explicitly-imported third-party module to <code>setup.cfg</code>. Only add new requirements if they provide a substantial benefit, as we try to keep the requirements to a minimum.</li> <li>Prefer file-level imports over imports inside methods, unless they significantly slow down the import process or concern an optional dependency that should not be absolutely required to import the module (ex. <code>transformers</code>). If an offending import is only required for a type hint, include it in a <code>if typing.TYPE_CHECKING:</code> block.</li> </ul> <p>Why this matters</p> <p>Most python conventions make no clear recommendation concerning imports, which can easily lead to inconsistent import formats across a repo, and can make it harder to understand. Our conventions aim to avoid these arbitrary choices by providing an explicit prescription, which should be good enough nearly everywhere. Our choice is justified as follows:</p> <ul> <li>For third-party and standard library packages, fully qualified identifiers are typically relatively short, so it makes sense to keep them. This also keeps identifier's origin explicit so anyone can tell where it came from with just a quick glance at the code. This is especially useful for identifiers that with otherwise ambiguous source (ex. <code>float32</code> may come from torch, numpy, triton, etc.; Fast-LLM's configuration scheme has many identifiers in common with <code>dataclasses</code>, <code>omegaconf</code> and <code>pydantic</code>)</li> <li>For first-package, fully qualified names are generally too long to use in code, since they include the entire directory structure to the Fast-LLM, so first-party identifiers need to be imported by name. There should be very little ambiguity, because name clashes are uncommon within Fast-LLM, and external identifiers are already clearly marked as such.</li> </ul> <p>Configuration modules</p> <p>Fast-LLM supports instantiation and validation of configurations with a barebone installation. Because of this, modules that contain configuration classes (usually named <code>config.py</code>) should not include any top-level third-party import (except for those installed in the barebone install), and the same applies for configuration initialization and validation methods. Third-party import may be included in other methods if needed.</p>"},{"location":"contributing/style-guide/#public-and-private-interface","title":"\ud83d\udd13 Public and Private Interface","text":"<p>We use the following conventions for class and module interfaces:</p> <ul> <li>Mark private and protected variables with an underscore <code>_</code> prefix. As is customary in python, we make no distinction between the two and avoid the double-underscore <code>__</code> notation.</li> <li>Keep public interfaces (methods and variables without underscore prefix) as lean as possible, i.e. mark everything as private/protected unless there is a clear need to make it public. We can always add to the public interface later, but removing from it is difficult.</li> <li>Use accessors sparingly through the <code>@property</code> decorator or equivalent, usually to define read-only public variables.</li> </ul> <p>Why this matters</p> <p>Although good practices of object-oriented programming are generally ignored in python, Fast-LLM attempts to follow them to an extent, while avoiding unnecessary bloat. Public interfaces are expected to be stable, which make further modifications difficult as they could break external code. On the other hand, private interface are freely modifiable, which provides more freedom for fixes, improvement, refactoring, etc. Therefore, having lean public interfaces is critical for us to keep maintaining and improving Fast-LLM.</p>"},{"location":"contributing/style-guide/#type-hints","title":"\ud83d\udca1 Type Hints","text":"<p>Fast-LLM uses type hints for several reasons, including code readability, type checking in IDEs, and type validation for configurations:</p> <ul> <li>Always use type hints for the public interface of a classes and modules. Type hints for method outputs may be omitted if they can be trivially inferred, ex. if they return the input, an explicitly typed variable or nothing.</li> <li>Prefer using type hints in private interfaces, especially if it improves readability and/or static type checking.</li> <li>Prefer newer type hint formats over older ones, ex. <code>typing.List -&gt; list</code>, <code>typing.Union(A,B) -&gt; A | B</code>.</li> </ul> <p>Why this matters</p> <p>We use type hints for various reasons. In addition to making the code more understandable, they are used by IDEs such as VS Code or PyCharm to perform static type checking, which speeds up development and is essential to keeping the code bug-free.</p>"},{"location":"contributing/style-guide/#misc","title":"\ud83d\uddd1\ufe0f Misc","text":"<ul> <li>Please add descriptions and comments as needed, especially for parts that would otherwise be difficult to understand.</li> <li>Please favor <code>pathlib</code> over <code>os.path</code> for file path operations because it offers a cleaner and more modern API.</li> <li>We encourage the use of modern python features when beneficial, up to the minimum python version (3.12).</li> </ul>"},{"location":"contributing/testing/","title":"Writing and running tests","text":""},{"location":"contributing/testing/#debugging-with-tests","title":"Debugging with tests","text":""},{"location":"contributing/testing/#selecting-tests","title":"Selecting tests","text":"<p>When debugging, it is often advisable to target specific tests that can be executed efficiently. Although Pytest allows targeting specific tests or files, complex parameterization and dependencies in our suite often make explicit selection difficult. To address this, several options for test selection are available:</p> <ul> <li><code>--skip-slow</code>: Executes a subset of expedited tests that encompass much of the codebase. This option is effective for quickly checking for major regressions prior to executing the comprehensive test suite. Please note, parallel testing (<code>-n</code>) is typically unnecessary\u2014and may even be counterproductive\u2014when using this argument.</li> <li><code>--run-extra-slow</code>: Certain tests are disabled by default due to their lengthy execution times (e.g., complex integration tests) or limited criticality. Use this flag to re-enable them.</li> <li><code>--models MODEL0 MODEL1 ...</code>: Enables targeting of one or more specific models within the model testing suite. This feature is particularly useful during model-specific debugging efforts. For instance, running <code>pytest tests/models/test_models/test_checkpoint.py -v -ra --models llama</code> will specifically test checkpointing functionality for the llama model. Note that parallelization (<code>-n</code>) may be unnecessary in this context, as model tests for a given model are only partially distributed due to dependency constraints.</li> </ul>"},{"location":"contributing/testing/#monitoring-distributed-tests","title":"Monitoring distributed tests","text":"<p>Distributed tests are generally the slowest due to the overhead associated with starting processes and process groups. To mitigate this, Fast-LLM incorporates several bundled tests that execute multiple subtests within a single subprocess call. As bundled calls can generate substantial output and potentially reduce report readability, Fast-LLM captures the output from each subtest and forwards it to an associated test. If necessary, this output capture can be disabled using <code>--no-distributed-capture</code>\u2014for instance, if a severe crash hinders output capture or to disable pytest capture entirely (<code>-s</code>). Captured logs are stored in the testing cache directory; please consult individual tests for specific locations.</p> <p>For example, <code>test_run_model_distributed[llama]</code> tries various distributed configurations for the <code>llama</code> model, each reported under an associated test such as <code>test_model_distributed[llama-distributed]</code>. Should a distributed subtest, say <code>tp2</code> (tensor-parallel), encounter a failure, <code>test_run_model_distributed</code> will log the issue, continue executing remaining subtests, and ultimately raise an error to designate the bundled test as failed. The associated test, <code>test_model_distributed[llama-tp2]</code>, will also fail and display the captured output (retrieved from <code>/tmp/fast_llm_tests/models/llama/tp2/</code>), separated by type (stdout, stderr and traceback) as would happen for a normal test (minus some advanced formating), but also by rank.</p>"},{"location":"contributing/testing/#other-options","title":"Other options","text":"<ul> <li><code>--show-gpu-memory N</code>: Monitors GPU memory use and reports the top N tests (default 10). Mainly helps ensure tests don't exceed memory limits, but results may not be precise.</li> <li><code>--show-skipped</code>: Many tests skipped for obvious reasons (ex. marked as slow or extra slow, skipped model testing groups (see below)) are removed entirely from the report to reduce clutter. Use this flag to display them.</li> </ul>"},{"location":"contributing/testing/#best-practices","title":"Best practices","text":""},{"location":"contributing/testing/#testing-models","title":"Testing models","text":"<p>Model integration tests are the most important part of our testing suite, ensuring that Fast-LLM works and yields consistent results for a variety of models, training configurations, optimizations, etc.</p> <p>For each tested model, we run a series of tests divided into several groups. Much of these tests consist of running a short Fast-LLM training run, then comparing intermediate tensors (ex. parameter initialization, layer outputs and gradients, parameter gradients) against a baseline.</p>"},{"location":"contributing/testing/#adding-a-model","title":"Adding a model","text":"<p>When adding support for a new model that comes with additional features, the simplest option to increase coverage is to add an example configuration to the tested modelsl. In general, adding a model means calling <code>_update_and_add_testing_config</code> with:</p> <ul> <li>A reference model configuration (ex. <code>llama</code>).</li> <li>A name for the new model (ex. <code>my_custom_llama</code>)</li> <li>A list of arguments that make the model unique (ex. <code>[\"model.base_model.my_feature=True\"]</code>)</li> <li>An optional checkpoint format to use for conversion tests (ex. <code>MyCustomLlamaCheckpointFormat</code>)</li> <li>A <code>groups</code> argument detailing the action to take for each testing group.</li> </ul> <p>Here is a detailed example and reference:</p> <pre><code>_update_and_add_testing_config(\n    \"llama\",\n    \"my_custom_llama\",\n    model_type=\"custom_llama_gpt\",\n    extra_args=[\"model.base_model.my_feature=True\"],\n    megatron_args=None, # A list of arguments for the associated Megatron run, if applicable.\n    checkpoint_format=MyCustomLlamaCheckpointFormat,\n    groups={\n        ModelTestingGroup.basic: ModelTestingGroupAction.normal,\n        ModelTestingGroup.checkpoint: ModelTestingGroupAction.normal,\n        ModelTestingGroup.convert: ModelTestingGroupAction.normal,\n        ModelTestingGroup.generate: ModelTestingGroupAction.normal,\n        ModelTestingGroup.megatron: ModelTestingGroupAction.not_implemented,\n        ModelTestingGroup.distributed: ModelTestingGroupAction.unimportant,\n    },\n    compare_factor=2.0, # Optionally adjust comparison thresholds, ex. for a model that comes with unusually high variances.\n)\n</code></pre> <p>Don't forget about unit tests!</p> <p>While adding a model is a quick and efficient way to increase coverage, it is not a replacement for unit tests. The model testing suite performs intensive consistency checks, but does little to make sure those results are correct to begin with. See functional tests and test_lm_head for good examples of unit tests for individual components and an entire layer.</p>"},{"location":"contributing/testing/#reference-for-groups","title":"Reference for groups","text":"<p>Fast-LLM currently supports the following testing groups:</p> <ul> <li><code>basic</code>: Run Fast-LLM training with a baseline configuration to make sure the model can be run. Run a variety of single-gpu configuration (ex. different data type, gradient accumulation) and check that the results are consistent with the baseline. Typically set to <code>normal</code> (runs for all models).</li> <li><code>checkpoint</code>: Test basic checkpoint saving and loading. Typically set to <code>normal</code> (runs for all models).</li> <li><code>convert</code>: Test more advanced checkpoint manipulation that involve conversion between different formats. Typically set to <code>normal</code> (runs for all models that support external checkpoint formats).</li> <li><code>generate</code>: Test generative inference through the Hugging Face model wrapper. Typically set to <code>normal</code> (runs for all models that support it).</li> <li><code>megatron</code>: Compare against an equivalent training run in Megatron-LM. Typically set to <code>not_implemented</code> or <code>unimportant</code>, (slow and only a small selection of models support it).</li> <li><code>distributed</code>: Run a variety of multi-gpu configurations and compare against the baseline. Typically set to <code>unimportant</code>, as it is very slow and resource-intensive.</li> </ul> <p>Each testing group may be associated with one of the following options:</p> <ul> <li><code>main</code>: Indicate that the group will always run, even when testing with <code>--skip-slow</code>. Typically not used for new models.</li> <li><code>normal</code>: Indicate that this group is part of the standard testing suite.</li> <li><code>not_implemented</code>: Indicate that the testing group is not supported and should never run.</li> <li><code>broken</code> Indicate that the testing group is supported, but expected to fail due to a known issue. This test will be disabled unless using with <code>--test-extra-slow</code> so the testing suite isn't flooded with known failures. If using, please make sure that the problem is tracked through a github issue.</li> <li><code>unimportant</code>: Indicate that the test is not worth running as part of the standard testing suite, ex. if all associated features already sufficiently tested through other models and/or groups. It will still run with <code>--test-extra-slow</code>.</li> </ul>"},{"location":"developer_guide/configuration/","title":"Configuration Reference","text":"<p>Warning</p> <p>Looking for more details on Fast-LLM's configuration system? This reference is on the way. Stay tuned!</p>"},{"location":"developer_guide/conversion/","title":"Model Conversion Reference","text":"<p>This reference guide describes all there is to know about Fast-LLM's checkpoint conversion system. After reading this, you should be able to create your own <code>External</code> converter, in Hugging Face format or other. And if you are familiar with the rest of Fast-LLM, you will also be able to create an entirely custom converter.</p> <p>Fast-LLM provides a simple and fully customizable interface to save/load checkpoints and configurations. This same interface is used both by Fast-LLM official checkpoint formats (<code>distributed</code> and <code>fast-llm</code>), and by the checkpoint conversion interface. It can also be used to define entirely new checkpoint formats, though this is generally not recommended.</p> <p>In this guide we focus on the checkpoint conversion interface, in particular for Hugging Face formats, since this is the most common use case.</p>"},{"location":"developer_guide/conversion/#checkpoint-format-metadata","title":"Checkpoint format metadata","text":"<p>When creating a new checkpoint format, the first step is to subclass <code>CheckpointFormat</code>. This data structure holds important properties of the format, and makes them accessible at the configuration level. Some important entries include:</p> <ul> <li><code>name</code>: A name for the format, as will appear for example in configuration files</li> <li><code>support_optimizer</code>: Whether the optimizer state can be included in a checkpoint.</li> <li><code>support_saving</code>, <code>support_loading</code>: This can be used to create read-only or write-only formats.</li> <li><code>get_handler_class()</code>: Return the actual checkpoint conversion class, as we'll soon describe. The class should be imported lazily so the <code>CheckpointFormat</code> remains accessible by configurations.</li> </ul> <p>Here is a simple example:</p> <pre><code>class AwesomeCheckpointFormat(CheckpointFormat):\n    name = \"awesome_checkpoint\"\n    support_optimizer = False\n\n    @classmethod\n    def get_handler_class(cls):\n        from package.module import AwesomeCheckpointHandler\n\n        return AwesomeCheckpointHandler\n</code></pre> <p>Once the metadata class is created, we want to let the model know about it. We do this by adding it to the <code>checkpoint_formats</code> property of the model configuration class. For example:</p> <pre><code>@config_class()\nclass AwesomeModelConfig(FastLLMModelConfig):\n    checkpoint_formats = FastLLMModelConfig.checkpoint_formats + (AwesomeCheckpointFormat,)\n    # ...\n</code></pre> <p>You can see a more complete example in the GPT model source code.</p>"},{"location":"developer_guide/conversion/#external-checkpoint-handler","title":"External checkpoint handler","text":"<p>Now that we've defined a format, we're ready to tackle the actual implementation of an external checkpoint handler. External handlers define a list of converters that can be used to convert configurations and state tensors automatically. They also require an implementation of checkpoint reading and writing, although we already provide such implementation for Hugging Face formats.</p> <p>Supported formats</p> <p>The external checkpoint conversion is principally designed for checkpoint formats that store state tensors in a variable list of <code>Savetensor</code> files. It comes with default saving and loading that handles lazy loading, management of memory usage, safety checks. It is possible to use a more generic format by overriding the <code>save</code> and (in some cases) <code>load</code> methods, but this requires significant effort. Note that we may provide better generalization options at some point in the future.</p> <p>Let's begin an example where we convert our <code>AwesomeModel</code> to its Hugging Face counterpart. The first step is to define a handler class and let it know about our model class:</p> <pre><code>class AwesomeHuggingfaceCheckpointHandler(HuggingfaceStateDictCheckpointHandler):\n    _model: AwesomeModel\n    _model_class= AwesomeModelConfig\n</code></pre>"},{"location":"developer_guide/conversion/#configuration-conversion","title":"Configuration conversion","text":"<p>The configuration conversion utility interfaces between two configurations in the form of nested dictionaries: a serialized Fast-LLM configuration and an external configuration. The <code>_load_config</code> method is expected to read the configuration on disk, as expected by the checkpoint format, and return the same configuration in the forma of a nested dictionary, with <code>_save_config</code> handling the reverse operation. See the Hugging Face implementation for an example.</p> <p>To perform the conversion, the checkpoint handler relies on a list of <code>ParamConverter</code> objects, which describe how individual parameters (or in some case multiple ones) should be converted. The <code>ParamConverter</code> base interface is a dataclass consisting of two variables and two methods:</p> <ul> <li><code>fast_llm_names: tuple[tuple[str, ...], ...]</code>: An array of entry names on the Fast-LLM side, in tuple format. For example, <code>((transformer, head_groups),)</code> refers to the single entry <code>config[\"transformer\"][\"head_groups\"]</code>.</li> <li><code>export_names: tuple[tuple[str, ...], ...]</code>: An array of entry names on the external side, in the same tuple format.</li> <li><code>export_params(self, fast_llm_values: tuple[typing.Any, ...]) -&gt; tuple[typing.Any, ...]</code>: This method takes the configuration parameters corresponding to <code>fast_llm_names</code> (in the same order), and returns converted parameters corresponding to <code>export_names</code>.</li> <li><code>import_params(self, export_values: tuple[typing.Any, ...]) -&gt; tuple[typing.Any, ...]</code>: The converse of<code>export_params</code>, converting parameters corresponding to <code>export_names</code> into those corresponding to <code>fast_llm_names</code>.</li> </ul> <p>While not strictly part of the interface, it may also be useful to define a dataclass <code>__post_init__</code>, for example to restrict the number of parameters in <code>fast_llm_names</code> and <code>export_names</code>.</p> <p>Fast-LLM offers several generic configuration converter classes, including:</p> <ul> <li><code>RenameParamConverter</code>: A simple 1-1 mapping between parameters, with optional renaming but identical value. Typically, most converters are of this type.</li> <li><code>ConstantImportParamConverter</code>: A 1-0 mapping for Fast-LLM parameters that without an equivalent in the external format, that must take a specific value <code>fast_llm_value</code> for conversion to make sense (i.e., they take a hard-coded value in the external format). This type of converter is common for Hugging Face converters, as Hugging Face models support much fewer configuration parameters.</li> <li><code>ConstantExportParamConverter</code>: A 0-1 mapping, the converse of <code>ConstantImportParamConverter</code></li> <li><code>MappedConfigParamConverter</code>: A 1-1 mapping similar to <code>RenameParamConverter</code>, but with a non-trivial relation between values.</li> </ul> <p>In addition to those, you may need to implement your own custom converter. Here is an example that associates several Fast-LLM variables with a tuple.</p> <pre><code>@dataclasses.dataclass(kw_only=True)\nclass PackingParamConverter(ParamConverter):\n    def __post_init__(self):\n        # There may be any number of Fast-LLM variables, but only one external one\n        Assert.eq(len(self.export_names), 1)\n\n    def export_params(self, fast_llm_values):\n        # Pack the values into a single tuple.\n        return (fast_llm_values,)\n\n    def import_params(self, export_values):\n        # Unpack the values. We can safely assume `export_values` has length one because of the assertion in `__post_init__`\n        return export_values[0]\n</code></pre> <p>Now that we've seen how parameter converters work, we're ready to add them to our handler class. We do so by creating a list of converters in the <code>_create_config_converters</code> class method. Continuing our <code>AwesomeModel</code> handler example, we define:</p> <pre><code>    @classmethod\n    def _create_config_converters(cls) -&gt; list[ParamConverter]:\n        # For Hugging Face handlers, we need to call the superclass method.\n        return super()._create_config_converters() + [\n            # A trivial example where both the name and value are the same on both sides.\n            RenameParamConverter(\n                fast_llm_names=((\"vocab_size\",),),\n                export_names=((\"vocab_size\",),),\n            ),\n            # A non-trivial example of `RenameParamConverter` with renaming and handling of nested dictionaries.\n            RenameParamConverter(\n                fast_llm_names=((\"transformer\", \"rotary\", \"theta\"),), export_names=((\"rope_theta\",),)\n            ),\n            # A constant import example indicating that the external format does not support absolute positional embeddings.\n            ConstantImportParamConverter(fast_llm_names=((\"use_position_embeddings\",),), fast_llm_value=False),\n            # The `architectures` parameter is a common use case for `ConstantExportParamConverter` in Hugging Face models.\n            ConstantExportParamConverter(export_names=((\"architectures\",),), export_value=[\"AwesomeModelForCausalLM\"]),\n            # A value mapping example, where we match Fast-LLM activation types with their Hugging Face equivalents.\n            MappedConfigParamConverter(\n                fast_llm_names=((\"transformer\", \"activation_type\"),),\n                export_names=((\"hidden_act\",),),\n                fast_llm_value=ActivationType.from_hf_name,\n                export_value=lambda activation_type: activation_type.hf_name,\n            ),\n            # A more hypothetical example using `PackingParamConverter` to pack two parameters `epsilon_1`, `epsilon_2` into a tuple `eps`.\n            PackingParamConverter(\n                fast_llm_names=((\"epsilon_1\",),(\"epsilon_2\",)),\n                export_names=((\"eps\",),),\n            ),\n        ]\n</code></pre> <p>How conversion works</p> <p>The once the converters are defined, the conversion utility takes it from there. Exporting works as follows (importing work similarly): *The handler creates an empty export config dict, then loops over its list of converters. For each converter, it: *   Reads the value of each parameter defined in <code>fast_llm_names</code>, and gathers them in a tuple. *Calls <code>converter.export_params</code>, providing the set of read values as argument. *   Ensure that the returned value has the correct length (that of <code>export_names</code>) *   Set the respective values in the export config dict.</p> <p>About <code>MISSING</code> and <code>DEFAULT</code></p> <p>If a value is not found during import, it will be replaced by the <code>MISSING</code> tag. The converter's <code>import_params</code> has the opportunity to handle this missing value, and if a <code>MISSING</code>, the handler will throw an error because it does not know what value to set on the Fast-LLM side.</p> <p>The <code>MISSING</code> tag is also supported during export, but has a different meaning as the value is always expected to be found in the Fast-LLM configuration. Instead, <code>export_params</code> may return a <code>MISSING</code> tag indicating that no value should not be added to the Fast-LLM config. It may also return <code>DEFAULT</code>, which will be replaced by the default value for the configuration parameter.</p> <p>Note that the handling of <code>MISSING</code> and <code>DEFAULT</code> is experimental and may be improved in the future.</p>"},{"location":"developer_guide/conversion/#state-conversion","title":"State conversion","text":"<p>State conversion follows the same principle as configuration conversion, but acts on flat dictionaries of state tensors. Converters are defined by subclassing <code>WeightConverter</code>, with the interface:</p> <ul> <li><code>fast_llm_name: str | tuple[str, ...]</code>: An entry name or array of entry names on the Fast-LLM side. For example, <code>((transformer, head_groups),)</code> refers to the single entry <code>config[\"transformer\"][\"head_groups\"]</code>.</li> <li><code>export_name: str | tuple[str, ...]</code>: An entry name or array of entry names on the external side.</li> <li><code>export_weight(self, weight: tuple[torch.Tensor | SafeTensorSlice, ...]) -&gt; tuple[torch.Tensor | SafeTensorSlice, ...]</code>: This method takes the state dict entries corresponding to <code>fast_llm_name</code> (in the same order), and returns converted entries corresponding to <code>export_name</code>.</li> <li><code>import_weight(self, weight: tuple[torch.Tensor | SafeTensorSlice, ...]) -&gt; tuple[torch.Tensor | SafeTensorSlice, ...]</code>: The converse of<code>export_weight</code>, converting state dict entries corresponding to <code>export_name</code> into those corresponding to <code>fast_llm_name</code>.</li> </ul> <p>Fast-LLM offers several generic state dict converter classes, including:</p> <ul> <li><code>WeightConverter</code>: The base class allows for a simple 1-1 mapping between parameters with optional renaming, similar to <code>RenameParamConverter</code>.</li> <li><code>SplitWeightConverter</code>: A 1-N mapping, where a Fast-LLM parameter corresponds to multiple equally-sized chunks in the external side. This happens for example in the MLP, where Hugging Fast keeps the <code>gate</code> and <code>up</code> parts separate, while Fast-LLM combines those in a single tensor to improve performance (and similarly for the multiple experts in the case of MoE).</li> </ul> <p>Since different libraries tend to hold weights in different formats, it is often necessary to define custom converters. Here is an example where a weight needs to be transposed during conversion:</p> <pre><code>class TransposeWeightConverter(WeightConverter):\n    def export_weight(\n        self, weight: tuple[torch.Tensor | SafeTensorSlice, ...]\n    ) -&gt; tuple[torch.Tensor | SafeTensorSlice, ...]:\n        Assert.eq(len(weight), 1)\n        return (weight[0][:].transpose().contiguous(),)\n\n    def import_weight(\n        self, weight: tuple[torch.Tensor | SafeTensorSlice, ...]\n    ) -&gt; tuple[torch.Tensor | SafeTensorSlice, ...]:\n        Assert.eq(len(weight), 1)\n        return (weight[0][:].transpose().contiguous(),)\n</code></pre> <p>We define the list of weight converters in the <code>_create_weight_converters</code> method. Continuing our <code>AwesomeModel</code> handler example, we define:</p> <pre><code>    def _create_weight_converters(self) -&gt; list[WeightConverter]:\n        converters = []\n        # The set of converters may depend on the base model configuration, which is accessible through `self._model.base_model_config`.\n        num_layers = len(self._model.config.base_model.decoder)\n\n        # A simple renaming example, for the word embeddings.\n        converters.append(WeightConverter(\"layers.0.word_embeddings_weight\", \"model.embed_tokens.weight\"))\n\n        # We usually want to loop dynamically over layers\n        for i in range(num_layers):\n            # A `SplitWeightConverter` example, splitting a weight in two.\n            converters.append(SplitWeightConverter(\n                f\"layers.{i + 1}.weight\",\n                (f\"model.layers.{i}.weight_1\", f\"model.layers.{i}.weight_2\"),\n            ))\n        return converters\n</code></pre> <p>And that's it! We're ready to use the new checkpoint format in Fast-LLM. For example, we may set the pretrained and export format in a configuration using</p> <pre><code>training:\n  export:\n    format: awesome_checkpoint\npretrained:\n  format: awesome_checkpoint\n</code></pre>"},{"location":"developer_guide/conversion/#external-converters-beyond-hugging-face","title":"External converters beyond Hugging Face","text":"<p>Warning</p> <p>Coming soon. Stay tuned for new updates!</p>"},{"location":"developer_guide/conversion/#creating-a-custom-checkpoint-format","title":"Creating a custom checkpoint format","text":"<p>Warning</p> <p>Coming soon. Stay tuned for new updates!</p>"},{"location":"developer_guide/model/","title":"Model Reference","text":"<p>Warning</p> <p>Looking to implement your own Fast-LLM model? This reference is on the way. Stay tuned!</p>"},{"location":"recipes/continue-training/","title":"Continual Pretraining of Llama 3.1 8B or Qwen 2.5 7B","text":"<p>In this guide, we provide step-by-step instructions to do continued pretraining on The Stack with Llama 3.1 8B  or Qwen 2.5 7B models.</p>"},{"location":"recipes/continue-training/#preliminary-steps","title":"Preliminary steps","text":"<ul> <li>Quick Start</li> <li>Data preparation</li> </ul>"},{"location":"recipes/continue-training/#download-the-pretrained-model","title":"Download the Pretrained Model","text":"<p>Let's download the model first:</p> Llama 3.1 8BQwen 2.5 7B <pre><code>git lfs install\ngit clone https://huggingface.co/meta-llama/Llama-3.1-8B ./fast-llm-tutorial/pretrained-model\n</code></pre> <pre><code>git lfs install\ngit clone https://huggingface.co/Qwen/Qwen2.5-7B ./fast-llm-tutorial/pretrained-model\n</code></pre>"},{"location":"recipes/continue-training/#training","title":"Training","text":"<p>This is not much different from a pretraining config. We will:</p> <ul> <li>specify the the model checkpoint to load and its format. Fast-LLM will automatically infer the corresponding model architecture.</li> <li>adapt some of the training parameters for our needs.</li> <li>and that's it!</li> </ul> Llama 3.1 8BQwen 2.5 7B <pre><code>training:\n  train_iters: 100_000\n  logs:\n    interval: 10\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:  # (1)!\n    format: llama\n    interval: 20_000\nbatch:\n  micro_batch_size: 2\n  sequence_length: 4096\n  batch_size: 256\ndata:\n  datasets:\n    training:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_training.yaml  # (2)!\n    validation:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_validation.yaml  # (2)!\noptimizer:\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:\n    base: 1.0e-04  # (3)!\n    minimum: 1.0e-05\n    decay_style: cosine\n    decay_iterations: 100_000\n    warmup_iterations: 2000\npretrained:  # (4)!\n  format: llama\n  path: fast-llm-tutorial/pretrained-model\n  model_weights: yes  # (5)!\nmodel:\n  base_model:\n    transformer:\n      use_flash_attention: yes\n    cross_entropy_impl: fused\n  multi_stage:\n    zero_stage: 2\n  distributed:\n    training_dtype: bf16\nrun:\n  experiment_dir: fast-llm-tutorial/Llama-3.1-8B-cpt\n</code></pre> <pre><code>training:\n  train_iters: 100_000\n  logs:\n    interval: 10\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:  # (1)!\n    format: qwen2\n    interval: 20_000\nbatch:\n  micro_batch_size: 1\n  sequence_length: 8192\n  batch_size: 256\ndata:\n  datasets:\n    training:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_training.yaml  # (6)!\n    validation:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_validation.yaml  # (6)!\noptimizer:\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:\n    base: 1.0e-04  # (3)!\n    minimum: 1.0e-05\n    decay_style: cosine\n    decay_iterations: 100_000\n    warmup_iterations: 2000\npretrained:  # (4)!\n  format: qwen2\n  path: fast-llm-tutorial/pretrained-model\n  model_weights: yes  # (5)!\nmodel:\n  base_model:\n    transformer:\n      use_flash_attention: yes\n    cross_entropy_impl: fused\n  multi_stage:\n    zero_stage: 2\n  distributed:\n    training_dtype: bf16\nrun:\n  experiment_dir: fast-llm-tutorial/qwen-2.5-7B-cpt\n</code></pre> <ol> <li>A the model will be saved in Hugging Face format to <code>~/results</code> directory every 20,000 iterations.</li> <li>Location of the dataset metadata file generated in Step 4 of quick start guide.</li> <li>The learning-rate can be used to trade-off between learning and forgetting. A higher learning-rate will learn quickly on our new dataset but will cause forgetting. A lower learning-rate will instead retain more of the pretrained model's knowledge, but will slow down adapting to the new domain.</li> <li>Config of the pretrained model. We load the model downloaded from the repository earlier.</li> <li>This tells Fast-LLM to load the weights of the pretrained model. If we wanted to use the model's configuration, but train from scratch, we could use the same config but set this to <code>no</code>.</li> </ol>"},{"location":"recipes/continue-training/#checkpoint-usage","title":"Checkpoint usage","text":"<p>Checkpoints will be saved regularly, and every 20k steps a checkpoint will be exported in the HF format. You can use it in <code>transformers</code> as you would use the pretrained  model, except this one should be stronger on programming languages!</p> Llama 3.1 8BQwen 2.5 7B <pre><code>from transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"fast-llm-tutorial/pretrained-model\")\npipe = pipeline(\"text-generation\", model=\"fast-llm-tutorial/Llama-3.1-8B-cpt/export/llama/20000/\", tokenizer=tokenizer)\n</code></pre> <pre><code>from transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"fast-llm-tutorial/pretrained-model\")\npipe = pipeline(\"text-generation\", model=\"fast-llm-tutorial/qwen-2.5-7B-cpt/export/qwen2/20000/\", tokenizer=tokenizer)\n</code></pre>"},{"location":"recipes/data-configuration/","title":"Configuring Data for Training","text":"<p>In this section we show how to configure datasets through a series of examples</p> <p>We already saw an example dataset configuration in the quick-start guide, where we prepared a simple dataset and split it into training and validation sub-datasets, and used these to train a small model. This was done by:</p> <ol> <li>Defining a dataset preparation configuration.</li> <li>Running <code>fast-llm prepare</code> with said configuration. This generated some binary files along with two fast-llm configuration files, <code>fast-llm-tutorial/dataset/fast_llm_config_training.yaml</code> and <code>fast-llm-tutorial/dataset/fast_llm_config_validation.yaml</code>.</li> <li> <p>Defining a fast-llm data configuration that use those datasets:</p> <pre><code>data:\n  datasets:\n    training:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_training.yaml\n    validation:\n      type: file\n      path: fast-llm-tutorial/dataset/fast_llm_config_validation.yaml\n</code></pre> </li> <li> <p>Running <code>fast-llm training</code> with said configuration.</p> </li> </ol> <p>In this section we are interested in generalizing step 3. For more details on steps 1 and 2, please refer to the quick-start guide or this example.</p> <p>The section <code>data.datasets</code> holds descriptions of datasets used in training, validation, and testing.</p> <p>The Training and Testing phases must have predetermined dataset names: <code>training</code> and <code>testing</code>, respectively. Each of these phases can have only one dataset.</p> <p>For datasets used for loss evaluator during a validation phase, the rules are different. There can be as many such datasets as needed, and their names are arbitrary. In the example above, the dataset name <code>validation</code> is chosen for simplicity. The datasets names used for validation and their application details are specified in the training config <code>evaluators</code> sections.</p> <p>Adding multiple datasets for loss evaluators in validation phase increases flexibility in tracking the accuracy of your trained model. One possible scenario is using a separate validation dataset for each blended training dataset, allowing you to track training progress on each subset separately and observe how the model performs in real time on different subsets of your training data.</p> <p>Below are examples of how to configure various aspects of training and validation datasets.</p>"},{"location":"recipes/data-configuration/#example-1-blending-multiple-datasets","title":"Example 1: Blending multiple datasets","text":"<p>In this example, we have three datasets and want to sample from each of them during training with probabilities 0.70, 0.25 and 0.05. For this, we use the <code>blended</code> type which takes other datasets as arguments:</p> <pre><code>data:\n  datasets:\n    training:\n      type: blended\n      datasets:\n        - type: file\n          path: path/to/dataset_0.yaml\n        - type: file\n          path: path/to/dataset_1.yaml\n        - type: file\n          path: path/to/dataset_2.yaml\n      weights: [0.70, 0.25, 0.05]\n</code></pre> <p>Dataset wrappers</p> <p>The <code>blended</code> dataset wrapper is one example of the many dataset wrappers available in fast-llm. Such wrappers may be nested (almost) arbitrarily to generate the dataset scheme that fits your needs. Fast-LLM will use the <code>type</code> argument to dynamically select the appropriate configuration class(es). With some effort you can even create your own wrapper!</p>"},{"location":"recipes/data-configuration/#example-2-configure-shuffling","title":"Example 2: Configure shuffling","text":"<p>In this example, we have a large dataset that comes pre-shuffled, so shuffling in unnecessary for the first epoch.</p> <pre><code>data:\n  datasets:\n    training:\n      type: file\n      path: path/to/dataset.yaml\n  sampling:\n    shuffle: skip_first_epoch\n</code></pre>"},{"location":"recipes/data-configuration/#example-3-disable-shuffling-for-validation","title":"Example 3: Disable shuffling for validation","text":"<p>In this example, we want to disable shuffling entirely, but only for the validation dataset. We can do this with the <code>sampled</code> dataset wrapper:</p> <pre><code>data:\n  datasets:\n    training:\n      type: file\n      path: path/to/training_dataset.yaml\n    validation:\n      type: sampled\n      dataset:\n        type: file\n        path: path/to/validation_dataset.yaml\n\n      sampling:\n        shuffle: disabled\n</code></pre> <p>More about sampling configuration</p> <p>Sampling parameters may be globally defined through data configuration (example 2), dataset wrapper(s) (examples 3, 4), or both (example 5). In the case where a dataset sampling is configured with both methods (or multiple nested wrappers), (innermost) wrapper overrides the data (or next-to-innermost wrapper) for the explicitly defined fields (and only those).</p>"},{"location":"recipes/data-configuration/#example-4-set-sampling-seed-for-individual-datasets","title":"Example 4: Set sampling seed for individual datasets","text":"<p>In this example, we have a blend of datasets as in example 1, but we wish to set the seed for each dataset individually for reproducibility reasons. For this, we use the <code>seed</code> field of the <code>sampling</code> wrapper:</p> <pre><code>data:\n  datasets:\n    training:\n      type: blended\n      datasets:\n        - type: sampled\n          dataset:\n            type: file\n            path: path/to/dataset_0.yaml\n          sampling:\n            seed:1234\n        - type: sampled\n          dataset:\n            type: file\n            path: path/to/dataset_0.yaml\n          sampling:\n            seed:2345\n        - type: sampled\n          dataset:\n            type: file\n            path: path/to/dataset_0.yaml\n          sampling:\n            seed:3456\n      weights: [0.70, 0.25, 0.05]\n</code></pre> <p>Default seed</p> <p>In the absence of explicit seed, Fast-LLM uses a default seed (<code>data.sampling</code>'s default) instead, and uses seed shifts to ensure different seeds for each phase and for the various blended datasets.</p>"},{"location":"recipes/data-configuration/#example-5-specifying-multiple-dataset-for-loss-evaluators-during-validation-phase","title":"Example 5: Specifying Multiple Dataset for Loss Evaluators During Validation phase","text":"<p>In this example, we show how to specify multiple  datasets for loss evaluators and configure how often they are applied, along with their usage attributes in the <code>training.evaluators</code> section.</p> <p>Please note that the same dataset names must be used in the <code>training.evaluators</code> section. If a dataset is specified in the <code>datasets</code> section but not in <code>training.evaluators</code>, it will not be used for loss evaluation.</p> <pre><code>training:\n  evaluators:\n    the_stack:\n      interval: 50\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: the_stack\n    fineweb:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 15\n        dataset_name: fineweb\ndata:\n  datasets:\n    the_stack:\n      type: file\n      path: path/to/validation_the_stack_dataset.yaml\n    fineweb:\n      type: file\n      path: path/to/validation_fineweb_dataset.yaml\n</code></pre>"},{"location":"recipes/data-configuration/#example-6-advanced-scenario","title":"Example 6: Advanced scenario","text":"<p>In this example, we combine everything we learned so far to create a complex scenario, where:</p> <ul> <li>The training dataset is a blend consists of two datasets, one of them being itself a blend of three datasets.</li> <li>All datasets except for one come pre-shuffled, so can skip shuffling for the first epoch.</li> <li>We want to set the seed explicitly for the validation and innermost blended datasets, but keep the default seed for the others.</li> </ul> <pre><code>data:\n  datasets:\n    training:\n      type: blended\n      datasets:\n        - type: sampled\n          dataset:\n            type: blended\n            datasets:\n              - type: file\n                # Seed = 1234\n                path: path/to/dataset_0.yaml\n              - type: file\n                # Seed = 1234 + blend_shift, shuffle = skip_first_epoch\n                path: path/to/dataset_1.yaml\n              - type: sampled\n                dataset:\n                  type: file\n                  # Seed = 1234 + 2 * blend_shift, shuffle = epoch\n                  path: path/to/dataset_2.yaml\n                sampling:\n                  # Shuffle each epoch independently (default shuffling)\n                  shuffle: epoch\n          sampling:\n            seed: 1234\n        - type: file\n          # Seed = default + train_shift + 2 * blend_shift, shuffle = skip_first_epoch\n          path: path/to/dataset_3.yaml\n      weights: [0.70, 0.25, 0.05]\n    validation:\n        type: sampled\n        dataset:\n          type: file\n          # Seed = 2345, shuffle = skip_first_epoch\n          path: path/to/validation_dataset.yaml\n        sampling:\n          seed: 2345\n  sampling:\n    shuffle: skip_first_epoch\n</code></pre> <p>Configure from file</p> <p>If a dataset configuration is especially complex and makes the dataset configuration excessively big, or is reused across many experiments, you may want to save it to a yaml file and refer to it un the config using a <code>file</code> dataset. This can be used to reduce the present example to</p> <pre><code>data:\n  datasets:\n    training:\n      type: file\n      path: path/to/training_dataset_config.yaml\n    validation:\n      type: file\n      path: path/to/validation_dataset_config.yaml\n  sampling:\n    shuffle: skip_first_epoch\n</code></pre> <p>In fact, all the elementary datasets from file we've been using so far are of this format, and consist of more elementary <code>memmap</code> datasets optionally wrapped with <code>blended</code> and/or <code>slice</code> wrappers.</p>"},{"location":"recipes/data-preparation/","title":"Preparing Data for Training","text":"<p>If you're wondering if you can use your favorite dataset from Huggingface Datasets with Fast-LLM, the answer is a resounding yes! Let's see how to do that.</p>"},{"location":"recipes/data-preparation/#prerequisites","title":"Prerequisites","text":"<p>For this guide, you would need:</p> <ul> <li> <p>Hardware: Just a machine with CPUs will do. But having a large numbers of CPUs and nodes helps distribute the data preparation job and significantly speed things up.</p> </li> <li> <p>Software: Depending on your setup, you'll need one of the following:</p> <ul> <li>Docker: If you're using the prebuilt Docker image on your local machine.</li> <li>Python 3.10: If you're setting up a custom environment (virtual environment, bare-metal, etc.) on your local machine.</li> <li>Cluster Setup: Access to a Docker-enabled Slurm cluster or to a Kubernetes cluster with Kubeflow if you're using those environments.</li> </ul> </li> </ul>"},{"location":"recipes/data-preparation/#step-1-download-the-dataset-from-huggingface","title":"\ud83d\udcda Step 1: Download the dataset from Huggingface","text":"<p>We'll use the-stack dataset for this tutorial, which is one of the largest collections of permissively-licensed source code files.</p> <p>First, set <code>HF_HOME</code> to your Huggingface cache folder:</p> <pre><code>export HF_HOME=/path/to/hf_cache\n</code></pre> <p>Next, let's create a working folder for this tutorial:</p> <pre><code>mkdir ./prep-stack-tutorial\n</code></pre> <p>Let's create a folder called <code>hf_dataset</code> and download the-stack dataset from huggingface here:</p> <pre><code>mkdir ./prep-stack-tutorial/hf_dataset\nwhile ! huggingface-cli download bigcode/the-stack --revision v1.2 --repo-type dataset --max_workers 64 --local-dir ./prep-stack-tutorial/hf_dataset;\ndo sleep 1; done\n</code></pre> <p>Choice of num_workers</p> <p>Setting a large num_workers sometimes leads to connection errors.</p>"},{"location":"recipes/data-preparation/#step-2-prepare-the-tokenizer-and-configs-for-conversion-of-data-to-fast-llms-memory-mapped-indexed-dataset-format","title":"\u2699\ufe0f Step 2: Prepare the tokenizer and configs for conversion of data to Fast-LLM's memory-mapped indexed dataset format","text":"<p>In this step, we download the tokenizer and create configs required to run the data preparation scripts.</p> <p>We'll use Mistral-Nemo-Base-2407's tokenizer for this tutorial. Let's create a folder first:</p> <pre><code>mkdir -p ./prep-stack-tutorial/checkpoints/Mistral-Nemo-Base-2407\n</code></pre> <p>And then download the tokenizer with this Python script:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mistral-Nemo-Base-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.save_pretrained(\"./prep-stack-tutorial/checkpoints/Mistral-Nemo-Base-2407\")\n</code></pre> <p>Let's create a folder to store the converted dataset:</p> <pre><code>mkdir -p ./prep-stack-tutorial/tokenized/Mistral-Nemo-Base-2407\n</code></pre> <p>Create a config like this -</p> <pre><code>output_path: ./prep-stack-tutorial/tokenized/Mistral-Nemo-Base-2407\n\nloading_workers: 32\ntokenize_workers: 32\nsaving_workers: 32\n\ndataset:\n  path: ./prep-stack-tutorial/hf_dataset\n  split: \"train\"\n  trust_remote_code: true\n\ntokenizer:\n  path: ./prep-stack-tutorial/checkpoints/Mistral-Nemo-Base-2407/tokenizer.json\n</code></pre> <p>Save it as <code>./prep-stack-tutorial/the-stack-prepare.yaml</code></p>"},{"location":"recipes/data-preparation/#step-3-launch-data-preparation-job","title":"\ud83d\ude80 Step 3: Launch data preparation job","text":"<p>Fast-LLM's prepare command processes the dataset by tokenizing and saving it in Fast-LLM's memory-mapped indexed dataset format.</p> Prebuilt DockerCustom InstallationSlurmKubeflow <pre><code>docker run -it --rm ghcr.io/servicenow/fast-llm:latest \\\n    -v ./prep-stack-tutorial:/app/prep-stack-tutorial \\\n    fast-llm prepare gpt_memmap --config /app/prep-stack-tutorial/the-stack-prepare.yaml\n</code></pre> <p>Please follow the instructions in the Quick-Start guide to set up Fast-LLM in your environment.</p> <p>Then, run the following command:</p> <pre><code>fast-llm prepare gpt_memmap --config ./prep-stack-tutorial/the-stack-prepare.yaml\n</code></pre> <pre><code>sbatch &lt;&lt;EOF\n#!/bin/bash\n# SBATCH --job-name=fast-llm-stack-prepare\n# SBATCH --nodes=4\n# SBATCH --ntasks-per-node=1\n# SBATCH --exclusive\n# SBATCH --output=/app/prep-stack-tutorial/prepare-output.log\n# SBATCH --error=/app/prep-stack-tutorial/prepare-error.log\n\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=8001\n\nexport PYTHONHASHSEED=0\n\nsrun \\\n    --container-image=\"ghcr.io/servicenow/fast-llm:latest\" \\\n    --container-mounts=\"$(pwd)/prep-stack-tutorial:/app/prep-stack-tutorial\" \\\n    --container-env=\"PYTHONHASHSEED\" \\\n    --ntasks-per-node=$SLURM_NTASKS_PER_NODE \\\n    bash -c \"\n        torchrun --rdzv_backend=static \\\n                 --rdzv_id=0 \\\n                 --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                 --node_rank=\\\\$SLURM_NODEID \\\n                 --nproc_per_node=\\\\$SLURM_NTASKS_PER_NODE \\\n                 --nnodes=\\\\$SLURM_NNODES:\\\\$SLURM_NNODES \\\n                 --max_restarts=0 \\\n                 --rdzv_conf=timeout=3600 \\\n                 --no_python \\\n                 fast-llm prepare gpt_memmap \\\n                 --config /app/prep-stack-tutorial/the-stack-prepare.yaml\"\nEOF\n</code></pre> <p>You can follow the job's progress by running <code>squeue -u $USER</code> and checking the logs in <code>prep-stack-tutorial/prepare-output.log</code> and <code>prep-stack-tutorial/prepare-error.log</code>, respectively.</p> <p>First, you need a shared PVC to store the dataset. If you haven't already, create a shared PVC to store the dataset:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: pvc-prep-stack-tutorial\nspec:\naccessModes:\n  - ReadWriteMany\nresources:\n  requests:\n    storage: 100Gi\nEOF\n</code></pre> <p>Next, create a pod to copy the files to the shared PVC:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod-prep-stack-tutorial\nspec:\ncontainers:\n  - name: busybox\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n      - mountPath: /app\n        name: prep-stack-tutorial\nvolumes:\n  - name: prep-stack-tutorial\n    persistentVolumeClaim:\n      claimName: pvc-prep-stack-tutorial\nEOF\n</code></pre> <p>Now, copy the files to the shared PVC:</p> <pre><code>kubectl cp ./prep-stack-tutorial pod-prep-stack-tutorial:/app\n</code></pre> <p>You can shut down the pod after copying the files now:</p> <pre><code>kubectl delete pod pod-prep-stack-tutorial\n</code></pre> <p>Then, run data preparation with the following command:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: \"kubeflow.org/v1\"\nkind: \"PyTorchJob\"\nmetadata:\n  name: \"fast-llm-stack-prepare\"\nspec:\n  nprocPerNode: \"1\"\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_id=0 \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES}:\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm prepare gpt_memmap \\\n                           --config prep-stack-tutorial/the-stack-prepare.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/prep-stack-tutorial\n                  name: prep-stack-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: prep-stack-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-prep-stack-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\n    Worker:\n      replicas: 3\n      restartPolicy: Never\n      template:\n        spec:\n          tolerations:\n            - key: nvidia.com/gpu\n              value: \"true\"\n              operator: Equal\n              effect: NoSchedule\n          containers:\n            - name: pytorch\n              image: ghcr.io/servicenow/fast-llm:latest\n              resources:\n                limits:\n                  memory: \"1024Gi\"\n                  cpu:\n                requests:\n                  memory: \"1024Gi\"\n                  cpu: 128\n              command:\n                - /bin/bash\n                - -c\n                - |\n                  torchrun --rdzv_backend=static \\\n                           --rdzv_id=0 \\\n                           --rdzv_endpoint=\\${MASTER_ADDR}:\\${MASTER_PORT} \\\n                           --node_rank=\\${RANK} \\\n                           --nproc_per_node=\\${PET_NPROC_PER_NODE} \\\n                           --nnodes=\\${PET_NNODES}:\\${PET_NNODES} \\\n                           --max_restarts=0 \\\n                           --rdzv_conf=timeout=3600 \\\n                           --no_python \\\n                           fast-llm prepare gpt_memmap \\\n                           --config prep-stack-tutorial/the-stack-prepare.yaml\n              env:\n                - name: PYTHONHASHSEED\n                  value: \"0\"\n              securityContext:\n                capabilities:\n                  add:\n                    - IPC_LOCK\n              volumeMounts:\n                - mountPath: /app/prep-stack-tutorial\n                  name: prep-stack-tutorial\n                - mountPath: /dev/shm\n                  name: dshm\n          volumes:\n            - name: prep-stack-tutorial\n              persistentVolumeClaim:\n                claimName: pvc-prep-stack-tutorial\n            - name: dshm\n              emptyDir:\n                medium: Memory\n                sizeLimit: \"1024Gi\"\nEOF\n</code></pre> <p>You can follow the job's progress by running <code>kubectl get pods</code> and checking the logs with <code>kubectl logs fast-llm-stack-prepare-master-0</code>.</p> <p>That is all! Once the jobs complete, you'll see the data in Fast-LLM's memory-mapped indexed dataset format in <code>./prep-stack-tutorial/tokenized/Mistral-Nemo-Base-2407</code> which can be used with Fast-LLM to set off a training run.</p>"},{"location":"recipes/generate/","title":"How to Generate with a Fast-LLM Model","text":"<p>Fast-LLM models support <code>generate</code> and <code>forward</code> operations through Hugging Face\u2013compatible wrappers.</p> <p>\u26a0\ufe0f Limitations:</p> <ul> <li>No support for <code>cache</code>, <code>past_key_values</code>, <code>labels</code>, <code>attention</code> outputs, or <code>inputs_embeds</code></li> <li><code>position_ids</code> are ignored and reconstructed from the attention mask</li> <li>model-parallel and sequence-data-parallel generation is not supported</li> </ul>"},{"location":"recipes/generate/#generating-text-from-a-fast-llm-model","title":"\ud83d\udd27 Generating Text from a Fast-LLM Model","text":"<p>Below is a step-by-step example of how to generate text using a Fast-LLM model checkpoint from Hugging Face Hub.</p> <pre><code># Import dependencies\nimport huggingface_hub\nfrom transformers import AutoTokenizer\nfrom fast_llm.engine.checkpoint.config import CheckpointLoadConfig\nfrom fast_llm.models.gpt.conversion.config import LlamaCheckpointFormat\nfrom fast_llm.models.gpt.huggingface import HuggingfaceGPTModelForCausalLM\n\n# Specify model and configuration\nmodel = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ncheckpoint_format = LlamaCheckpointFormat\nmax_new_tokens = 50\n\n# Download model checkpoint from the Hugging Face Hub to a local directory\nmodel_path = huggingface_hub.snapshot_download(repo_id=model, local_dir=\"/tmp\")\n\n# Load tokenizer from the downloaded model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Optional: updates to Fast-LLM config before loading the model\nupdates = {\n    (\"base_model\", \"transformer\", \"use_flash_attention\"): True,\n    (\"distributed\", \"training_dtype\"): \"bf16\"\n}\n\n# Load the model from the checkpoint with the given configuration\nmodel = HuggingfaceGPTModelForCausalLM.from_pretrained(\n    CheckpointLoadConfig(\n        path=model_path,\n        format=checkpoint_format,\n        model_weights=True,\n    ),\n    updates,\n)\n\n# Example input messages formatted for chat-style generation\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is gravity?\"},\n    {\"role\": \"user\", \"content\": \"Who is the president of EU?\"},\n]\n\n# Convert messages into model input format using chat template\ninput_text = [tokenizer.apply_chat_template([el], tokenize=False) for el in messages]\n\n# Prepare tokenized input for the model\ntokenizer.padding_side = \"left\"  # Important for correct padding\ninputs = tokenizer(input_text, padding=\"longest\", return_tensors=\"pt\").to(\"cuda\")\n\n# Generate text using the model\noutputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=False)\n\n# Decode and display outputs\noutputs = [tokenizer.decode(el, skip_special_tokens=True) for el in outputs]\n\nprint(\"--------------------------------------------------------------------\")\nfor el in outputs:\n    print(el)\n    print(\"--------------------------------------------------------------------\")\n</code></pre>"},{"location":"recipes/instruction-finetuning/","title":"Instruction Finetuning on Llama 3.1 8B","text":"<p>In this guide, we provide step-by-step instructions to do a supervised finetuning (SFT) of the Llama 3.1 8B model on an instruction tuning dataset Magpie-Align/Magpie-Llama-3.1-Pro-MT-300K-Filtered. Typically, SFT differs from pretraining in 2 key ways:</p> <ol> <li>Loss Computation: Instruction tuning datasets typically contain system, user and assistant messages formatted using a chat template e.g., ChatML. The loss is typically computed only on the tokens from the assistant messages, and hence the gradient updates do not include the system or user tokens.</li> <li>Packing: Fast-LLM packs multiple documents into a sequence to maximize training throughput. However, paying attention to other documents in a sequence can detrimental to instruction-finetuned models. Moreover, packing to sequence length can mean some document are split across multiple sequences.</li> </ol> <p>Fast-LLM provides options to modify both of these, as we will see in this guide. Please follow the Quick Start guide for the initial setup.</p>"},{"location":"recipes/instruction-finetuning/#step-1-download-the-pretrained-model","title":"\ud83d\udcda Step 1: Download the Pretrained Model","text":"<pre><code>git lfs install\ngit clone https://huggingface.co/meta-llama/Llama-3.1-8B ./sft-tutorial/Llama-3.1-8B\n</code></pre>"},{"location":"recipes/instruction-finetuning/#step-2-format-the-dataset-into-a-chat-template","title":"\ud83d\udd04 Step 2: Format the dataset into a chat template","text":"<p>We'll use Llama-3.1-8B-Instruct's tokenizer for this tutorial. Let's create a folder first:</p> <pre><code>mkdir -p ./sft-tutorial/checkpoints/Llama-3.1-8B-Instruct\n</code></pre> <p>And then download the tokenizer with this Python script:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.save_pretrained(\"./sft-tutorial/Llama-3.1-8B-Instruct\")\n</code></pre> <p>To disable loss computation on pieces of texts like the user/system messages or the chat template tags, we need to define the character spans for this masking. The example below formats the Magpie dataset using Llama-3.1-8B-Instruct's chat template and defines spans where the loss should be masked.</p> <pre><code>from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ndef apply_chat_template(conversation):\n    chatml_conv = []\n    for conv in conversation:\n        if conv[\"from\"] == \"human\":\n            chatml_conv.append({\"role\": \"user\", \"content\": conv[\"value\"]})\n        elif conv[\"from\"] == \"gpt\":\n            chatml_conv.append({\"role\": \"assistant\", \"content\": conv[\"value\"]})\n    return tokenizer.apply_chat_template(conversation=chatml_conv, tokenize=False)\n\n\ndef get_spans(text: str, start_delimiter: str, end_delimiter: str):\n    spans = []\n    start = 0\n    while start &lt; len(text):\n        end = text.find(start_delimiter, start) + len(start_delimiter)\n        if end == -1:\n            break\n        spans.append([start, end - 1])  # character span indices are inclusive\n        start = text.find(end_delimiter, end) + len(end_delimiter)\n    return spans\n\n\ndef get_sample_with_spans(sample, start_delimiter, end_delimiter):\n    text = apply_chat_template(sample[\"conversations\"])\n    spans = get_spans(text, start_delimiter, end_delimiter)\n    return {\"text\": text, \"spans\": spans}\n\ndataset_name = \"Magpie-Align/Magpie-Llama-3.1-Pro-MT-300K-Filtered\"\ntokenizer_path = \"./sft-tutorial/Llama-3.1-8B-Instruct\"\ndataset = load_dataset(dataset_name)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\nstart_delimiter = \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nend_delimiter = \"&lt;|eot_id|&gt;\"\ndataset = dataset.map(lambda x: get_sample_with_spans(x, start_delimiter, end_delimiter), num_proc=16)\ndataset.save_to_disk(\"./sft-tutorial/chatml_dataset\")\n</code></pre>"},{"location":"recipes/instruction-finetuning/#step-3-prepare-the-dataset","title":"\ud83d\udd0d Step 3: Prepare the dataset","text":"<p>This step is similar to Data preparation, with one additional option for the field with loss masking spans.</p> <pre><code>output_path: ./sft-tutorial/tokenized/Llama-3.1-8B\n\nloading_workers: 32\ntokenize_workers: 32\nsaving_workers: 32\n\ndataset:\n  path: ./sft-tutorial/hf_dataset\n  split: \"train\"\n  trust_remote_code: true\n  field: text\n  loss_masking_spans: spans\n\ntokenizer:\n  path: ./sft-tutorial/Llama-3.1-8B\nsplits:\n  training: 0.998\n  validation: 0.002\n</code></pre>"},{"location":"recipes/instruction-finetuning/#step-4-configure-fast-llm","title":"\u2699\ufe0f Step 4: Configure Fast-LLM","text":"<p>It's time to configure the Fast-LLM training config. This is very similar to Quick Start with two additional options, namely, <code>truncate_documents</code> and <code>cross_document_attention</code> which are important for improving the task performance of instruction-tuned models.</p> <pre><code>training:\n  train_iters: 5_000\n  logs:\n    interval: 1\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:\n    format: llama\n    interval: 1000\nbatch:\n  micro_batch_size: 1\n  sequence_length: 4096\n  batch_size: 32\n  cross_document_attention: no # (1)!\ndata:\n  datasets:\n    training:\n      type: file\n      path: ./sft-tutorial/tokenized/Llama-3.1-8B/fast_llm_config_training.yaml\n    validation:\n      type: file\n      path: ./sft-tutorial/tokenized/Llama-3.1-8B/fast_llm_config_validation.yaml\n  truncate_documents: no # (2)!\n  sampling:\n    use_loss_masking_spans: yes\noptimizer:\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:\n    base: 2.0e-05\n    minimum: 0.0\n    decay_style: cosine\n    decay_iterations: 4_900\n    warmup_iterations: 100\npretrained:\n  format: llama\n  path: ./sft-tutorial/Llama-3.1-8B\n  model_weights: yes\nmodel:\n  base_model:\n    transformer:\n      use_flash_attention: yes\n    cross_entropy_impl: fused\n  multi_stage:\n    zero_stage: 3\n  distributed:\n    timeout: 3600\n    training_dtype: bf16\nrun:\n  experiment_dir: ./sft-tutorial/llama-3.1-8b-instruct-magpie\n</code></pre> <ol> <li>Prevents paying attention to other documents in a packed sequence</li> <li>Avoids truncating documents to fit into a packed sequence and starts a new sequence instead. Documents longer than sequence length will be skipped altogether.</li> </ol> <p>Launching the training run is similar to Step 7 in the Quick Start guide.</p>"},{"location":"recipes/train/","title":"Training Llama 3.1 8B","text":"<p>Follow this guide to train a Llama-3.1 or Qwen 2.5 7B like model from scratch!</p>"},{"location":"recipes/train/#preliminary-steps","title":"Preliminary steps","text":"<ul> <li>Quick Start</li> <li>Data preparation</li> </ul>"},{"location":"recipes/train/#training-configuration","title":"Training configuration","text":"<p>In this guide, we show you how to configure a model architecture and train a model from scratch. Let's start from the following training configuration:</p> Llama 3.1 8BQwen 2.5 7B <pre><code>training:\n  train_iters: 100_000\n  logs:\n    interval: 10\n  evaluators:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:\n    format: llama\n    interval: 20_000\nbatch:\n  micro_batch_size: 2\n  sequence_length: 4096\n  batch_size: 256\ndata:\n  datasets:\n    training:\n      type: file\n      path: path/to/training_dataset_config.yaml\n    validation:\n      type: file\n      path: path/to/validation_dataset_config.yaml\noptimizer:\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:\n    base: 6.0e-04\n    minimum: 6.0e-05\n    decay_style: cosine\n    decay_iterations: 100_000\n    warmup_iterations: 2000\nmodel:\n  base_model:\n    cross_entropy_impl: fused\n  multi_stage:\n    zero_stage: 2\n  distributed:\n    training_dtype: bf16\nrun:\n  experiment_dir: fast-llm-tutorial/experiment\n</code></pre> <pre><code>training:\n  train_iters: 100_000\n  logs:\n    interval: 10\n  evaluators:\n    validation:\n      interval: 100\n      evaluator:\n        type: loss\n        iterations: 25\n        dataset_name: validation\n  checkpoint:\n    interval: 1000\n    keep: 5\n  test_iters: 0\n  export:\n    format: qwen2\n    interval: 20_000\nbatch:\n  micro_batch_size: 1\n  sequence_length: 8192\n  batch_size: 256\ndata:\n  datasets:\n    training:\n      type: file\n      path: path/to/training_dataset_config.yaml\n    validation:\n      type: file\n      path: path/to/validation_dataset_config.yaml\noptimizer:\n  weight_decay: 0.1\n  beta_1: 0.9\n  beta_2: 0.95\n  learning_rate:\n    base: 6.0e-04\n    minimum: 6.0e-05\n    decay_style: cosine\n    decay_iterations: 100_000\n    warmup_iterations: 2000\nmodel:\n  base_model:\n    cross_entropy_impl: fused\n  multi_stage:\n    zero_stage: 2\n  distributed:\n    training_dtype: bf16\nrun:\n  experiment_dir: fast-llm-tutorial/experiment\n</code></pre> <p>This configuration will not work because it misses important arguments to define model architecture. There are 2 ways of instantiating our a model.</p> <p>We could use a pretrained model config. This step is similar to what is done in the Quick Start guide. First download the model configuration:</p> Llama 3.1 8BQwen 2.5 7B <pre><code>git lfs install\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/meta-llama/Llama-3.1-8B ./fast-llm-tutorial/pretrained-model\n</code></pre> <pre><code>git lfs install\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Qwen/Qwen2.5-7B ./fast-llm-tutorial/pretrained-model\n</code></pre> <p>By specifying a pretrained model from the HuggingFace hub, Fast-LLM automatically converts the config to load the model.     Only the configuration is loaded, not the weights, because of <code>model_weights: no</code>.</p> Llama 3.1 8BQwen 2.5 7B <pre><code>pretrained:\n  format: llama\n  path: fast-llm-tutorial/pretrained_model\n  model_weights: no\n</code></pre> <pre><code>pretrained:\n  format: qwen2\n  path: fast-llm-tutorial/pretrained_model\n  model_weights: no\n</code></pre> <p>Alternatively, we define the model architecture ourselves as follows:</p> Llama 3.1 8BQwen 2.5 7B <pre><code>model:\n  base_model:\n    tie_word_embeddings: false\n    use_position_embeddings: false\n    vocab_size: 128256\n    transformer:\n      activation_type: silu\n      add_linear_biases: false\n      ffn_hidden_size: 14336\n      gated: true\n      head_groups: 8\n      hidden_size: 4096  # (1)!\n      kv_channels: 128\n      normalization:\n        type: rms_norm\n      num_attention_heads: 32\n      num_layers: 32\n      rotary:\n        type: llama3\n        theta: 500_000\n</code></pre> <pre><code>model:\n  base_model:\n    tie_word_embeddings: false\n    use_position_embeddings: false\n    vocab_size: 152064\n    transformer:\n      activation_type: silu\n      add_linear_biases: only_attn_qkv\n      ffn_hidden_size: 18944\n      gated: true\n      head_groups: 4\n      hidden_size: 3584  # (1)!\n      normalization:\n        type: rms_norm\n        epsilon: 1e-06\n      num_attention_heads: 28\n      num_layers: 28\n      rotary:\n        type: default\n        theta: 1_000_000\n</code></pre> <ol> <li>Hidden-size/num-layers will be used to provide good defaults for weight initialization std.</li> </ol> <p>Configuring the model this way is a bit more verbose than using the pretrained configuration, but gives an idea of how to configure a the model with Fast-LLM.</p>"},{"location":"recipes/upcycle-llama-3b-to-moe/","title":"Upcycling Llama 3B to MoE","text":"<p>Warning</p> <p>This guide is under construction. Check back soon to see how to give your Llama 3B a new life as an MoE!</p>"},{"location":"success-stories/starcoder-2/","title":"StarCoder2","text":"<p>2023 was a transformative year for ServiceNow Research's Foundation Model Lab. Partnering with BigCode, we set out to build StarCoder2 <sup>1</sup>, an open-source language model designed specifically for coding tasks. This iteration of StarCoder <sup>2</sup> has been built to handle a wide range of programming languages with performance on par with some larger models.</p> <p>Our goal was ambitious: to train the 3-billion-parameter StarCoder2 model on over 3 trillion tokens from The Stack V2\u2014a rich, diverse dataset compiled by BigCode from the Software Heritage archive. This data provided StarCoder2 with the breadth of real-world code examples and programming paradigms it needed to tackle complex coding tasks with high accuracy and deep contextual understanding.</p> <p>To bring StarCoder2 to life, we ran Fast-LLM on NVIDIA's DGX SuperCloud, utilizing DGX A100-80GB nodes. Fast-LLM allowed us to maximize GPU throughput and streamline our entire training pipeline. The complexity of scaling StarCoder2's training across nodes became a seamless experience.</p>"},{"location":"success-stories/starcoder-2/#how-fast-llm-made-starcoder2-possible","title":"How Fast-LLM Made StarCoder2 Possible","text":"<p>Fast-LLM was designed to maximize efficiency in large-scale language model training\u2014especially for tasks like StarCoder2. Here's how Fast-LLM's capabilities helped us achieve our goals:</p> <ul> <li> <p>Optimized Throughput and GPU Utilization: Fast-LLM's data parallelism allowed each A100-80GB GPU to operate at its peak, sustaining 10,000 tokens per second throughput. This boosted GPU utilization and brought down training time by 20% compared to other frameworks. Fast-LLM made sure every GPU cycle was used efficiently, cutting down on idle time across the board.</p> </li> <li> <p>Support for Long Contexts: With Fast-LLM's built-in Grouped Query Attention (GQA), StarCoder2-3B was able to leverage a 16,384 token context window. This is essential for code comprehension, where context often spans hundreds of lines or more. GQA enabled the model to hold extensive context across sequences, which translates into better understanding of long code snippets, in-depth documentation, and detailed coding conversations.</p> </li> <li> <p>Fill-in-the-Middle (FIM) Training: Fast-LLM supported FIM training objectives natively, allowing StarCoder2-3B to complete and understand code by predicting missing snippets in various contexts. This structure-focused training enhanced the model's performance, making it adept at understanding code structure, flow, and syntax.</p> </li> </ul>"},{"location":"success-stories/starcoder-2/#the-takeaway","title":"The Takeaway","text":"<p>StarCoder2-3B is the first large-scale, real-world demonstration of Fast-LLM's capabilities in specialized language model training. This project exemplifies how Fast-LLM not only powers large models but does so with adaptability and efficiency. It's not just about achieving results\u2014it's about doing so in a way that's replicable and accessible to labs of all sizes.</p> <p>With Fast-LLM, we've made a leap in efficiency and performance, setting the stage for future innovation in LLM training. This is just the beginning, and we're excited to see how Fast-LLM will continue to push the boundaries of language model development for coding and beyond.</p> <ol> <li> <p>Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, and others. Starcoder 2 and the stack v2: the next generation. arXiv preprint arXiv:2402.19173, 2024.\u00a0\u21a9</p> </li> <li> <p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, and others. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/configuration/","title":"Configuration Reference","text":"<p>Warning</p> <p>Looking for the full config details? This reference is on the way. Stay tuned!</p>"},{"location":"user_guide/evaluators/","title":"Evaluations","text":"<p>Fast-LLM allows you to perform various evaluations during training or as a separate evaluation step. In both cases, you need to use your training config with <code>training.evaluators</code> specified.</p> <p>For evaluators used during training, both <code>interval</code> and <code>offset</code> must be specified. Then, start training as usual with:</p> <p><code>fast-llm train gpt --config path/to/training/config.yaml</code></p> <p>To perform evaluation as a separate step, use the same training config. Depending on the training progress, either the start model or the latest checkpoint will be loaded, and <code>interval</code> and <code>offset</code> will be ignored. To start evaluation:</p> <p><code>fast-llm evaluate gpt --config path/to/training/config.yaml</code></p>"},{"location":"user_guide/evaluators/#currently-supported-evaluators","title":"Currently Supported Evaluators","text":"<ul> <li><code>loss</code></li> <li><code>lm_eval</code></li> </ul>"},{"location":"user_guide/evaluators/#loss-evaluator","title":"Loss Evaluator","text":"<p>To set up loss evaluation, specify a dataset to be used in the <code>data.datasets</code> section of the config. You must also define the loss evaluator in the <code>training.evaluators</code> config section. See example below.</p> <pre><code>training:\n  evaluations:\n    stack_3b:\n      interval: 10\n      evaluator:\n        type: loss\n        iterations: 10\n        dataset_name: stack_3b\n    fineweb:\n      evaluator:\n        type: loss\n        iterations: 10\n        dataset_name: stack_3b\n      interval: 10\ndata:\n  datasets:\n    stack_3b:\n      type: memmap\n      path: path/to/memmap/dataset\n    fineweb:\n      type: memmap\n      path: path/to/memmap/dataset1\n</code></pre>"},{"location":"user_guide/evaluators/#evaluation-harness-lm_eval-evaluator","title":"Evaluation Harness (<code>lm_eval</code>) Evaluator","text":"<p>Note: Only data parallelism is currently supported for the <code>lm_eval</code> evaluator.</p> <p>To run <code>lm_eval</code> evaluations, version <code>0.4.9</code> of <code>lm_eval</code> must be installed along with all dependencies required for your evaluation tasks.</p> <p>The following environment variables may need to be set:</p> <ul> <li><code>HF_HOME</code>: Path for Hugging Face data caching</li> <li><code>WANDB_API_KEY_PATH</code>: Path to a file containing your Weights &amp; Biases API key (if logging to W&amp;B)</li> <li><code>HUGGINGFACE_API_KEY_PATH</code>: Path to a file containing your Hugging Face hub token</li> <li><code>NLTK_DATA</code>: Path to a directory that will contain downloaded NLTK packages (needed for some tasks)</li> <li><code>HF_ALLOW_CODE_EVAL=1</code>: Required for some evaluation tasks</li> </ul> <p>You may need to specify additional environment variables depending on the <code>lm_eval</code> tasks you want to run.</p> <p>To specify an <code>lm_eval</code> task, the evaluator config includes the following fields:</p>"},{"location":"user_guide/evaluators/#model-config","title":"Model Config","text":"<p>The model instantiated for training is reused for evaluation, so you don't need to specify it separately. However, there are some parameters specific to <code>lm_eval</code>. See <code>fast_llm/engine/evaluation/config.EvaluatorLmEvalConfig</code> for details.</p>"},{"location":"user_guide/evaluators/#cli-parameters-for-lm_eval","title":"CLI Parameters for <code>lm_eval</code>","text":"<p>All other parameters are specified as if you were calling the <code>lm_eval</code> CLI, using a list of strings. Some CLI parameters are ignored or restricted\u2014specifically those related to model loading, W&amp;B, batch sizes, and device setup, as these are managed by the rest of the Fast-LLM configuration.</p> <p>Also, the tokenizer must be specified in <code>data.tokenizer</code>. If the tokenizer does not have a <code>bos_token</code>, it must be specified explicitly in <code>data.tokenizer.bos_token</code>. Although <code>lm_eval</code> does not use the <code>bos_token</code> directly, it is still required because the same tokenizer is used by other Fast-LLM components.</p> <p>Below is an example of the config:</p> <pre><code>training:\n  evaluations:\n    lm_eval_tasks1:\n      interval: 10\n      evaluator:\n        type: lm_eval\n        cli_args:\n          - --tasks\n          - gsm8k,xnli_en,wikitext,ifeval\n          - --output_path\n          - /path/to/lm_eval/output\ndata:\n  tokenizer:\n    path: path/to/the/tokenizer\n</code></pre> <p>It is also possible to run different tasks with different intervals and offsets\u2014for example, to run slower or more comprehensive tasks less frequently.:</p> <pre><code>training:\n  evaluations:\n    gsm8k:\n      interval: 20\n      evaluator:\n        type: lm_eval\n        cli_args:\n          - --tasks\n          - gsm8k\n          - --output_path\n          - /path/to/lm_eval/output\n          - --limit\n          - \"64\"\n    ifeval:\n      offset: 10\n      interval: 40\n      evaluator:\n        type: lm_eval\n        cli_args:\n          - --tasks\n          - ifeval\n          - --output_path\n          - /path/to/lm_eval/output\n          - --limit\n          - \"32\"\n    faster_tasks:\n      interval: 10\n      evaluator:\n        type: lm_eval\n        cli_args:\n          - --tasks\n          - xnli_en,wikitext\n          - --output_path\n          - /path/to/lm_eval/output\ndata:\n  tokenizer:\n    path: path/to/the/tokenizer\n</code></pre>"},{"location":"user_guide/multi-stage/","title":"Multi-Stage Training in Fast-LLM","text":"<p>Fast-LLM trains large models by splitting them into stages, each running on a separate GPU or node. It reduces memory usage by distributing (or sharding) model state (weights, gradients, or optimizer states) across devices.</p> <p>A stage refers to a logical partition of a model, typically containing a subset of layers or computational steps. Each stage runs independently on its own GPU or node.</p> <p>This guide explains how to configure multi-stage training for both common and advanced use cases.</p>"},{"location":"user_guide/multi-stage/#zero-stage-sharding","title":"ZeRO-Stage Sharding","text":"<p>Fast-LLM uses ZeRO-style sharding to reduce memory usage by partitioning model state (such as weights, gradients, and optimizer states) across GPUs that would otherwise maintain full replicas in data parallelism. This is compatible with and complementary to model-parallel techniques like pipeline and tensor parallelism.</p> <p>The primary setting for ZeRO sharding is <code>zero_stage</code> in your configuration:</p> <pre><code>multi_stage:\n  zero_stage: ...\n</code></pre> <p>The following table summarizes the behavior of <code>zero_stage</code>:</p> <code>zero_stage</code> Weights Gradients Optimizer States Communication overhead <code>1</code> (default) Replicated Replicated Sharded Moderate, default choice <code>2</code> Replicated Sharded Sharded Moderate<sup>1</sup> <code>3</code> Sharded Sharded Sharded High<sup>2</sup> <p>Optimizer states are always sharded by default. ZeRO Stage 0 (full replication) is not supported.</p> <p>While ZeRO Stage 3 introduces the most communication overhead, the practical difference between Stages 1 and 2 is minimal except during gradient accumulation.</p> <p>Recommendation:</p> <ul> <li>ZeRO Stage 1 (default): Ideal for most training scenarios.</li> <li>ZeRO Stage 2: Useful if gradients cause memory pressure.</li> <li>ZeRO Stage 3: Useful for very large models exceeding GPU memory.</li> </ul> <p>In general, start with the default (<code>zero_stage: 1</code>) and verify if your model trains without memory errors. If you encounter out-of-memory issues, try increasing <code>zero_stage</code>:</p> <pre><code>multi_stage:\n  zero_stage: 2\n</code></pre> <p>Increasing ZeRO-style sharding reduces memory consumption but may add communication overhead between GPUs or nodes, potentially slowing down training. Before increasing <code>zero_stage</code>, first try lowering the micro batch size or sequence length, as this typically incurs less overhead.</p> <p>You'll likely iterate between adjusting <code>zero_stage</code>, micro batch size, and sequence length to find the optimal balance of memory usage and training throughput. If these adjustments don't resolve your issue, or you're unsatisfied with tradeoffs like sequence length versus throughput, reconsider your broader parallelism strategy. This includes adjusting tensor parallelism, pipeline parallelism, or sequence data parallelism, covered in greater depth in the Parallelism Guide.</p>"},{"location":"user_guide/multi-stage/#expert-options","title":"Expert Options","text":"<p>Beyond <code>zero_stage</code>, Fast-LLM offers additional multi-stage settings for fine-tuning. These advanced options typically don't need manual adjustment. Change them only if you're certain about your goals and tradeoffs.</p>"},{"location":"user_guide/multi-stage/#buffers","title":"Buffers","text":"<p>Fast-LLM streams sharded tensors through communication buffers, allowing network transfers to overlap with GPU computation. These buffers temporarily store gradient or weight shards during forward and backward passes, improving training throughput by hiding communication latency.</p> <p>Buffers are only relevant when gradients or parameters are actually sharded, depending on your ZeRO stage:</p> Buffer type Active when Config key Default Gradient buffers ZeRO stage 2 or 3 <code>num_grad_buffers</code> <code>1</code> Weight buffers ZeRO stage 3 only <code>num_weight_buffers</code> <code>1</code> <ul> <li> <p>Gradient buffers (<code>num_grad_buffers</code>):</p> <ul> <li>Applies when gradients are sharded (ZeRO stages 2 and 3).</li> <li>Default (<code>1</code>) means no overlap (gradients are communicated layer-by-layer).</li> <li>Setting to <code>2</code> enables double-buffering (second buffer lets gradients transfer asynchronously while the GPU computes the next layer). Values of <code>3</code> or more add additional buffers, further increasing overlap at the cost of extra GPU memory per additional buffer.</li> </ul> </li> <li> <p>Weight buffers (<code>num_weight_buffers</code>):</p> <ul> <li>Applies only at ZeRO stage 3 when parameters (weights) are sharded.</li> <li>Default (<code>1</code>) means no overlap (parameters communicated without asynchronous transfer).</li> <li>Setting to <code>2</code> enables double-buffering for weights (second buffer lets parameter transfers overlap with GPU computation). Higher values add more overlap, consuming additional GPU memory per buffer.</li> </ul> </li> </ul> <p>These buffer settings have no effect when their respective tensors aren't sharded:</p> <ul> <li>At ZeRO stage 1, gradients and parameters are fully replicated, so both <code>num_grad_buffers</code> and <code>num_weight_buffers</code> are ignored.</li> <li>At ZeRO stage 2, parameters remain replicated; thus, only <code>num_grad_buffers</code> is relevant.</li> </ul> <p>Buffers do not reduce the total amount of communication, Rather, they shift when communication occurs, improving throughput if your training is network-bound and you have spare GPU memory.</p> <p>If you want explicit control, you can override these values in your configuration:</p> <pre><code>multi_stage:\n  num_grad_buffers: 3        # ZeRO 2 or 3\n  num_weight_buffers: 2      # ZeRO 3 only\n</code></pre> <p>Adjust buffers only if you observe GPU utilization drops due to frequent waiting for network transfers, and have GPU memory to spare. Start with defaults (<code>1</code>) and tune upward cautiously.</p>"},{"location":"user_guide/multi-stage/#stage-layout-control","title":"Stage Layout Control","text":"<p>You can adjust how layers and pipeline stages map onto GPUs or nodes:</p> <pre><code>multi_stage:\n  layers_per_stage: 1.0\n  stages_per_pipeline_stage: 1\n</code></pre> <p>Defaults work well in most cases:</p> <ul> <li> <p><code>layers_per_stage</code>: Determines the number of layers per stage. Defaults to <code>1.0</code> (one layer per stage). Increase to reduce inter-stage communication or decrease for better load balancing. Fractional values are allowed.</p> <p>Warning</p> <p>This setting is supported but hasn't been tested in recent versions. Use with caution.</p> </li> <li> <p><code>stages_per_pipeline_stage</code>: Intended to specify how many stages run per pipeline worker when pipeline parallelism is active.</p> <p>Warning</p> <p>This feature is currently not implemented. Changing this value will currently cause a validation error.</p> </li> </ul> <ol> <li> <p>Communication overhead for ZeRO Stage 2 is similar to Stage 1, except during (depth-first) gradient accumulation when additional reduce-scatter operations occur.\u00a0\u21a9</p> </li> <li> <p>Communication overhead for ZeRO Stage 3 is higher than Stage 2, especially during (depth-first) gradient accumulation.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/parallelism/","title":"Parallelism","text":"<p>Warning</p> <p>Looking for the parallelism guide? It's on its way, come back soon!</p>"}]}