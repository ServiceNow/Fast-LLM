{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fast_llm.engine.checkpoint.config import CheckpointLoadConfig\n",
    "from fast_llm.models.gpt.model import GPTModel, GPTModelConfig\n",
    "from fast_llm.engine.distributed.config import DistributedConfig\n",
    "from fast_llm.engine.checkpoint.config import DistributedCheckpointFormat\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from fast_llm.config import NoAutoValidate\n",
    "from fast_llm.engine.checkpoint.config import CheckpointLoadMetadataConfig\n",
    "from fast_llm.models.gpt.config import GPTModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading metadata from /mnt/checkpoints/fast_llm_exp/slam/checkpoints/pt_runs/slam-5_1b-z1-4096-mbs6/checkpoint/290000/metadata.yaml\n",
      "\n",
      "------------------------------- Loaded metadata --------------------------------\n",
      "fast_llm_version: 0.2.0\n",
      "model: gpt\n",
      "format: distributed\n",
      "config:\n",
      "  base_model:\n",
      "    transformer:\n",
      "      normalization:\n",
      "        type: rms_norm\n",
      "      num_layers: 28\n",
      "      hidden_size: 4096\n",
      "      num_attention_heads: 24\n",
      "      head_groups: 8\n",
      "      add_linear_biases: false\n",
      "      ffn_hidden_size: 8192\n",
      "      kv_channels: 128\n",
      "      gated: true\n",
      "      activation_type: silu\n",
      "      init_method_std: 0.015625\n",
      "      init_method_std_qkv: 0.015625\n",
      "      init_method_std_attn_proj: 0.0020879784524408156\n",
      "      init_method_std_mlp_1: 0.015625\n",
      "      init_method_std_mlp_2: 0.0020879784524408156\n",
      "      mlp_lr_scale:\n",
      "      - null\n",
      "      '!!! rotary':\n",
      "        theta: 1000000.0\n",
      "        type: llama3\n",
      "    vocab_size: 131072\n",
      "    use_position_embeddings: false\n",
      "    tie_word_embeddings: false\n",
      "    init_method_std_embed: 0.015625\n",
      "    cross_entropy_impl: fused\n",
      "  multi_stage:\n",
      "    zero_stage: 1\n",
      "  distributed:\n",
      "    world_size: 480\n",
      "    rank: 0\n",
      "    local_world_size: 8\n",
      "    distributed_timeout: 7200.0\n",
      "    seed: 984059\n",
      "    training_dtype: bfloat16\n",
      "shards:\n",
      "- weights\n",
      "- exp_avgs\n",
      "- exp_avgs_sq\n",
      "metadata:\n",
      "  completed_steps: 290000\n",
      "  metrics:\n",
      "    Training:\n",
      "      allocated: 27872.25048828125\n",
      "      batch_size: 2880\n",
      "      completion_time: 1735738932.3544912\n",
      "      consumed_samples: 835200000\n",
      "      consumed_tokens: 3420979200000\n",
      "      global_max_reserved: 140194.0\n",
      "      grad_norm: 0.09805893898010254\n",
      "      hardware_tflops: 383.54373803127896\n",
      "      iteration: 290000\n",
      "      language_model_loss: 2.17189679145813\n",
      "      learning_rate: 0.0003\n",
      "      loss_scale: 1.0\n",
      "      max_allocated: 112038.07763671875\n",
      "      max_reserved: 135330.0\n",
      "      model_tflops: 378.7579872893557\n",
      "      nan_iters: 0\n",
      "      percent_done: 85.52453079472933\n",
      "      remaining_time: 85877.69865661154\n",
      "      reserved: 135330.0\n",
      "      run: 1\n",
      "      skipped_iters: 0\n",
      "      step_time_average_ms: 1749.6067691429294\n",
      "      step_time_ms: 1809.2572170309722\n",
      "      tokens_per_sec_per_gpu: 13583.47490266183\n",
      "      train_iters: 339084\n",
      "    Validation:\n",
      "      allocated: 27872.25048828125\n",
      "      batch_size: 2880\n",
      "      consumed_samples: 835200000\n",
      "      consumed_tokens: 3420979200000\n",
      "      global_max_reserved: 140194.0\n",
      "      hardware_tflops: 550.2478916849097\n",
      "      iteration: 290000\n",
      "      language_model_loss: 2.171331968307495\n",
      "      max_allocated: 52832.7197265625\n",
      "      max_reserved: 135330.0\n",
      "      model_tflops: 544.6575647737423\n",
      "      reserved: 135330.0\n",
      "      step_time_ms: 516.2878788262606\n",
      "      tokens_per_sec_per_gpu: 47601.34995977744\n",
      "      train_iters: 339084\n",
      "  optimizer:\n",
      "    current_step: 290000\n",
      "    grad_scaler:\n",
      "      type: NoopGradScaler\n",
      "------------------------------------- end --------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Validation failed for checkpoint metadata. See logs above for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNestedValidationError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/engine/multi_stage/config.py:301\u001b[0m, in \u001b[0;36mFastLLMModelConfig.load_metadata\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/config.py:333\u001b[0m, in \u001b[0;36mConfig.validate\u001b[0;34m(self, _is_validating)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(e\u001b[38;5;241m.\u001b[39margs)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNestedValidationError\u001b[0m: Validation failed for field `config` of type `fast_llm.engine.multi_stage.config.FastLLMModelConfig` in class fast_llm.engine.multi_stage.config.CheckpointMetadata:\n  Validation failed for field `base_model` of type `fast_llm.models.gpt.config.GPTBaseModelConfig` in class fast_llm.models.gpt.config.GPTModelConfig:\n    Validation failed for field `transformer` of type `fast_llm.layers.transformer.config.TransformerConfig` in class fast_llm.models.gpt.config.GPTBaseModelConfig:\n      Unknown field `rotary` in class fast_llm.layers.transformer.config.TransformerConfig",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NoAutoValidate():\n\u001b[1;32m      4\u001b[0m     pretrained_config \u001b[38;5;241m=\u001b[39m CheckpointLoadMetadataConfig(\n\u001b[1;32m      5\u001b[0m         path\u001b[38;5;241m=\u001b[39mPath(path_to_checkpoint)\n\u001b[1;32m      6\u001b[0m     )   \n\u001b[0;32m----> 7\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModelConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# model_config = GPTModelConfig.from_pretrained(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#     pretrained=pretrained_config,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#     metadata=metadata\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/engine/multi_stage/config.py:304\u001b[0m, in \u001b[0;36mFastLLMModelConfig.load_metadata\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n\u001b[1;32m    303\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mto_logs(log_fn\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation failed for checkpoint metadata. See logs above for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m Assert\u001b[38;5;241m.\u001b[39meq(metadata\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "\u001b[0;31mValueError\u001b[0m: Validation failed for checkpoint metadata. See logs above for details."
     ]
    }
   ],
   "source": [
    "path_to_checkpoint = \"/mnt/checkpoints/fast_llm_exp/slam/checkpoints/pt_runs/slam-5_1b-z1-4096-mbs6/checkpoint/290000\"\n",
    "\n",
    "with NoAutoValidate():\n",
    "    pretrained_config = CheckpointLoadMetadataConfig(\n",
    "        path=Path(path_to_checkpoint)\n",
    "    )   \n",
    "    metadata = GPTModelConfig.load_metadata(pretrained_config)\n",
    "    # model_config = GPTModelConfig.from_pretrained(\n",
    "    #     pretrained=pretrained_config,\n",
    "    #     metadata=metadata\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckpointLoadMetadataConfig(format=<class 'fast_llm.engine.checkpoint.config.FastLLMCheckpointFormat'>, path=PosixPath('/mnt/checkpoints/fast_llm_exp/slam/checkpoints/pt_runs/slam-5_1b-z1-4096-mbs6/checkpoint/290000'), load_config=<ModelConfigType.architecture: 'architecture'>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = \"/mnt/checkpoints/fast_llm_exp/slam/checkpoints/pt_runs/slam-5_1b-z1-4096-mbs6/checkpoint/290000\"\n",
    "with NoAutoValidate():\n",
    "    checkpoint_config = CheckpointLoadConfig(\n",
    "        path=Path(path_to_checkpoint),\n",
    "        format=DistributedCheckpointFormat,\n",
    "        optimizer_state=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------- Loaded metadata --------------------------------\n",
      "fast_llm_version: 0.2.0\n",
      "model: gpt\n",
      "format: distributed\n",
      "config:\n",
      "  base_model:\n",
      "    transformer:\n",
      "      normalization:\n",
      "        type: rms_norm\n",
      "      num_layers: 28\n",
      "      hidden_size: 4096\n",
      "      num_attention_heads: 24\n",
      "      head_groups: 8\n",
      "      add_linear_biases: false\n",
      "      ffn_hidden_size: 8192\n",
      "      kv_channels: 128\n",
      "      gated: true\n",
      "      activation_type: silu\n",
      "      init_method_std: 0.015625\n",
      "      init_method_std_qkv: 0.015625\n",
      "      init_method_std_attn_proj: 0.0020879784524408156\n",
      "      init_method_std_mlp_1: 0.015625\n",
      "      init_method_std_mlp_2: 0.0020879784524408156\n",
      "      mlp_lr_scale:\n",
      "      - null\n",
      "      '!!! rotary':\n",
      "        theta: 1000000.0\n",
      "        type: llama3\n",
      "    vocab_size: 131072\n",
      "    use_position_embeddings: false\n",
      "    tie_word_embeddings: false\n",
      "    init_method_std_embed: 0.015625\n",
      "    cross_entropy_impl: fused\n",
      "  multi_stage:\n",
      "    zero_stage: 1\n",
      "  distributed:\n",
      "    world_size: 480\n",
      "    rank: 0\n",
      "    local_world_size: 8\n",
      "    distributed_timeout: 7200.0\n",
      "    seed: 984059\n",
      "    training_dtype: bfloat16\n",
      "shards:\n",
      "- weights\n",
      "- exp_avgs\n",
      "- exp_avgs_sq\n",
      "metadata:\n",
      "  completed_steps: 290000\n",
      "  metrics:\n",
      "    Training:\n",
      "      allocated: 27872.25048828125\n",
      "      batch_size: 2880\n",
      "      completion_time: 1735738932.3544912\n",
      "      consumed_samples: 835200000\n",
      "      consumed_tokens: 3420979200000\n",
      "      global_max_reserved: 140194.0\n",
      "      grad_norm: 0.09805893898010254\n",
      "      hardware_tflops: 383.54373803127896\n",
      "      iteration: 290000\n",
      "      language_model_loss: 2.17189679145813\n",
      "      learning_rate: 0.0003\n",
      "      loss_scale: 1.0\n",
      "      max_allocated: 112038.07763671875\n",
      "      max_reserved: 135330.0\n",
      "      model_tflops: 378.7579872893557\n",
      "      nan_iters: 0\n",
      "      percent_done: 85.52453079472933\n",
      "      remaining_time: 85877.69865661154\n",
      "      reserved: 135330.0\n",
      "      run: 1\n",
      "      skipped_iters: 0\n",
      "      step_time_average_ms: 1749.6067691429294\n",
      "      step_time_ms: 1809.2572170309722\n",
      "      tokens_per_sec_per_gpu: 13583.47490266183\n",
      "      train_iters: 339084\n",
      "    Validation:\n",
      "      allocated: 27872.25048828125\n",
      "      batch_size: 2880\n",
      "      consumed_samples: 835200000\n",
      "      consumed_tokens: 3420979200000\n",
      "      global_max_reserved: 140194.0\n",
      "      hardware_tflops: 550.2478916849097\n",
      "      iteration: 290000\n",
      "      language_model_loss: 2.171331968307495\n",
      "      max_allocated: 52832.7197265625\n",
      "      max_reserved: 135330.0\n",
      "      model_tflops: 544.6575647737423\n",
      "      reserved: 135330.0\n",
      "      step_time_ms: 516.2878788262606\n",
      "      tokens_per_sec_per_gpu: 47601.34995977744\n",
      "      train_iters: 339084\n",
      "  optimizer:\n",
      "    current_step: 290000\n",
      "    grad_scaler:\n",
      "      type: NoopGradScaler\n",
      "------------------------------------- end --------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Validation failed for checkpoint metadata. See logs above for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNestedValidationError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/engine/multi_stage/config.py:301\u001b[0m, in \u001b[0;36mFastLLMModelConfig.load_metadata\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/config.py:333\u001b[0m, in \u001b[0;36mConfig.validate\u001b[0;34m(self, _is_validating)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(e\u001b[38;5;241m.\u001b[39margs)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNestedValidationError\u001b[0m: Validation failed for field `config` of type `fast_llm.engine.multi_stage.config.FastLLMModelConfig` in class fast_llm.engine.multi_stage.config.CheckpointMetadata:\n  Validation failed for field `base_model` of type `fast_llm.models.gpt.config.GPTBaseModelConfig` in class fast_llm.models.gpt.config.GPTModelConfig:\n    Validation failed for field `transformer` of type `fast_llm.layers.transformer.config.TransformerConfig` in class fast_llm.models.gpt.config.GPTBaseModelConfig:\n      Unknown field `rotary` in class fast_llm.layers.transformer.config.TransformerConfig",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NoAutoValidate():\n\u001b[0;32m----> 2\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModelConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m GPTModelConfig\u001b[38;5;241m.\u001b[39mfrom_metadata(\n\u001b[1;32m      4\u001b[0m         checkpoint_config,\n\u001b[1;32m      5\u001b[0m         metadata\n\u001b[1;32m      6\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/Fast-LLM/fast_llm/engine/multi_stage/config.py:304\u001b[0m, in \u001b[0;36mFastLLMModelConfig.load_metadata\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n\u001b[1;32m    303\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mto_logs(log_fn\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation failed for checkpoint metadata. See logs above for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m Assert\u001b[38;5;241m.\u001b[39meq(metadata\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "\u001b[0;31mValueError\u001b[0m: Validation failed for checkpoint metadata. See logs above for details."
     ]
    }
   ],
   "source": [
    "with NoAutoValidate():\n",
    "    metadata = GPTModelConfig.load_metadata(checkpoint_config)\n",
    "    model_config = GPTModelConfig.from_metadata(\n",
    "        checkpoint_config,\n",
    "        metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single-GPU configuration\n",
    "distributed_config = DistributedConfig(\n",
    "    tensor_parallel=1,\n",
    "    pipeline_parallel=1,\n",
    "    sequence_data_parallel=1,\n",
    "    local_world_size=1,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "# Initialize single-GPU environment\n",
    "distributed = Distributed(config=distributed_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "consolidated_dir = Path(tempfile.mkdtemp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_files = sorted(checkpoint_dir.glob(\"rank_*.safetensors\"))\n",
    "consolidated_state = {}\n",
    "\n",
    "# Read metadata to understand the sharding\n",
    "tp = metadata.get('tensor_parallel', 1)\n",
    "pp = metadata.get('pipeline_parallel', len(rank_files) // tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m rank_state \u001b[38;5;241m=\u001b[39m load_file(rank_file)\n\u001b[1;32m      4\u001b[0m shard_state \u001b[38;5;241m=\u001b[39m rank_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_shard\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mshard_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "for rank_file in rank_files:\n",
    "    # Use safetensors to load the file\n",
    "    rank_state = load_file(rank_file)\n",
    "    shard_state = rank_state['state_shard']\n",
    "    print(shard_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = metadata.get('model_config', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'base_model': {'cross_entropy_impl': 'fused',\n",
       "   'init_method_std_embed': 0.015625,\n",
       "   'tie_word_embeddings': False,\n",
       "   'transformer': {'activation_type': 'silu',\n",
       "    'add_linear_biases': False,\n",
       "    'ffn_hidden_size': 8192,\n",
       "    'gated': True,\n",
       "    'head_groups': 8,\n",
       "    'hidden_size': 4096,\n",
       "    'init_method_std': 0.015625,\n",
       "    'init_method_std_attn_proj': 0.0020879784524408156,\n",
       "    'init_method_std_mlp_1': 0.015625,\n",
       "    'init_method_std_mlp_2': 0.0020879784524408156,\n",
       "    'init_method_std_qkv': 0.015625,\n",
       "    'kv_channels': 128,\n",
       "    'mlp_lr_scale': [None],\n",
       "    'normalization': {'type': 'rms_norm'},\n",
       "    'num_attention_heads': 24,\n",
       "    'num_layers': 28,\n",
       "    'rotary': {'theta': 1000000.0, 'type': 'llama3'}},\n",
       "   'use_position_embeddings': False,\n",
       "   'vocab_size': 131072},\n",
       "  'distributed': {'distributed_timeout': 7200.0,\n",
       "   'local_world_size': 8,\n",
       "   'rank': 0,\n",
       "   'seed': 984059,\n",
       "   'training_dtype': 'bfloat16',\n",
       "   'world_size': 480},\n",
       "  'multi_stage': {'zero_stage': 1}},\n",
       " 'fast_llm_version': '0.2.0',\n",
       " 'format': 'distributed',\n",
       " 'metadata': {'completed_steps': 290000,\n",
       "  'metrics': {'Training': {'allocated': 27872.25048828125,\n",
       "    'batch_size': 2880,\n",
       "    'completion_time': 1735738932.3544912,\n",
       "    'consumed_samples': 835200000,\n",
       "    'consumed_tokens': 3420979200000,\n",
       "    'global_max_reserved': 140194.0,\n",
       "    'grad_norm': 0.09805893898010254,\n",
       "    'hardware_tflops': 383.54373803127896,\n",
       "    'iteration': 290000,\n",
       "    'language_model_loss': 2.17189679145813,\n",
       "    'learning_rate': 0.0003,\n",
       "    'loss_scale': 1.0,\n",
       "    'max_allocated': 112038.07763671875,\n",
       "    'max_reserved': 135330.0,\n",
       "    'model_tflops': 378.7579872893557,\n",
       "    'nan_iters': 0,\n",
       "    'percent_done': 85.52453079472933,\n",
       "    'remaining_time': 85877.69865661154,\n",
       "    'reserved': 135330.0,\n",
       "    'run': 1,\n",
       "    'skipped_iters': 0,\n",
       "    'step_time_average_ms': 1749.6067691429294,\n",
       "    'step_time_ms': 1809.2572170309722,\n",
       "    'tokens_per_sec_per_gpu': 13583.47490266183,\n",
       "    'train_iters': 339084},\n",
       "   'Validation': {'allocated': 27872.25048828125,\n",
       "    'batch_size': 2880,\n",
       "    'consumed_samples': 835200000,\n",
       "    'consumed_tokens': 3420979200000,\n",
       "    'global_max_reserved': 140194.0,\n",
       "    'hardware_tflops': 550.2478916849097,\n",
       "    'iteration': 290000,\n",
       "    'language_model_loss': 2.171331968307495,\n",
       "    'max_allocated': 52832.7197265625,\n",
       "    'max_reserved': 135330.0,\n",
       "    'model_tflops': 544.6575647737423,\n",
       "    'reserved': 135330.0,\n",
       "    'step_time_ms': 516.2878788262606,\n",
       "    'tokens_per_sec_per_gpu': 47601.34995977744,\n",
       "    'train_iters': 339084}},\n",
       "  'optimizer': {'current_step': 290000,\n",
       "   'grad_scaler': {'type': 'NoopGradScaler'}}},\n",
       " 'model': 'gpt',\n",
       " 'shards': ['weights', 'exp_avgs', 'exp_avgs_sq']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fml_ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
