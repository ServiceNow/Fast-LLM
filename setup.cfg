[metadata]
name = llm

[options]
packages = find_namespace:
include_package_data = True
install_requires =
    # Huggingface tools
    transformers>=4.44.2
    hf-transfer>=0.1.8
    safetensors>=0.4.4
    # Weights and biases
    wandb>=0.17.7
    # Testing
    pytest>=8.3.2
    pytest-depends>=1.0.1
    einops>=0.8.0
    PyYAML>=6.0.1
    # Numpy major needs to match torch
    numpy>=1.24.4,<2.0.0

# TODO: Sort out this mess
[options.extras_require]
CUDA =
    # Available through the nvidia base image (nvcr.io/nvidia/pytorch:24.07-py3)
    torch >=2.4.0a0
    # Update the base image (version fixed to ensure there is a wheel for the base image), may need --no-build-isolation
    flash-attn==2.6.3

# Temporary fix because of missing torch wheel
# FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE FLASH_ATTENTION_FORCE_BUILD=TRUE pip install -e ".[MAC]" --no-build-isolation
MAC =
    # No x86 wheel for later versions
    torch >=2.2.2
    # Same as CUDA but also needs environment variables
    flash-attn==2.6.3


[options.entry_points]
console_scripts =
    fast-llm = fast_llm.tools.cli:fast_llm
