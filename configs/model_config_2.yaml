training:
  train_iters: 1050
  num_workers: 1
  logs:
    interval: 10
  checkpoint:
    interval: 400
    keep: 1
  test_iters: 0
  export:  
    format:
      name: mistral3
      vision_name: pixtral
      text_name: mistral
    interval: 400
  wandb:  
    project_name: mistral-7b-cpt
    group_name: test
    entity_name: akshaykalkunte
batch:
  micro_batch_size: 1
  sequence_length: 16384
  batch_size: 8
  max_image_size: 1540
  cross_document_attention: no
  use_loss_masking_spans: yes
data:
  tokenizer: {}
  fim: {}
  truncate_documents: False
  datasets:
    training:
      type: file
      path: /mnt/core_llm2/data/2_1_26_ministral_14b_mm_sft/fastllm_text_downsampled/fast_llm_config.yaml
optimizer:  
  weight_decay: 0.1
  beta_1: 0.9
  beta_2: 0.95
  learning_rate:  
    base: 1.0e-05
    decay_iterations: 1050
    decay_style: cosine
    warmup_iterations: 40
pretrained:
  format: mistral3
  path: /mnt/core_llm2/shruthan/models/Ministral-3-14B-Base-2512
  model_weights: yes
  load_config: model
model:
  base_model:
    vision_encoder:
      image_break_token: 12
      image_end_token: 13
      patch_size: 14
      adapter_size: 4096
      spatial_merge_size: 2
      transformer:
        type: image_encoder
        mlp_lr_scale: 1.0e-12
        router_lr_scale: 1.0e-12
        attention_lr_scale: 1.0e-12
      conv_lr_scale: 1.0e-12
    transformer:
      rotary:
        llama_4_scaling_beta: 0.1
        original_context_length: 16384
      use_flash_attention: yes
    cross_entropy_impl: fused
  multi_stage:
    zero_stage: 3
  distributed:
    training_dtype: bf16
    timeout: 3600
    tensor_parallel: 8
    sequence_tensor_parallel: True

run:
  experiment_dir: /mnt/core_llm2/shruthan/experiments/dummy_2_8_26_ministral_14b_small_sft_text_only